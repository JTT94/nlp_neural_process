{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Class_BERT_NeuralProcesses_notebook_fixedSuppliedContext.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/JTT94/nlp_neural_process/blob/master/Class_BERT_NeuralProcesses_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SZKAwmizhzHo","outputId":"3f78458c-45f3-4691-d439-8942ac53364b","executionInfo":{"status":"ok","timestamp":1559035379051,"user_tz":-60,"elapsed":4384,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":203}},"source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_hub as hub\n","import os\n","import re\n","\n","from keras import backend as K\n","import numpy as np\n","import string\n","from datetime import datetime \n","import tensorflow_probability as tfp\n","from tensorflow_probability import distributions as tfd"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0528 09:22:55.800379 139661544327040 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pgB8X8dti3qc","outputId":"266bff20-027e-4b2b-8798-99b12cfb2054","executionInfo":{"status":"ok","timestamp":1559035383466,"user_tz":-60,"elapsed":8011,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":226}},"source":["!pip install bert-tensorflow\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\u001b[K     |████████████████████████████████| 71kB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jDvh4JVNQFck","colab_type":"code","outputId":"60e9fb7a-06a6-4f53-c6c3-e53df479780b","executionInfo":{"status":"ok","timestamp":1559035409954,"user_tz":-60,"elapsed":25714,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZCJDPOgQOsqJ","colab_type":"code","colab":{}},"source":["# initialiase tensorboard \n","# from https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab\n","\n","\n","# Get TensorBoard running in the background. \n","LOG_DIR = './gdrive/My Drive/Kaggle_toxic_comments/test_output'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sZ3Bl48oaxU","colab_type":"code","outputId":"be397014-ee8b-4406-9738-d14db33e6755","executionInfo":{"status":"ok","timestamp":1559035413144,"user_tz":-60,"elapsed":5013,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":347}},"source":["# # #Download and unzip ngrok. \n","!test -e ngrok-stable-linux-amd64.zip || wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!test -e ngrok || unzip ngrok-stable-linux-amd64.zip\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2019-05-28 09:23:30--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.4.95.48, 34.226.180.131, 52.55.191.55, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.4.95.48|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16648024 (16M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  15.88M  27.7MB/s    in 0.6s    \n","\n","2019-05-28 09:23:31 (27.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [16648024/16648024]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mLp9PmAzoWhE","colab_type":"code","colab":{}},"source":["# #Launch ngrok background process...\n","get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL1A1ibjOwnL","colab_type":"code","outputId":"990eb307-bfb1-4b52-b6cf-87f1b9d03860","executionInfo":{"status":"ok","timestamp":1559035415751,"user_tz":-60,"elapsed":1989,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":186}},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["https://d5a9c0ab.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bweIS72l5WS5","colab_type":"code","outputId":"c68204cb-1a6b-441a-f529-6f8f373935d5","executionInfo":{"status":"ok","timestamp":1559035418023,"user_tz":-60,"elapsed":2899,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["import os, sys\n","sys.path.append('../') # add personal code dir to path for import\n","\n","\n","!test -d neural_process || git clone https://github.com/JTT94/nlp_neural_process.git neural_process\n","sys.path.append('./neural_process/')\n","\n","import random\n","from neural_process import split_context_target, NeuralProcessParams\n","from neural_process.network import *\n","from neural_process.loss import *\n","from neural_process.predict import *\n","from neural_process.process import *\n","\n","from neural_process.tf_model_builder_AUC import *\n","from neural_process.bert_utils import *"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'neural_process'...\n","remote: Enumerating objects: 129, done.\u001b[K\n","remote: Counting objects:   0% (1/129)   \u001b[K\rremote: Counting objects:   1% (2/129)   \u001b[K\rremote: Counting objects:   2% (3/129)   \u001b[K\rremote: Counting objects:   3% (4/129)   \u001b[K\rremote: Counting objects:   4% (6/129)   \u001b[K\rremote: Counting objects:   5% (7/129)   \u001b[K\rremote: Counting objects:   6% (8/129)   \u001b[K\rremote: Counting objects:   7% (10/129)   \u001b[K\rremote: Counting objects:   8% (11/129)   \u001b[K\rremote: Counting objects:   9% (12/129)   \u001b[K\rremote: Counting objects:  10% (13/129)   \u001b[K\rremote: Counting objects:  11% (15/129)   \u001b[K\rremote: Counting objects:  12% (16/129)   \u001b[K\rremote: Counting objects:  13% (17/129)   \u001b[K\rremote: Counting objects:  14% (19/129)   \u001b[K\rremote: Counting objects:  15% (20/129)   \u001b[K\rremote: Counting objects:  16% (21/129)   \u001b[K\rremote: Counting objects:  17% (22/129)   \u001b[K\rremote: Counting objects:  18% (24/129)   \u001b[K\rremote: Counting objects:  19% (25/129)   \u001b[K\rremote: Counting objects:  20% (26/129)   \u001b[K\rremote: Counting objects:  21% (28/129)   \u001b[K\rremote: Counting objects:  22% (29/129)   \u001b[K\rremote: Counting objects:  23% (30/129)   \u001b[K\rremote: Counting objects:  24% (31/129)   \u001b[K\rremote: Counting objects:  25% (33/129)   \u001b[K\rremote: Counting objects:  26% (34/129)   \u001b[K\rremote: Counting objects:  27% (35/129)   \u001b[K\rremote: Counting objects:  28% (37/129)   \u001b[K\rremote: Counting objects:  29% (38/129)   \u001b[K\rremote: Counting objects:  30% (39/129)   \u001b[K\rremote: Counting objects:  31% (40/129)   \u001b[K\rremote: Counting objects:  32% (42/129)   \u001b[K\rremote: Counting objects:  33% (43/129)   \u001b[K\rremote: Counting objects:  34% (44/129)   \u001b[K\rremote: Counting objects:  35% (46/129)   \u001b[K\rremote: Counting objects:  36% (47/129)   \u001b[K\rremote: Counting objects:  37% (48/129)   \u001b[K\rremote: Counting objects:  38% (50/129)   \u001b[K\rremote: Counting objects:  39% (51/129)   \u001b[K\rremote: Counting objects:  40% (52/129)   \rremote: Counting objects:  41% (53/129)   \u001b[K\rremote: Counting objects:  42% (55/129)   \u001b[K\rremote: Counting objects:  43% (56/129)   \u001b[K\rremote: Counting objects:  44% (57/129)   \u001b[K\rremote: Counting objects:  45% (59/129)   \u001b[K\rremote: Counting objects:  46% (60/129)   \u001b[K\rremote: Counting objects:  47% (61/129)   \u001b[K\rremote: Counting objects:  48% (62/129)   \u001b[K\rremote: Counting objects:  49% (64/129)   \u001b[K\rremote: Counting objects:  50% (65/129)   \u001b[K\rremote: Counting objects:  51% (66/129)   \u001b[K\rremote: Counting objects:  52% (68/129)   \rremote: Counting objects:  53% (69/129)   \u001b[K\rremote: Counting objects:  54% (70/129)   \u001b[K\rremote: Counting objects:  55% (71/129)   \u001b[K\rremote: Counting objects:  56% (73/129)   \u001b[K\rremote: Counting objects:  57% (74/129)   \u001b[K\rremote: Counting objects:  58% (75/129)   \u001b[K\rremote: Counting objects:  59% (77/129)   \u001b[K\rremote: Counting objects:  60% (78/129)   \u001b[K\rremote: Counting objects:  61% (79/129)   \u001b[K\rremote: Counting objects:  62% (80/129)   \u001b[K\rremote: Counting objects:  63% (82/129)   \u001b[K\rremote: Counting objects:  64% (83/129)   \u001b[K\rremote: Counting objects:  65% (84/129)   \u001b[K\rremote: Counting objects:  66% (86/129)   \u001b[K\rremote: Counting objects:  67% (87/129)   \u001b[K\rremote: Counting objects:  68% (88/129)   \u001b[K\rremote: Counting objects:  69% (90/129)   \u001b[K\rremote: Counting objects:  70% (91/129)   \u001b[K\rremote: Counting objects:  71% (92/129)   \u001b[K\rremote: Counting objects:  72% (93/129)   \u001b[K\rremote: Counting objects:  73% (95/129)   \u001b[K\rremote: Counting objects:  74% (96/129)   \u001b[K\rremote: Counting objects:  75% (97/129)   \u001b[K\rremote: Counting objects:  76% (99/129)   \u001b[K\rremote: Counting objects:  77% (100/129)   \u001b[K\rremote: Counting objects:  78% (101/129)   \u001b[K\rremote: Counting objects:  79% (102/129)   \u001b[K\rremote: Counting objects:  80% (104/129)   \u001b[K\rremote: Counting objects:  81% (105/129)   \u001b[K\rremote: Counting objects:  82% (106/129)   \u001b[K\rremote: Counting objects:  83% (108/129)   \u001b[K\rremote: Counting objects:  84% (109/129)   \u001b[K\rremote: Counting objects:  85% (110/129)   \u001b[K\rremote: Counting objects:  86% (111/129)   \u001b[K\rremote: Counting objects:  87% (113/129)   \u001b[K\rremote: Counting objects:  88% (114/129)   \u001b[K\rremote: Counting objects:  89% (115/129)   \u001b[K\rremote: Counting objects:  90% (117/129)   \u001b[K\rremote: Counting objects:  91% (118/129)   \u001b[K\rremote: Counting objects:  92% (119/129)   \u001b[K\rremote: Counting objects:  93% (120/129)   \u001b[K\rremote: Counting objects:  94% (122/129)   \u001b[K\rremote: Counting objects:  95% (123/129)   \u001b[K\rremote: Counting objects:  96% (124/129)   \u001b[K\rremote: Counting objects:  97% (126/129)   \u001b[K\rremote: Counting objects:  98% (127/129)   \u001b[K\rremote: Counting objects:  99% (128/129)   \u001b[K\rremote: Counting objects: 100% (129/129)   \u001b[K\rremote: Counting objects: 100% (129/129), done.\u001b[K\n","remote: Compressing objects: 100% (66/66), done.\u001b[K\n","remote: Total 129 (delta 77), reused 102 (delta 61), pack-reused 0\u001b[K\n","Receiving objects: 100% (129/129), 721.42 KiB | 2.05 MiB/s, done.\n","Resolving deltas: 100% (77/77), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"69MNF4fEi5Ly","outputId":"114ae346-1ce2-495f-9246-427f2270a6f5","executionInfo":{"status":"ok","timestamp":1559035421044,"user_tz":-60,"elapsed":2958,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"source":["# Initialize session\n","sess = tf.Session()\n","K.set_session(sess)\n","K.tensorflow_backend._get_available_gpus()\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/job:localhost/replica:0/task:0/device:GPU:0']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cc1rwgOZi9F1","colab":{}},"source":["# filename = './cleaned_data.csv'\n","# filename = './data1.csv'\n","# df = pd.read_csv(filename, index_col=0)\n","# cols = ['comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# cols = ['comment','cleaned_comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# df = df[cols]\n","\n","#score_column = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# text_col_name = 'cleaned_comment'\n","# text_col_name = 'comment'\n","\n","#--------\n","\n","### For kaggle dataset\n","\n","## Training data\n","\n","# filename = './gdrive/My Drive/Data/train.csv'\n","train_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_train.csv'\n","\n","df = pd.read_csv(train_filename)\n","cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","df = df[cols]\n","\n","score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","text_col_name = 'comment_text'\n","\n","#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n","for i in score_column:\n","  df[i] = pd.to_numeric(df[i],downcast='float')\n","\n","# over represent toxic comments\n","\n","df['num_toxic_atts'] = df[cols[1:]].apply(lambda x: np.sum(x), axis = 1)\n","\n","df_toxic = df[df.num_toxic_atts  > 0]\n","# print(len(df_toxic))\n","\n","df_healthy_sample = df[df.num_toxic_atts == 0].sample(frac=1.0)[:len(df_toxic)]\n","# print(len(df_healthy_sample))\n","\n","df_train = pd.concat([df_toxic, df_healthy_sample]).sample(frac=1.0)\n","\n","\n","## Test data:\n","\n","test_data_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_test.csv'\n","test_labels_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_test_labels.csv'\n","test_df = pd.read_csv(test_data_filename)\n","test_labels = pd.read_csv(test_labels_filename)\n","test_df = pd.merge(test_df, test_labels, on='id')\n","#get rid of -1 values\n","test_df = test_df.replace(-1, np.nan).dropna(subset=cols)\n","\n","#restrict comment length\n","test_df = test_df[test_df.comment_text.str.len() <= 250]\n","\n","test_df = test_df[cols]\n","#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n","for i in score_column:\n","  test_df[i] = pd.to_numeric(test_df[i],downcast='float')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p_JIbqhY55L_","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mUIe15uTi9Ok","outputId":"d24eed38-eaeb-42e9-9f33-f9ec03320c7e","executionInfo":{"status":"ok","timestamp":1559035451122,"user_tz":-60,"elapsed":23228,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":200}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:24:07.873405 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:24:09.987522 139661544327040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jECyqGRfyht4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IVESPLmgl98I","colab":{}},"source":["# #Pre process data for bert embedding\n","max_seq_length = 128\n","\n","import pickle\n","\n","# train_input_examples = create_examples(df_train.sample(100), score_column, text_col_name)\n","# # # test_input_examples = create_examples(df_test, score_column, text_col_name)\n","\n","# train_features = convert_examples_to_features(train_input_examples, max_seq_length, tokenizer)\n","# # # test_features = convert_examples_to_features(test_input_examples, max_seq_length, tokenizer)\n","\n","# pickle.dump(train_features, open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'wb'))\n","# # pickle.dump(test_features, open('./gdrive/My Drive/Kaggle_toxic_comments/test_features.p', 'wb'))\n","\n","# # ## Load features previously saved\n","# train_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))\n","# test_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOdj6CWiPYSX","colab_type":"code","colab":{}},"source":["# utility methods\n","from collections import namedtuple\n","NeuralProcessParams = namedtuple('NeuralProcessParams', ['dim_z', 'n_hidden_units_h', 'n_hidden_units_g'])\n","GaussianParams = namedtuple('GaussianParams', ['mu', 'sigma'])\n","\n","\n","def batch_mlp(input, inner_layer_dims, output_dim, variable_scope):\n","  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n","  \n","  Args:\n","    input: input tensor of shape [B,n,d_in].\n","    output_sizes: An iterable containing the output sizes of the MLP as defined \n","        in `basic.Linear`.\n","    variable_scope: String giving the name of the variable scope. If this is set\n","        to be the same as a previously defined MLP, then the weights are reused.\n","    \n","  Returns:\n","    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n","  \"\"\"\n","  # Get the shapes of the input and reshape to parallelise across observations\n","\n","  output = input\n","\n","  \n","  # Pass through MLP\n","  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n","    for i, size in enumerate(inner_layer_dims):\n","      output = tf.nn.relu(\n","          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n","\n","    # Last layer without a ReLu\n","    output = tf.layers.dense(output, output_dim, name=\"layer_{}\".format(i + 1))\n","\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_gK0D7AhoLK","colab_type":"code","colab":{}},"source":["# class Embedder(object):\n","  \n","#   def __init__(self, BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True):\n","#     self.BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","#     self.tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)\n","#     self.trainable = trainable\n","    \n","#   def __call__(self, input_ids, input_mask, segment_ids):\n","#     embedder = hub.Module(self.BERT_model_hub,trainable=self.trainable)\n","#     bert_inputs = dict(input_ids=input_ids,\n","#                        input_mask=input_mask, \n","#                        segment_ids=segment_ids)\n","\n","#     bert_outputs = embedder(inputs=bert_inputs,\n","#                                signature=\"tokens\", \n","#                                as_dict=True)\n","    \n","#   # Use \"pooled_output\" for classification tasks on an entire sentence. Use \"sequence_outputs\" for token-level output\n","#     return bert_outputs[\"pooled_output\"]\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HMqLmD8PTEP","colab_type":"code","colab":{}},"source":["class Decoder(object):\n","  \"\"\"The Decoder.\"\"\"\n","\n","  def __init__(self, layer_dims, num_classes):\n","    self.layer_dims = layer_dims\n","    self.num_classes = num_classes\n","    \n","  def __call__(self, input_xs_embedding, z_samples):\n","  \n","        # inputs dimensions\n","    # z_sample has dim [n_draws, dim_z]\n","    # x_star has dim [N_star, dim_x]\n","    n_draws = z_samples.get_shape().as_list()[0]\n","    n_xs = tf.shape(input_xs_embedding)[0]\n","\n","    # Repeat z samples for each x*\n","    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n","\n","    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n","    z_samples_repeat = tf.tile(z_samples, [1, n_xs, 1])\n","\n","    # Repeat x* for each z sample\n","    x_star_repeat = tf.expand_dims(input_xs_embedding, axis=0)\n","    x_star_repeat = tf.tile(x_star_repeat, [n_draws, 1, 1])\n","\n","    # Concatenate x* and z\n","    inputs = tf.concat([x_star_repeat, z_samples_repeat], axis=2)\n","\n","    # decoder mlp\n","    inner_layer_dims = self.layer_dims\n","    output_dim = self.num_classes *2\n","    hidden = batch_mlp(inputs, inner_layer_dims, output_dim, \"decoder\")\n","\n","    # Get the mean an the variance\n","    mu, log_sigma = tf.split(hidden, 2, axis= -1)\n","\n","    # Bound the variance\n","    sigma_star = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n","    mu_star = tf.math.sigmoid(mu)\n","\n","\n","    return GaussianParams(mu_star, sigma_star)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6DmGdqzaGjS","colab_type":"code","colab":{}},"source":["class Encoder(object):\n","\n","  def __init__(self, layer_dims, latent_dim):\n","    self.layer_dims = layer_dims\n","    self.latent_dim = latent_dim\n","    \n","  def __call__(self, xs, ys):\n","    print(xs)\n","    print(ys)\n","    xys = tf.concat([xs, ys], axis=1)\n","\n","\n","    # encoder mlp\n","    inner_layer_dims = self.layer_dims[:-1]\n","    output_dim = self.layer_dims[-1]\n","    rs = batch_mlp(xys, inner_layer_dims, output_dim, \"encoder\")\n","    \n","    # aggregate rs\n","    r = self._aggregate_r(rs)\n","    \n","    # get mu and sigma\n","    z_params = self._get_z_params(r)\n","    \n","    # distribution\n","    dist = tfd.MultivariateNormalDiag(loc=z_params.mu,\n","                                          scale_diag=z_params.sigma)\n","    return dist\n","    \n","  def _aggregate_r(self, context_rs: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Aggregate the output of the encoder to a single representation\n","\n","    Creates an aggregation (mean) operator to combine the encodings of multiple context inputs\n","\n","    Parameters\n","    ----------\n","    context_rs\n","        Input encodings tensor, shape: (n_samples, dim_r)\n","\n","    Returns\n","    -------\n","        Output tensor of aggregation result\n","    \"\"\"\n","    mean = tf.reduce_mean(context_rs, axis=0)\n","    r = tf.reshape(mean, [1, -1])\n","    return r\n","  \n","  def _get_z_params(self, context_r: tf.Tensor) -> GaussianParams:\n","    \"\"\"Map encoding to mean and covariance of the random variable Z\n","\n","    Creates a linear dense layer to map encoding to mu_z, and another linear mapping + a softplus activation for Sigma_z\n","\n","    Parameters\n","    ----------\n","    context_r\n","        Input encoding tensor, shape: (1, dim_r)\n","    params\n","        Neural process parameters\n","\n","    Returns\n","    -------\n","        Output tensors of the mappings for mu_z and Sigma_z\n","    \"\"\"\n","    hidden = context_r\n","    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n","      # First apply intermediate relu layer \n","      hidden = tf.nn.relu(\n","          tf.layers.dense(hidden, \n","                          (self.layer_dims[-1] + self.latent_dim)/2, \n","                          name=\"penultimate_layer\"))\n","      \n","      # Then apply further linear layers to output latent mu and log sigma\n","      mu = tf.layers.dense(hidden, self.latent_dim, name=\"mean_layer\")\n","      log_sigma = tf.layers.dense(hidden, self.latent_dim, name=\"std_layer\")\n","      \n","\n","    # Compute sigma\n","    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n","\n","    return GaussianParams(mu, sigma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWZZAs1haFkc","colab_type":"code","colab":{}},"source":["class NLP_NeuralProcess(object):\n","  \n","  def __init__(self,\n","                  score_col, \n","                  params = NeuralProcessParams(dim_z=20, \n","                                                     n_hidden_units_h=[128, 128, 128], \n","                                                     n_hidden_units_g=[128, 128, 128]),\n","                  num_classes = 6, \n","                  num_draws = 2, \n","                  lr = 2e-5,\n","                  batch_size = 32, \n","                  num_warmup_steps=100,\n","                  num_train_steps = 10**3,\n","                  save_summary_steps = 100,\n","                  save_checkpoints_steps = 500,\n","                  output_dir = \"./test_output\",\n","                  context_features = None\n","                 ):\n","    \n","    self.params = params\n","    self.encoder = Encoder(layer_dims = self.params.n_hidden_units_h, \n","                           latent_dim=self.params.dim_z)\n","    self.decoder = Decoder(layer_dims= self.params.n_hidden_units_g, \n","                           num_classes=num_classes)\n","    self.num_draws = num_draws\n","    self.num_classes = num_classes\n","#     self.estimator = None\n","    #self.embedder = Embedder()\n","    #####  \n","    num_labels = len(score_col)\n","    \n","    \n","    # Specify outpit directory and number of checkpoint steps to save\n","    \n","    run_config = tf.estimator.RunConfig(model_dir=output_dir,\n","          save_summary_steps=save_summary_steps, save_checkpoints_steps=save_checkpoints_steps)\n","    \n","    model_fn = self.model_fn_builder(num_labels = num_labels, learning_rate=lr,\n","      num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n","    \n","    if context_features is not None:\n","      estimator_params = {\"batch_size\": batch_size, \"context_features\": context_features}\n","      self.estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n","                                              params=estimator_params)\n","    \n","    else: \n","      self.estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n","            params={\"batch_size\": batch_size})\n","    \n","    \n","  def create_model(self, \n","                   target_input_ids, \n","                   target_input_mask, \n","                   target_segment_ids, \n","                   target_scores=None,\n","                   context_input_ids = None, \n","                   context_input_mask = None, \n","                   context_segment_ids= None, \n","                   context_scores = None\n","                   ):\n","    \n","    # apply embedder\n","    embedder = hub.Module(BERT_model_hub,trainable=True)\n","    \n","    valid_context = (context_input_ids is not None) & (context_input_mask is not None) & (context_segment_ids is not None) & (context_scores is not None)\n","    \n","    # target processing - all scenarios\n","    target_inputs = dict(input_ids=target_input_ids,\n","                     input_mask=target_input_mask, \n","                     segment_ids=target_segment_ids)\n","    target_embeddings = embedder(inputs=target_inputs,\n","                               signature=\"tokens\", \n","                               as_dict=True)\n","    target_xs = target_embeddings[\"pooled_output\"]\n","    \n","    \n","    if valid_context:\n","      \n","      # context processing - training\n","      context_inputs = dict(input_ids=context_input_ids,\n","                         input_mask=context_input_mask, \n","                         segment_ids=context_segment_ids)\n","      context_embeddings = embedder(inputs=context_inputs,\n","                                 signature=\"tokens\", \n","                                 as_dict=True)\n","      context_xs = context_embeddings[\"pooled_output\"]\n","      context_ys = context_scores\n","      # total x,y \n","      x_all = tf.concat([context_xs, target_xs], axis=0)\n","      \n","      # get encoding params with context\n","      context_z_dist = self.encoder(context_xs, context_ys)\n","      # predictions with context\n","      posterior_pred = self.decoder(target_xs, context_z_dist.sample(self.num_draws))\n","        \n","       # target scores - context training / evaluation\n","      if target_scores is not None:\n","        target_ys = target_scores\n","        y_all = tf.concat([context_ys, target_ys], axis=0)\n","        all_z_dist = self.encoder(x_all, y_all)\n","        \n","        # loss\n","        loglike = self.loglikelihood(target_ys, posterior_pred)\n","        KL_loss = self.KLqp_gaussian(all_z_dist.parameters['loc'], \n","                                     all_z_dist.parameters['scale_diag'], \n","                                     context_z_dist.parameters['loc'], \n","                                     context_z_dist.parameters['scale_diag'])\n","        loss = tf.negative(loglike) + KL_loss\n","        # context and training / evaluation\n","        return (loss, posterior_pred, target_ys)\n","      \n","      # context prediction\n","      return  (None, posterior_pred, None)\n","    \n","    # no context\n","    else:\n","      x_all = target_xs \n","      # get internal representation\n","      mean_zero = tf.constant(np.repeat(0., params.dim_z))\n","      epsilon_dist = tfd.MultivariateNormalDiag(loc= mean_zero)                            \n","      epsilon = tf.expand_dims(epsilon_dist.sample(self.num_draws), axis=1)\n","      epsilon = tf.cast(epsilon, tf.float32)\n","      prior_predict = self.decoder(x_all, epsilon)\n","      \n","      # target scores - no context training / evaluation\n","      if target_scores is not None:\n","        target_ys = target_scores\n","        loglike = self.loglikelihood(target_ys, prior_predict)\n","        loss = tf.negative(loglike)\n","        # no context/ training / evaluation\n","        return  (loss, prior_predict, target_ys)\n","   \n","    \n","      # no context prediction\n","      return  (None, prior_predict, None)\n","\n","  \n","  def loglikelihood(self, y_star: tf.Tensor, dist):\n","    \"\"\"Log-likelihood of an output given a predicted \"\"\"\n","    p_normal = tfd.MultivariateNormalDiag(loc = dist.mu, scale_diag=dist.sigma)\n","    loglike = p_normal.log_prob(y_star)\n","    loglike = tf.reduce_sum(loglike, axis=0)\n","    loglike = tf.reduce_mean(loglike)\n","    return loglike\n","  \n","  def KLqp_gaussian(self, mu_q: tf.Tensor, sigma_q: tf.Tensor, mu_p: tf.Tensor, sigma_p: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Kullback-Leibler divergence between two Gaussian distributions\n","\n","    Determines KL(q || p) = < log( q / p ) >_q\n","\n","    Parameters\n","    ----------\n","    mu_q\n","        Mean tensor of distribution q, shape: (1, dim)\n","    sigma_q\n","        Variance tensor of distribution q, shape: (1, dim)\n","    mu_p\n","        Mean tensor of distribution p, shape: (1, dim)\n","    sigma_p\n","        Variance tensor of distribution p, shape: (1, dim)\n","\n","    Returns\n","    -------\n","        KL tensor, shape: (1)\n","    \"\"\"\n","    sigma2_q = tf.square(sigma_q) + 1e-16\n","    sigma2_p = tf.square(sigma_p) + 1e-16\n","    temp = sigma2_q / sigma2_p + tf.square(mu_q - mu_p) / sigma2_p - 1.0 + tf.log(sigma2_p / sigma2_q + 1e-16)\n","    return 0.5 * tf.reduce_sum(temp)\n","  \n","  def context_target_split(self, batch_size =32):\n","    btch_sz = batch_size\n","    n_context = tf.random_shuffle(tf.range(1,btch_sz))[0]\n","    \n","    indices = tf.range(0, btch_sz)\n","    context_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context))\n","    target_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context, btch_sz))\n","    \n","    return context_set_indices, target_set_indices\n","    \n","  def model_fn_builder(self, num_labels, learning_rate, num_train_steps, num_warmup_steps):\n","      \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","      \n","      \n","      def model_fn(features, mode, params):  # pylint: disable=unused-argument\n","          \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","          \n","          # run model\n","          # -------------------------------------------------------------------------------------------\n","          target_input_ids = None\n","          target_input_mask = None\n","          target_segment_ids = None \n","          target_scores = None\n","          \n","          context_input_ids = None \n","          context_input_mask = None \n","          context_segment_ids = None\n","          context_scores = None\n","          \n","          # training \n","          if mode == tf.estimator.ModeKeys.TRAIN:\n","            input_ids = features[\"input_ids\"]\n","            input_mask = features[\"input_mask\"]\n","            segment_ids = features[\"segment_ids\"]    \n","            scores = features[\"scores\"]\n","            \n","            # context split\n","            context_set_indices, target_set_indices= self.context_target_split(batch_size =32)\n","            \n","            target_input_ids = tf.gather(input_ids,target_set_indices)\n","            target_input_mask = tf.gather(input_mask,target_set_indices)\n","            target_segment_ids = tf.gather(segment_ids,target_set_indices) \n","            target_scores = tf.gather(scores,target_set_indices)\n","\n","            context_input_ids = tf.gather(input_ids,context_set_indices) \n","            context_input_mask = tf.gather(input_mask,context_set_indices) \n","            context_segment_ids = tf.gather(segment_ids,context_set_indices)\n","            context_scores = tf.gather(scores,context_set_indices)\n","            \n","            \n","          elif mode == tf.estimator.ModeKeys.PREDICT:\n","            print('Prediction')\n","            target_input_ids = features[\"input_ids\"]\n","            target_input_mask = features[\"input_mask\"]\n","            target_segment_ids = features[\"segment_ids\"]    \n","            target_scores = features[\"scores\"]\n","            \n","            try:\n","              context_input_ids = features[\"supplied_context_input_ids\"]\n","              context_input_mask = features[\"supplied_context_input_mask\"]\n","              context_segment_ids = features[\"supplied_context_segment_ids\"] \n","              context_scores = features[\"supplied_context_scores\"]\n","               \n","            except:\n","              print(\"****No context supplied ****\")\n","              context_input_ids = None\n","              context_input_mask = None\n","              context_segment_ids = None\n","              context_scores = None\n","            \n","          else:\n","            print('Evaluation')\n","            target_input_ids = features[\"input_ids\"]\n","            target_input_mask = features[\"input_mask\"]\n","            target_segment_ids = features[\"segment_ids\"]    \n","            target_scores = features[\"scores\"]\n","\n","            try:\n","              context_input_ids = features[\"supplied_context_input_ids\"]\n","              context_input_mask = features[\"supplied_context_input_mask\"]\n","              context_segment_ids = features[\"supplied_context_segment_ids\"] \n","              context_scores = features[\"supplied_context_scores\"]\n","               \n","            except:\n","              print(\"****No context supplied ****\")\n","              context_input_ids = None\n","              context_input_mask = None\n","              context_segment_ids = None\n","              context_scores = None\n","\n","\n","          (loss, prediction, true_y) = self.create_model(target_input_ids, \n","                                                         target_input_mask, \n","                                                         target_segment_ids, \n","                                                         target_scores,\n","                                                         context_input_ids, \n","                                                         context_input_mask, \n","                                                         context_segment_ids, \n","                                                         context_scores)\n","\n","          train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","          ystar, _ = tf.nn.moments(prediction.mu,[0])\n","          variance, _ = tf.nn.moments(tf.math.square(prediction.sigma),[0])\n","          \n","          # Calculate evaluation metrics\n","          eval_metrics = {}\n","          # AUC\n","          def metric_fn(pred_scores, real_scores, trait_num):\n","              auc_value = tf.metrics.auc(real_scores[:,trait_num], pred_scores[:,trait_num])\n","              accuracy_value = tf.metrics.accuracy(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n","              return {\"auc\"+str(trait_num): auc_value, \"accuracy\"+str(trait_num): accuracy_value}\n","\n","          labels = true_y # need to round them if true labels are not 1 or 0\n","          eval_metrics_lst = [metric_fn(ystar, labels, trait_num) for trait_num in range(num_labels)]\n","\n","          for d in eval_metrics_lst:\n","              tf.summary.scalar(list(d.keys())[0], list(d.values())[0][1])  # make available to tensorboard\n","              tf.summary.scalar(list(d.keys())[1], list(d.values())[1][1])  # make available to tensorboard\n","              eval_metrics.update(d)\n","              \n","          \n","          # output from model\n","          # -------------------------------------------------------------------------------------------\n","\n","          # training \n","          if mode == tf.estimator.ModeKeys.TRAIN:\n","              return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","          # prediction\n","          elif mode == tf.estimator.ModeKeys.PREDICT:\n","              return tf.estimator.EstimatorSpec(mode=mode, predictions={'prediction_mean': ystar, 'prediction_var': variance})\n","\n","          # evaluation\n","          else:\n","            return tf.estimator.EstimatorSpec(mode=mode,loss=loss, eval_metric_ops=eval_metrics)\n","\n","      return model_fn\n","  \n","#   def model_fn_builder(self, num_labels, learning_rate, num_train_steps, num_warmup_steps):\n","#       \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","      \n","      \n","#       def model_fn(features, mode, params):  # pylint: disable=unused-argument\n","#           \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","          \n","#           # run model\n","#           # -------------------------------------------------------------------------------------------\n","#           target_input_ids = None\n","#           target_input_mask = None\n","#           target_segment_ids = None \n","#           target_scores = None\n","          \n","#           context_input_ids = None \n","#           context_input_mask = None \n","#           context_segment_ids = None\n","#           context_scores = None\n","          \n","#           # training \n","#           if mode == tf.estimator.ModeKeys.TRAIN:\n","#             input_ids = features[\"input_ids\"]\n","#             input_mask = features[\"input_mask\"]\n","#             segment_ids = features[\"segment_ids\"]    \n","#             scores = features[\"scores\"]\n","            \n","#             # context split\n","#             context_set_indices, target_set_indices= self.context_target_split(batch_size =32)\n","            \n","#             target_input_ids = tf.gather(input_ids,target_set_indices)\n","#             target_input_mask = tf.gather(input_mask,target_set_indices)\n","#             target_segment_ids = tf.gather(segment_ids,target_set_indices) \n","#             target_scores = tf.gather(scores,target_set_indices)\n","\n","#             context_input_ids = tf.gather(input_ids,context_set_indices) \n","#             context_input_mask = tf.gather(input_mask,context_set_indices) \n","#             context_segment_ids = tf.gather(segment_ids,context_set_indices)\n","#             context_scores = tf.gather(scores,context_set_indices)\n","            \n","            \n","#           elif mode == tf.estimator.ModeKeys.PREDICT:\n","#             print('Prediction')\n","#             target_input_ids = features[\"input_ids\"]\n","#             target_input_mask = features[\"input_mask\"]\n","#             target_segment_ids = features[\"segment_ids\"]    \n","#             target_scores = features[\"scores\"]\n","            \n","#             try:\n","#               context_input_ids = []\n","#               context_input_mask = []\n","#               context_segment_ids = []\n","#               context_scores = []  \n","#               num_context = len(params[\"context_features\"])\n","              \n","#               for feature in params[\"context_features\"]:\n","#                 context_input_ids.append(feature.input_ids)\n","#                 context_input_mask.append(feature.input_mask)\n","#                 context_segment_ids.append(feature.segment_ids)\n","#                 context_scores.append(feature.score)\n","              \n","#               context_input_ids = tf.constant(context_input_ids, shape=[num_context, 128],\n","# \t\t\t\t\t\tdtype=tf.int32)\n","              \n","#               context_input_mask = tf.constant(context_input_mask, shape=[num_context, 128],\n","# \t\t\t\t\t\tdtype=tf.int32)\n","#               context_segment_ids = tf.constant(context_segment_ids, shape=[num_context, 128],\n","# \t\t\t\t\t\tdtype=tf.int32)\n","#               context_scores = tf.constant(context_scores, shape=[num_context, self.num_classes])\n","              \n","#             except:\n","#               print(\"****No context supplied ****\")\n","#               context_input_ids = None\n","#               context_input_mask = None\n","#               context_segment_ids = None\n","#               context_scores = None\n","            \n","#           else:\n","#             print('Evaluation')\n","#             target_input_ids = features[\"input_ids\"]\n","#             target_input_mask = features[\"input_mask\"]\n","#             target_segment_ids = features[\"segment_ids\"]    \n","#             target_scores = features[\"scores\"]\n","\n","#             try:\n","#               context_input_ids = features[\"supplied_context_input_ids\"]\n","#               context_input_mask = features[\"supplied_context_input_mask\"]\n","#               context_segment_ids = features[\"supplied_context_segment_ids\"] \n","#               context_scores = features[\"supplied_context_scores\"]\n","               \n","#             except:\n","#               print(\"****No context supplied ****\")\n","#               context_input_ids = None\n","#               context_input_mask = None\n","#               context_segment_ids = None\n","#               context_scores = None\n","\n","\n","#           (loss, prediction, true_y) = self.create_model(target_input_ids, \n","#                                                          target_input_mask, \n","#                                                          target_segment_ids, \n","#                                                          target_scores,\n","#                                                          context_input_ids, \n","#                                                          context_input_mask, \n","#                                                          context_segment_ids, \n","#                                                          context_scores)\n","\n","#           train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","#           ystar, _ = tf.nn.moments(prediction.mu,[0])\n","#           variance, _ = tf.nn.moments(tf.math.square(prediction.sigma),[0])\n","          \n","          \n","#           # output from model\n","#           # -------------------------------------------------------------------------------------------\n","\n","#           # training \n","#           if mode == tf.estimator.ModeKeys.TRAIN:\n","#               return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","#           # prediction\n","#           elif mode == tf.estimator.ModeKeys.PREDICT:\n","#               return tf.estimator.EstimatorSpec(mode=mode, predictions={'prediction_mean': ystar, 'prediction_var': variance})\n","\n","#           # evaluation\n","#           else:\n","#             # Calculate evaluation metrics.\n","#             eval_metrics = {}\n","\n","#             # AUC\n","#             def metric_fn(pred_scores, real_scores, trait_num):\n","#                 auc_value = tf.metrics.auc(real_scores[:,trait_num], pred_scores[:,trait_num])\n","#                 accuracy_value = tf.metrics.accuracy(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n","#                 return {\"auc\"+str(trait_num): auc_value, \"accuracy\"+str(trait_num): accuracy_value}\n","\n","#             labels = true_y # need to round them if true labels are not 1 or 0\n","#             eval_metrics_lst = [metric_fn(ystar, labels, trait_num) for trait_num in range(num_labels)]\n","\n","#             for d in eval_metrics_lst:\n","#                 tf.summary.scalar(list(d.keys())[0], list(d.values())[0][1])  # make available to tensorboard\n","#                 tf.summary.scalar(list(d.keys())[1], list(d.values())[1][1])  # make available to tensorboard\n","#                 eval_metrics.update(d)\n","#             return tf.estimator.EstimatorSpec(mode=mode,loss=loss, eval_metric_ops=eval_metrics)\n","\n","#       return model_fn\n","\n","  def prepare_examples(self, df, score_column, text_col_name, supplied_context_df=None):\n","      num_labels = len(score_column)\n","      input_examples = create_examples(df, score_column, text_col_name)\n","      input_features = convert_examples_to_features(input_examples, max_seq_length, tokenizer)\n","      \n","      if supplied_context_df is not None:\n","        supplied_context_examples = create_examples(supplied_context_df, score_column, text_col_name)\n","        supplied_context_features = convert_examples_to_features(supplied_context_examples, max_seq_length, tokenizer)\n","        input_fn = input_fn_builder(\n","          features=input_features, seq_length=max_seq_length, \n","          num_labels = num_labels, is_training=True, drop_remainder=False,\n","          supplied_context_features = supplied_context_features)\n","      \n","      else:\n","        input_fn = input_fn_builder(\n","          features=input_features, seq_length=max_seq_length, \n","          num_labels = num_labels, is_training=True, drop_remainder=False)\n","        \n","      return input_fn\n","  \n","  def predict(self, \n","            df, \n","            score_col, \n","            text_col,\n","            supplied_context_df=None\n","             ):\n","    \n","    pred_input_fn = self.prepare_examples(df, score_col, text_col, supplied_context_df)\n","    preds = self.estimator.predict(input_fn=pred_input_fn)\n","    \n","    return preds\n","  \n","  def evaluate(self, \n","               eval_steps,\n","               df, \n","               score_col, \n","               text_col,\n","               supplied_context_df=None):\n","    \n","    eval_input_fn = self.prepare_examples(df, score_col, text_col, supplied_context_df)\n","    \n","    result = self.estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","    return result\n","  \n","  def train(self,num_train_steps,\n","            df_train, \n","            score_col, \n","            text_col,\n","           ):\n","\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = self.prepare_examples(df_train, score_col, text_col)\n","\n","    print('Beginning Training!')\n","    current_time = datetime.now()\n","    self.estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","    print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8iSEczc6NS1","colab_type":"code","outputId":"55d9fc35-6165-4a0c-b183-665c23d32a1b","executionInfo":{"status":"ok","timestamp":1559046225627,"user_tz":-60,"elapsed":22,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":9059}},"source":["\n","# tf.reset_default_graph()\n","\n","score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] \n","text_col_name = 'comment_text'\n","\n","params = NeuralProcessParams(dim_z=1000, n_hidden_units_h=[512, 256, 128], n_hidden_units_g=[512, 256, 128])\n","num_train_steps = 12*(10**3)\n","\n","neural_process = NLP_NeuralProcess(score_col=score_column, params = params, num_draws = 20, \n","                                   num_train_steps=num_train_steps,\n","                                   output_dir = \"./gdrive/My Drive/Kaggle_toxic_comments/test_output\",\n","                                  context_features = None)\n","\n","### Train:\n","\n","neural_process.train(df_train=df_train, \n","                     score_col= score_column, \n","                     text_col=text_col_name, \n","                     num_train_steps=num_train_steps) "],"execution_count":17,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': './gdrive/My Drive/Kaggle_toxic_comments/test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f04d511cb38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:25:49.777951 139661544327040 estimator.py:201] Using config: {'_model_dir': './gdrive/My Drive/Kaggle_toxic_comments/test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f04d511cb38>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"},{"output_type":"stream","text":["Beginning Training!\n","INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:26:54.965257 139661544327040 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:26:58.537780 139661544327040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:26:59.231843 139661544327040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"GatherV2_9:0\", shape=(?, 6), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-13-504275df30f7>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:26:59.367227 139661544327040 deprecation.py:323] From <ipython-input-13-504275df30f7>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:27:00.226061 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:27:00.356963 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:27:12.926330 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:14.161494 139661544327040 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:14.165546 139661544327040 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:18.945141 139661544327040 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:27:18.956576 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-1500\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:18.963002 139661544327040 saver.py:1270] Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-1500\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:27:36.520728 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1070: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file utilities to get mtimes.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:37.074708 139661544327040 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:27:37.362774 139661544327040 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 1500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:28:00.472533 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 1500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -65.8425, step = 1500\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:28:34.682662 139661544327040 basic_session_run_hooks.py:249] loss = -65.8425, step = 1500\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.890283\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:30:27.005620 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.890283\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -52.19886, step = 1601 (112.327 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:30:27.009183 139661544327040 basic_session_run_hooks.py:247] loss = -52.19886, step = 1601 (112.327 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10856\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:31:57.212742 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10856\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -144.2867, step = 1700 (90.209 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:31:57.217746 139661544327040 basic_session_run_hooks.py:247] loss = -144.2867, step = 1700 (90.209 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11375\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:33:27.000125 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11375\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -66.59814, step = 1800 (89.788 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:33:27.005464 139661544327040 basic_session_run_hooks.py:247] loss = -66.59814, step = 1800 (89.788 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11449\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:34:56.726835 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11449\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -102.09718, step = 1900 (89.724 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:34:56.728983 139661544327040 basic_session_run_hooks.py:247] loss = -102.09718, step = 1900 (89.724 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:36:25.601591 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 2000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.985117\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:36:38.237603 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.985117\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -120.034004, step = 2000 (101.513 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:36:38.241957 139661544327040 basic_session_run_hooks.py:247] loss = -120.034004, step = 2000 (101.513 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11073\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:38:08.268624 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11073\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -134.6275, step = 2100 (90.029 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:38:08.270903 139661544327040 basic_session_run_hooks.py:247] loss = -134.6275, step = 2100 (90.029 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11279\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:39:38.132551 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11279\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -51.361156, step = 2200 (89.866 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:39:38.137176 139661544327040 basic_session_run_hooks.py:247] loss = -51.361156, step = 2200 (89.866 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1117\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:41:08.085144 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1117\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -112.885, step = 2300 (89.952 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:41:08.088980 139661544327040 basic_session_run_hooks.py:247] loss = -112.885, step = 2300 (89.952 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11085\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:42:38.106672 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11085\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -71.82333, step = 2400 (90.022 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:42:38.110690 139661544327040 basic_session_run_hooks.py:247] loss = -71.82333, step = 2400 (90.022 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 2500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:44:06.677677 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 2500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"],"name":"stdout"},{"output_type":"stream","text":["W0528 09:44:16.364090 139661544327040 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.982004\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:44:19.939211 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.982004\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -118.25233, step = 2500 (101.832 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:44:19.943028 139661544327040 basic_session_run_hooks.py:247] loss = -118.25233, step = 2500 (101.832 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10937\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:45:50.080646 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10937\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -130.05304, step = 2600 (90.142 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:45:50.085355 139661544327040 basic_session_run_hooks.py:247] loss = -130.05304, step = 2600 (90.142 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11366\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:47:19.874920 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11366\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -110.032684, step = 2700 (89.795 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:47:19.880892 139661544327040 basic_session_run_hooks.py:247] loss = -110.032684, step = 2700 (89.795 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11728\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:48:49.378166 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11728\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -122.87558, step = 2800 (89.504 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:48:49.384879 139661544327040 basic_session_run_hooks.py:247] loss = -122.87558, step = 2800 (89.504 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10988\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:50:19.477663 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10988\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -97.22847, step = 2900 (90.096 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:50:19.480032 139661544327040 basic_session_run_hooks.py:247] loss = -97.22847, step = 2900 (90.096 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:51:48.303384 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 3000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.981466\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:52:01.366076 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.981466\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -128.05739, step = 3000 (101.889 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:52:01.368594 139661544327040 basic_session_run_hooks.py:247] loss = -128.05739, step = 3000 (101.889 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11017\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:53:31.442561 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11017\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -103.23041, step = 3100 (90.077 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:53:31.445299 139661544327040 basic_session_run_hooks.py:247] loss = -103.23041, step = 3100 (90.077 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11363\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:55:01.239064 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11363\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -56.878845, step = 3200 (89.798 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:55:01.243394 139661544327040 basic_session_run_hooks.py:247] loss = -56.878845, step = 3200 (89.798 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11597\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:56:30.847531 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11597\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -10.755559, step = 3300 (89.607 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:56:30.850398 139661544327040 basic_session_run_hooks.py:247] loss = -10.755559, step = 3300 (89.607 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11543\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:58:00.498705 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11543\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -84.07802, step = 3400 (89.651 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:58:00.501162 139661544327040 basic_session_run_hooks.py:247] loss = -84.07802, step = 3400 (89.651 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 3500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:59:29.367573 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 3500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.982728\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:59:42.256284 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.982728\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -3.980772, step = 3500 (101.761 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 09:59:42.261707 139661544327040 basic_session_run_hooks.py:247] loss = -3.980772, step = 3500 (101.761 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11051\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:01:12.304738 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11051\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -142.72867, step = 3600 (90.046 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:01:12.307589 139661544327040 basic_session_run_hooks.py:247] loss = -142.72867, step = 3600 (90.046 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11555\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:02:41.946856 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11555\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -107.0373, step = 3700 (89.647 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:02:41.954356 139661544327040 basic_session_run_hooks.py:247] loss = -107.0373, step = 3700 (89.647 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11824\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:04:11.373363 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11824\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -71.989555, step = 3800 (89.421 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:04:11.375798 139661544327040 basic_session_run_hooks.py:247] loss = -71.989555, step = 3800 (89.421 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11275\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:05:41.240414 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11275\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -82.921326, step = 3900 (89.872 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:05:41.247473 139661544327040 basic_session_run_hooks.py:247] loss = -82.921326, step = 3900 (89.872 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:07:10.070172 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 4000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.984554\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:07:22.809209 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.984554\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -4.4062443, step = 4000 (101.570 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:07:22.817845 139661544327040 basic_session_run_hooks.py:247] loss = -4.4062443, step = 4000 (101.570 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1097\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:08:52.923372 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1097\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -125.8131, step = 4100 (90.113 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:08:52.930817 139661544327040 basic_session_run_hooks.py:247] loss = -125.8131, step = 4100 (90.113 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1134\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:10:22.737999 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1134\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -110.30045, step = 4200 (89.813 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:10:22.743921 139661544327040 basic_session_run_hooks.py:247] loss = -110.30045, step = 4200 (89.813 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1176\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:11:52.215548 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1176\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -14.426718, step = 4300 (89.479 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:11:52.222649 139661544327040 basic_session_run_hooks.py:247] loss = -14.426718, step = 4300 (89.479 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11139\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:13:22.192816 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11139\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -38.434326, step = 4400 (89.977 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:13:22.199353 139661544327040 basic_session_run_hooks.py:247] loss = -38.434326, step = 4400 (89.977 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 4500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:14:51.192003 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 4500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.985225\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:15:03.692540 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.985225\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -101.54626, step = 4500 (101.499 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:15:03.698816 139661544327040 basic_session_run_hooks.py:247] loss = -101.54626, step = 4500 (101.499 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10914\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:16:33.852555 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10914\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -92.14984, step = 4600 (90.162 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:16:33.860643 139661544327040 basic_session_run_hooks.py:247] loss = -92.14984, step = 4600 (90.162 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11623\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:18:03.440166 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11623\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -78.85983, step = 4700 (89.595 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:18:03.455600 139661544327040 basic_session_run_hooks.py:247] loss = -78.85983, step = 4700 (89.595 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11456\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:19:33.161453 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11456\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -82.85824, step = 4800 (89.712 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:19:33.167617 139661544327040 basic_session_run_hooks.py:247] loss = -82.85824, step = 4800 (89.712 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11389\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:21:02.936934 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11389\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -34.826183, step = 4900 (89.771 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:21:02.938947 139661544327040 basic_session_run_hooks.py:247] loss = -34.826183, step = 4900 (89.771 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:22:31.764918 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 5000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.984696\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:22:44.491097 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.984696\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -66.04384, step = 5000 (101.554 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:22:44.493155 139661544327040 basic_session_run_hooks.py:247] loss = -66.04384, step = 5000 (101.554 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11043\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:24:14.546023 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11043\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -30.460026, step = 5100 (90.059 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:24:14.552557 139661544327040 basic_session_run_hooks.py:247] loss = -30.460026, step = 5100 (90.059 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1148\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:25:44.247873 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1148\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -98.06426, step = 5200 (89.698 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:25:44.250241 139661544327040 basic_session_run_hooks.py:247] loss = -98.06426, step = 5200 (89.698 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11638\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:27:13.823101 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11638\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -136.79991, step = 5300 (89.575 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:27:13.825074 139661544327040 basic_session_run_hooks.py:247] loss = -136.79991, step = 5300 (89.575 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11287\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:28:43.681240 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11287\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -92.50066, step = 5400 (89.860 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:28:43.684635 139661544327040 basic_session_run_hooks.py:247] loss = -92.50066, step = 5400 (89.860 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 5500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:30:12.421479 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 5500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.986508\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:30:25.048888 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.986508\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -77.42014, step = 5500 (101.368 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:30:25.053068 139661544327040 basic_session_run_hooks.py:247] loss = -77.42014, step = 5500 (101.368 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11022\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:31:55.121043 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11022\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -131.1599, step = 5600 (90.070 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:31:55.123337 139661544327040 basic_session_run_hooks.py:247] loss = -131.1599, step = 5600 (90.070 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11622\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:33:24.709417 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11622\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -116.518486, step = 5700 (89.589 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:33:24.712613 139661544327040 basic_session_run_hooks.py:247] loss = -116.518486, step = 5700 (89.589 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11565\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:34:54.343470 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11565\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 90.36782, step = 5800 (89.635 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:34:54.347789 139661544327040 basic_session_run_hooks.py:247] loss = 90.36782, step = 5800 (89.635 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11427\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:36:24.088381 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11427\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -99.98319, step = 5900 (89.746 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:36:24.093971 139661544327040 basic_session_run_hooks.py:247] loss = -99.98319, step = 5900 (89.746 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 6000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:37:52.818075 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 6000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.988094\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:38:05.293329 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.988094\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -134.19981, step = 6000 (101.202 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:38:05.295879 139661544327040 basic_session_run_hooks.py:247] loss = -134.19981, step = 6000 (101.202 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11127\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:39:35.280176 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11127\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -103.75964, step = 6100 (89.987 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:39:35.283284 139661544327040 basic_session_run_hooks.py:247] loss = -103.75964, step = 6100 (89.987 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11469\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:41:04.990799 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11469\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -116.744705, step = 6200 (89.712 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:41:04.995526 139661544327040 basic_session_run_hooks.py:247] loss = -116.744705, step = 6200 (89.712 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11386\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:42:34.768611 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11386\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -57.306274, step = 6300 (89.779 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:42:34.774450 139661544327040 basic_session_run_hooks.py:247] loss = -57.306274, step = 6300 (89.779 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11378\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:44:04.552806 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11378\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -144.36578, step = 6400 (89.786 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:44:04.560370 139661544327040 basic_session_run_hooks.py:247] loss = -144.36578, step = 6400 (89.786 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 6500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:45:33.227509 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 6500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.983606\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:45:46.219501 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.983606\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -114.82296, step = 6500 (101.667 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:45:46.227583 139661544327040 basic_session_run_hooks.py:247] loss = -114.82296, step = 6500 (101.667 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10856\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:47:16.426288 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10856\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -108.93121, step = 6600 (90.202 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:47:16.429145 139661544327040 basic_session_run_hooks.py:247] loss = -108.93121, step = 6600 (90.202 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11598\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:48:46.033643 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11598\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -138.91382, step = 6700 (89.607 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:48:46.036134 139661544327040 basic_session_run_hooks.py:247] loss = -138.91382, step = 6700 (89.607 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11607\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:50:15.633852 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11607\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -119.62003, step = 6800 (89.607 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:50:15.643492 139661544327040 basic_session_run_hooks.py:247] loss = -119.62003, step = 6800 (89.607 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11282\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:51:45.495719 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11282\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -125.410706, step = 6900 (89.857 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:51:45.500795 139661544327040 basic_session_run_hooks.py:247] loss = -125.410706, step = 6900 (89.857 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 7000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:53:14.090039 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 7000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.988644\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:53:26.644324 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.988644\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -116.08174, step = 7000 (101.148 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:53:26.648843 139661544327040 basic_session_run_hooks.py:247] loss = -116.08174, step = 7000 (101.148 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10992\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:54:56.741268 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10992\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -30.42204, step = 7100 (90.097 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:54:56.745756 139661544327040 basic_session_run_hooks.py:247] loss = -30.42204, step = 7100 (90.097 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11664\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:56:26.295945 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11664\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -166.0299, step = 7200 (89.557 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:56:26.302953 139661544327040 basic_session_run_hooks.py:247] loss = -166.0299, step = 7200 (89.557 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11282\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:57:56.157717 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11282\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -94.395874, step = 7300 (89.862 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:57:56.164780 139661544327040 basic_session_run_hooks.py:247] loss = -94.395874, step = 7300 (89.862 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11529\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:59:25.820628 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11529\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -137.96288, step = 7400 (89.661 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 10:59:25.825858 139661544327040 basic_session_run_hooks.py:247] loss = -137.96288, step = 7400 (89.661 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 7500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:00:54.654730 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 7500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.981886\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:01:07.665412 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.981886\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -144.90775, step = 7500 (101.845 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:01:07.670668 139661544327040 basic_session_run_hooks.py:247] loss = -144.90775, step = 7500 (101.845 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10964\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:02:37.784455 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10964\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -106.26479, step = 7600 (90.121 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:02:37.791638 139661544327040 basic_session_run_hooks.py:247] loss = -106.26479, step = 7600 (90.121 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11465\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:04:07.499039 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11465\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -130.76143, step = 7700 (89.715 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:04:07.506284 139661544327040 basic_session_run_hooks.py:247] loss = -130.76143, step = 7700 (89.715 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11754\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:05:36.980891 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11754\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -106.13468, step = 7800 (89.481 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:05:36.987371 139661544327040 basic_session_run_hooks.py:247] loss = -106.13468, step = 7800 (89.481 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11257\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:07:06.862681 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11257\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -138.49039, step = 7900 (89.877 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:07:06.864772 139661544327040 basic_session_run_hooks.py:247] loss = -138.49039, step = 7900 (89.877 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 8000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:08:35.728028 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 8000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.985188\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:08:48.366153 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.985188\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -104.1745, step = 8000 (101.506 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:08:48.370871 139661544327040 basic_session_run_hooks.py:247] loss = -104.1745, step = 8000 (101.506 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11002\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:10:18.454475 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11002\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -162.50763, step = 8100 (90.091 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:10:18.462129 139661544327040 basic_session_run_hooks.py:247] loss = -162.50763, step = 8100 (90.091 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11571\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:11:48.083825 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11571\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -140.8794, step = 8200 (89.626 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:11:48.087781 139661544327040 basic_session_run_hooks.py:247] loss = -140.8794, step = 8200 (89.626 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1144\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:13:17.817941 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1144\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -159.29851, step = 8300 (89.735 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:13:17.822474 139661544327040 basic_session_run_hooks.py:247] loss = -159.29851, step = 8300 (89.735 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11662\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:14:47.374120 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11662\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -139.86797, step = 8400 (89.554 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:14:47.376210 139661544327040 basic_session_run_hooks.py:247] loss = -139.86797, step = 8400 (89.554 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 8500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:16:16.394945 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 8500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.981094\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:16:29.301189 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.981094\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -145.65184, step = 8500 (101.932 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:16:29.307806 139661544327040 basic_session_run_hooks.py:247] loss = -145.65184, step = 8500 (101.932 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11068\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:17:59.336395 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11068\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -156.47807, step = 8600 (90.039 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:17:59.346977 139661544327040 basic_session_run_hooks.py:247] loss = -156.47807, step = 8600 (90.039 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11711\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:19:28.852860 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11711\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -79.602875, step = 8700 (89.515 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:19:28.862251 139661544327040 basic_session_run_hooks.py:247] loss = -79.602875, step = 8700 (89.515 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1168\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:20:58.394525 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1168\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -152.02768, step = 8800 (89.539 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:20:58.401442 139661544327040 basic_session_run_hooks.py:247] loss = -152.02768, step = 8800 (89.539 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11493\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:22:28.086562 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11493\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -144.33984, step = 8900 (89.689 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:22:28.090148 139661544327040 basic_session_run_hooks.py:247] loss = -144.33984, step = 8900 (89.689 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 9000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:23:57.033193 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 9000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.988891\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:24:09.210114 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.988891\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -71.08029, step = 9000 (101.126 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:24:09.216291 139661544327040 basic_session_run_hooks.py:247] loss = -71.08029, step = 9000 (101.126 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10849\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:25:39.422620 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10849\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -127.94719, step = 9100 (90.209 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:25:39.424913 139661544327040 basic_session_run_hooks.py:247] loss = -127.94719, step = 9100 (90.209 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11507\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:27:09.103221 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11507\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 3.6081028, step = 9200 (89.684 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:27:09.109375 139661544327040 basic_session_run_hooks.py:247] loss = 3.6081028, step = 9200 (89.684 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11776\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:28:38.567652 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11776\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -75.02, step = 9300 (89.464 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:28:38.573324 139661544327040 basic_session_run_hooks.py:247] loss = -75.02, step = 9300 (89.464 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11408\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:30:08.327702 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11408\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -62.336292, step = 9400 (89.761 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:30:08.334368 139661544327040 basic_session_run_hooks.py:247] loss = -62.336292, step = 9400 (89.761 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 9500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:31:37.429997 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 9500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.987521\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:31:49.591401 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.987521\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -166.03185, step = 9500 (101.259 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:31:49.593345 139661544327040 basic_session_run_hooks.py:247] loss = -166.03185, step = 9500 (101.259 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11172\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:33:19.542258 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11172\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -134.29382, step = 9600 (89.952 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:33:19.545160 139661544327040 basic_session_run_hooks.py:247] loss = -134.29382, step = 9600 (89.952 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11396\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:34:49.312222 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11396\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -164.32986, step = 9700 (89.770 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:34:49.314799 139661544327040 basic_session_run_hooks.py:247] loss = -164.32986, step = 9700 (89.770 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11628\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:36:18.895169 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11628\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -131.13628, step = 9800 (89.583 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:36:18.898060 139661544327040 basic_session_run_hooks.py:247] loss = -131.13628, step = 9800 (89.583 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11354\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:37:48.698688 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11354\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -100.58549, step = 9900 (89.803 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:37:48.701380 139661544327040 basic_session_run_hooks.py:247] loss = -100.58549, step = 9900 (89.803 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 10000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:39:17.442216 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 10000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.988472\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:39:29.864925 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.988472\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -30.871025, step = 10000 (101.167 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:39:29.868263 139661544327040 basic_session_run_hooks.py:247] loss = -30.871025, step = 10000 (101.167 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10741\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:41:00.165788 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10741\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -145.97488, step = 10100 (90.303 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:41:00.171094 139661544327040 basic_session_run_hooks.py:247] loss = -145.97488, step = 10100 (90.303 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11696\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:42:29.694252 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11696\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -136.8302, step = 10200 (89.528 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:42:29.698714 139661544327040 basic_session_run_hooks.py:247] loss = -136.8302, step = 10200 (89.528 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11677\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:43:59.237988 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11677\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -146.64284, step = 10300 (89.546 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:43:59.244532 139661544327040 basic_session_run_hooks.py:247] loss = -146.64284, step = 10300 (89.546 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11666\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:45:28.790549 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11666\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -95.461525, step = 10400 (89.549 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:45:28.793635 139661544327040 basic_session_run_hooks.py:247] loss = -95.461525, step = 10400 (89.549 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 10500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:46:57.526296 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 10500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.984416\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:47:10.373607 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.984416\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -134.06914, step = 10500 (101.583 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:47:10.376172 139661544327040 basic_session_run_hooks.py:247] loss = -134.06914, step = 10500 (101.583 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.10987\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:48:40.474577 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.10987\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -154.32928, step = 10600 (90.101 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:48:40.477643 139661544327040 basic_session_run_hooks.py:247] loss = -154.32928, step = 10600 (90.101 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11564\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:50:10.109476 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11564\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 5.3371105, step = 10700 (89.636 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:50:10.113458 139661544327040 basic_session_run_hooks.py:247] loss = 5.3371105, step = 10700 (89.636 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.12062\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:51:39.345657 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.12062\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -162.44107, step = 10800 (89.235 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:51:39.348652 139661544327040 basic_session_run_hooks.py:247] loss = -162.44107, step = 10800 (89.235 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11987\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:53:08.641867 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11987\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -99.3812, step = 10900 (89.299 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:53:08.647901 139661544327040 basic_session_run_hooks.py:247] loss = -99.3812, step = 10900 (89.299 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 11000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:54:37.157162 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 11000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.988181\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:54:49.837892 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.988181\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -111.34999, step = 11000 (101.192 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:54:49.840306 139661544327040 basic_session_run_hooks.py:247] loss = -111.34999, step = 11000 (101.192 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1131\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:56:19.677409 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1131\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -147.34824, step = 11100 (89.840 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:56:19.679969 139661544327040 basic_session_run_hooks.py:247] loss = -147.34824, step = 11100 (89.840 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11683\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:57:49.216689 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11683\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -149.89046, step = 11200 (89.540 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:57:49.219825 139661544327040 basic_session_run_hooks.py:247] loss = -149.89046, step = 11200 (89.540 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11501\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:59:18.901660 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11501\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -87.37291, step = 11300 (89.689 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 11:59:18.909170 139661544327040 basic_session_run_hooks.py:247] loss = -87.37291, step = 11300 (89.689 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11866\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:00:48.294247 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11866\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -153.90895, step = 11400 (89.392 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:00:48.300967 139661544327040 basic_session_run_hooks.py:247] loss = -153.90895, step = 11400 (89.392 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 11500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:02:16.949067 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 11500 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.991652\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:02:29.136093 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 0.991652\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -163.942, step = 11500 (100.840 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:02:29.141155 139661544327040 basic_session_run_hooks.py:247] loss = -163.942, step = 11500 (100.840 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11201\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:03:59.063284 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11201\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -66.94929, step = 11600 (89.925 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:03:59.066045 139661544327040 basic_session_run_hooks.py:247] loss = -66.94929, step = 11600 (89.925 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.11625\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:05:28.649267 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.11625\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -126.80039, step = 11700 (89.588 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:05:28.654440 139661544327040 basic_session_run_hooks.py:247] loss = -126.80039, step = 11700 (89.588 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1223\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:06:57.751800 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1223\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -70.35898, step = 11800 (89.102 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:06:57.756513 139661544327040 basic_session_run_hooks.py:247] loss = -70.35898, step = 11800 (89.102 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 1.1189\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:08:27.125379 139661544327040 basic_session_run_hooks.py:680] global_step/sec: 1.1189\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = -148.33908, step = 11900 (89.375 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:08:27.131652 139661544327040 basic_session_run_hooks.py:247] loss = -148.33908, step = 11900 (89.375 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 12000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:09:55.922056 139661544327040 basic_session_run_hooks.py:594] Saving checkpoints for 12000 into ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: -166.0372.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:10:08.234185 139661544327040 estimator.py:359] Loss for final step: -166.0372.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  2:43:31.629822\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uHP88st76N4l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1062},"outputId":"0c79f9af-e625-46bb-89fc-a6719274c06a","executionInfo":{"status":"ok","timestamp":1559046671927,"user_tz":-60,"elapsed":425793,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["df_test = test_df#.sample(120)\n","\n","# supp_context = df_train.sample(100)\n","\n","batch_size = 32\n","eval_steps = int(len(df_test) / batch_size)\n","\n","neural_process.evaluate(eval_steps,\n","                       df_test, \n","                       score_col= score_column, \n","                       text_col=text_col_name, supplied_context_df = None)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:24:50.317733 139661544327040 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["Evaluation\n","****No context supplied ****\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:24:53.567503 139661544327040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:03.691341 139661544327040 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-05-28T12:25:03Z\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:03.717844 139661544327040 evaluation.py:257] Starting evaluation at 2019-05-28T12:25:03Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:05.191919 139661544327040 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:05.204211 139661544327040 saver.py:1270] Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:08.384936 139661544327040 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:08.636967 139661544327040 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [116/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:25:43.570955 139661544327040 evaluation.py:169] Evaluation [116/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [232/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:26:19.235514 139661544327040 evaluation.py:169] Evaluation [232/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [348/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:26:56.071602 139661544327040 evaluation.py:169] Evaluation [348/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [464/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:27:31.462188 139661544327040 evaluation.py:169] Evaluation [464/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [580/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:28:07.439357 139661544327040 evaluation.py:169] Evaluation [580/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [696/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:28:43.118926 139661544327040 evaluation.py:169] Evaluation [696/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [812/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:29:18.665435 139661544327040 evaluation.py:169] Evaluation [812/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [928/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:29:54.361365 139661544327040 evaluation.py:169] Evaluation [928/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [1044/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:30:30.203632 139661544327040 evaluation.py:169] Evaluation [1044/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [1160/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:31:06.113717 139661544327040 evaluation.py:169] Evaluation [1160/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [1165/1165]\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:31:07.662622 139661544327040 evaluation.py:169] Evaluation [1165/1165]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-05-28-12:31:08\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:31:08.119894 139661544327040 evaluation.py:277] Finished evaluation at 2019-05-28-12:31:08\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 12000: accuracy0 = 0.89262336, accuracy1 = 0.98495173, accuracy2 = 0.94662017, accuracy3 = 0.99321353, accuracy4 = 0.9415236, accuracy5 = 0.983074, auc0 = 0.96047705, auc1 = 0.98244643, auc2 = 0.97237784, auc3 = 0.8966579, auc4 = 0.9696056, auc5 = 0.8407147, global_step = 12000, loss = -103.14413\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:31:08.124746 139661544327040 estimator.py:1979] Saving dict for global step 12000: accuracy0 = 0.89262336, accuracy1 = 0.98495173, accuracy2 = 0.94662017, accuracy3 = 0.99321353, accuracy4 = 0.9415236, accuracy5 = 0.983074, auc0 = 0.96047705, auc1 = 0.98244643, auc2 = 0.97237784, auc3 = 0.8966579, auc4 = 0.9696056, auc5 = 0.8407147, global_step = 12000, loss = -103.14413\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 12000: ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 12:31:11.666625 139661544327040 estimator.py:2039] Saving 'checkpoint_path' summary for global step 12000: ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'accuracy0': 0.89262336,\n"," 'accuracy1': 0.98495173,\n"," 'accuracy2': 0.94662017,\n"," 'accuracy3': 0.99321353,\n"," 'accuracy4': 0.9415236,\n"," 'accuracy5': 0.983074,\n"," 'auc0': 0.96047705,\n"," 'auc1': 0.98244643,\n"," 'auc2': 0.97237784,\n"," 'auc3': 0.8966579,\n"," 'auc4': 0.9696056,\n"," 'auc5': 0.8407147,\n"," 'global_step': 12000,\n"," 'loss': -103.14413}"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"fu6bWjywkRly","colab_type":"code","colab":{}},"source":["# ! cp -r ./test_output \"./gdrive/My Drive/Kaggle_toxic_comments/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_NqA06NHYgxh","colab_type":"code","colab":{}},"source":["preds = neural_process.predict(\n","                       df_test, \n","                       score_col= score_column, \n","                       text_col=text_col_name,supplied_context_df = None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwZSmXI0YgvZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":801},"outputId":"48053ab6-a6d3-49b8-eea4-b4e96c0729f8","executionInfo":{"status":"ok","timestamp":1559048930046,"user_tz":-60,"elapsed":41908,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["i=0\n","for pred in preds:\n","  i += 1\n","  if i<10:\n","    print(pred)\n","  else:\n","    break"],"execution_count":21,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:29.141242 139661544327040 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["Prediction\n","****No context supplied ****\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:32.729330 139661544327040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:42.402638 139661544327040 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:44.346084 139661544327040 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:44.356490 139661544327040 saver.py:1270] Restoring parameters from ./gdrive/My Drive/Kaggle_toxic_comments/test_output/model.ckpt-12000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:47.632882 139661544327040 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0528 13:08:47.882883 139661544327040 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["{'prediction_mean': array([9.8771753e-04, 8.2227739e-04, 7.6934844e-03, 2.7445262e-02,\n","       2.8924644e-05, 9.2196189e-02], dtype=float32), 'prediction_var': array([0.01002009, 0.0100431 , 0.0100469 , 0.01001257, 0.01000196,\n","       0.01001668], dtype=float32)}\n","{'prediction_mean': array([6.34933996e-04, 8.83944333e-04, 8.66532512e-03, 4.09781672e-02,\n","       2.21207738e-05, 1.12177834e-01], dtype=float32), 'prediction_var': array([0.01000948, 0.01005954, 0.01004871, 0.01001398, 0.01000148,\n","       0.010012  ], dtype=float32)}\n","{'prediction_mean': array([0.92653036, 0.03144516, 0.8688755 , 0.0486189 , 0.7748625 ,\n","       0.10828374], dtype=float32), 'prediction_var': array([0.0171299 , 0.03541978, 0.432202  , 0.01083835, 0.23058406,\n","       0.0558685 ], dtype=float32)}\n","{'prediction_mean': array([5.2458793e-04, 8.9413521e-04, 8.7813903e-03, 4.9624957e-02,\n","       2.1216274e-05, 1.2607256e-01], dtype=float32), 'prediction_var': array([0.01000761, 0.010068  , 0.01005081, 0.010015  , 0.01000133,\n","       0.01001111], dtype=float32)}\n","{'prediction_mean': array([0.91311777, 0.02272715, 0.62499416, 0.03326634, 0.83643275,\n","       0.12976208], dtype=float32), 'prediction_var': array([0.02742786, 0.02239181, 1.1925408 , 0.01067005, 0.26652068,\n","       0.1687814 ], dtype=float32)}\n","{'prediction_mean': array([0.05940829, 0.00114744, 0.01165812, 0.00317849, 0.00120744,\n","       0.03274533], dtype=float32), 'prediction_var': array([0.0169123 , 0.01003575, 0.01021617, 0.01002889, 0.0100975 ,\n","       0.01037507], dtype=float32)}\n","{'prediction_mean': array([5.71383513e-04, 8.02654016e-04, 7.79207796e-03, 4.02751975e-02,\n","       2.08780166e-05, 1.12493336e-01], dtype=float32), 'prediction_var': array([0.01000882, 0.01005417, 0.01004431, 0.01001286, 0.01000129,\n","       0.01001138], dtype=float32)}\n","{'prediction_mean': array([5.99680818e-04, 8.65097332e-04, 8.48408230e-03, 4.43946160e-02,\n","       2.15202563e-05, 1.14589475e-01], dtype=float32), 'prediction_var': array([0.01000863, 0.01006039, 0.01004831, 0.01001444, 0.01000143,\n","       0.01001125], dtype=float32)}\n","{'prediction_mean': array([0.78202385, 0.17720586, 0.26298124, 0.83186084, 0.3053477 ,\n","       0.5338253 ], dtype=float32), 'prediction_var': array([0.11648017, 0.41029033, 0.32123986, 0.6819841 , 0.09799958,\n","       0.08110423], dtype=float32)}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7hEFhEGHkrVy","colab_type":"code","colab":{}},"source":["test_scores = df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n","       'identity_hate']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fri1X4hdkrev","colab_type":"code","colab":{}},"source":["uncertainty = pd.DataFrame(columns=['accuracy', 'variance'])\n","pred_means = pd.DataFrame(columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n","       'identity_hate'])\n","pred_variances = pd.DataFrame(columns = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n","       'identity_hate'])\n","\n","for i, pred in enumerate(preds):\n","  if i<len(test_scores):\n","    scores = np.array(test_scores.iloc[i])\n","    #     print(scores)\n","    mean_prediction = np.round(pred['prediction_mean'])\n","    pred_means = pred_means.append({score_column[k]:pred['prediction_mean'][k] for k in range(len(score_column))}, ignore_index=True)\n","    #     print(mean_prediction)\n","    acc = scores==mean_prediction\n","    variance = pred['prediction_var']\n","    pred_variances = pred_variances.append({score_column[k]:variance[k] for k in range(len(score_column))}, ignore_index=True)\n","    for j in range(len(scores)):\n","      uncertainty = uncertainty.append({'accuracy': acc[j], 'variance': variance[j]}, ignore_index=True)\n","  else:\n","    break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gc3034n5pYtd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"9da3ada9-da4d-428e-a0c8-4029957be10f","executionInfo":{"status":"ok","timestamp":1559055475792,"user_tz":-60,"elapsed":432,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["print(uncertainty[uncertainty.accuracy==True]['variance'].mean())\n","print(uncertainty[uncertainty.accuracy==False]['variance'].mean())"],"execution_count":106,"outputs":[{"output_type":"stream","text":["0.03968876489706338\n","0.2590321116743508\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h5PsPzmyH6sA","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"RJvlFgZ9j_r6","colab_type":"code","colab":{}},"source":["def total_accuaracy(pred_means, test_scores):\n","  preds = np.round(pred_means)\n","  comparison = preds == test_scores\n","  correct_guesses = np.reshape(comparison.values,-1)\n","  return np.sum(correct_guesses) / len(correct_guesses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPZACnkUGrr5","colab_type":"code","colab":{}},"source":["def class_accuaracy(pred_means, test_scores):\n","  preds = np.round(pred_means)\n","  comparison = preds == test_scores\n","  correct_guesses = comparison.values\n","  return correct_guesses.sum(axis=0)/ len(correct_guesses)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xVcqWkdHT-s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"c268f1f9-b375-4ab2-e2a8-21ddb71989e7","executionInfo":{"status":"ok","timestamp":1559055480466,"user_tz":-60,"elapsed":621,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["print(total_accuaracy(pred_means, test_scores.reset_index(drop=True)))\n","print(class_accuaracy(pred_means, test_scores.reset_index(drop=True)))"],"execution_count":109,"outputs":[{"output_type":"stream","text":["0.8877807269185054\n","[0.71103783 0.97731843 0.83179174 0.98662163 0.83903054 0.98088421]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wjZUS-Ergg00","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":553},"outputId":"93ab92bc-8a92-4c7e-f0e8-d2df97e87ebb","executionInfo":{"status":"ok","timestamp":1559058584997,"user_tz":-60,"elapsed":609,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["def subset_accuracy(att):\n","  print(\"accuracy on \"+att+ \" comments:\")\n","  print(class_accuaracy(pred_means.iloc[np.where(test_scores[att] == 1)].reset_index(drop=True),\n","                        test_scores.iloc[np.where(test_scores[att] == 1)].reset_index(drop=True)))\n","\n","  print(\"proportion of \"+att+ \" comments:\")\n","  print(len(test_scores[test_scores[att] ==1])/len(test_scores))\n","  print()\n","\n","for att in score_column:\n","  subset_accuracy(att)"],"execution_count":133,"outputs":[{"output_type":"stream","text":["accuracy on toxic comments:\n","[0.22029045 0.93495602 0.42667212 0.95868276 0.46430763 0.88668439]\n","proportion of toxic comments:\n","0.13107590015818119\n","\n","accuracy on severe_toxic comments:\n","[0.19650655 0.         0.09606987 0.89519651 0.18340611 0.65938865]\n","proportion of severe_toxic comments:\n","0.006139574787527816\n","\n","accuracy on obscene comments:\n","[0.22695035 0.9047619  0.11043566 0.95812226 0.31273219 0.86018237]\n","proportion of obscene comments:\n","0.0793855063138422\n","\n","accuracy on threat comments:\n","[0.23353293 0.83233533 0.40718563 0.01796407 0.38922156 0.79041916]\n","proportion of threat comments:\n","0.004477331831952599\n","\n","accuracy on insult comments:\n","[0.23107715 0.90829694 0.27620087 0.95487627 0.10953421 0.82860262]\n","proportion of insult comments:\n","0.07367489745033379\n","\n","accuracy on identity_hate comments:\n","[0.22504537 0.83666062 0.33212341 0.9292196  0.21778584 0.00181488]\n","proportion of identity_hate comments:\n","0.014772514008418456\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m2flPKNig2zn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":74},"outputId":"0eb97fbe-d5d2-4676-db24-4bc7b7663b35","executionInfo":{"status":"ok","timestamp":1559059183414,"user_tz":-60,"elapsed":743,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["len(test_scores[test_scores.severe_toxic==1])"],"execution_count":137,"outputs":[{"output_type":"execute_result","data":{"text/plain":["229"]},"metadata":{"tags":[]},"execution_count":137}]},{"cell_type":"code","metadata":{"id":"NZ1zjhdxZByn","colab_type":"code","colab":{}},"source":["at_least_one = (test_scores.toxic==1) | (test_scores.severe_toxic==1) | \\\n","               (test_scores.obscene==1) | (test_scores.threat==1) | \\\n","               (test_scores.insult==1) | (test_scores.identity_hate==1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VzhiqkLZB2h","colab_type":"code","colab":{}},"source":["print(uncertainty[uncertainty.accuracy==True]['variance'].mean())\n","print(uncertainty[uncertainty.accuracy==False]['variance'].mean())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"75VhsEk5ZB5E","colab_type":"code","outputId":"d1d7411d-4cdc-43de-c697-c41c82472cfa","executionInfo":{"status":"ok","timestamp":1558900798557,"user_tz":-60,"elapsed":648,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":74}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["37299"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"wXgIhm8zujWn","colab_type":"code","outputId":"4e9d86f8-5349-47a8-f0d2-ee399497879e","executionInfo":{"status":"ok","timestamp":1558903339593,"user_tz":-60,"elapsed":636,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}},"colab":{"base_uri":"https://localhost:8080/","height":92}},"source":["try: \n","  print(\"this\")\n","  True/\"this\"\n","except:\n","  print(\"didnt work\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["this\n","didnt work\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iWn5PHX-4Tdt","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}