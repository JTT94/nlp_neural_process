{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Class_BERT_NeuralProcesses_notebook.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/JTT94/nlp_neural_process/blob/master/Class_BERT_NeuralProcesses_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SZKAwmizhzHo","colab":{"base_uri":"https://localhost:8080/","height":203},"outputId":"e851b01f-dbdc-428b-b426-16f45f7de2ba","executionInfo":{"status":"ok","timestamp":1558898665789,"user_tz":-60,"elapsed":4660,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_hub as hub\n","import os\n","import re\n","\n","from keras import backend as K\n","import numpy as np\n","import string\n","from datetime import datetime \n","import tensorflow_probability as tfp\n","from tensorflow_probability import distributions as tfd"],"execution_count":1,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0526 19:24:22.406549 140326760654720 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pgB8X8dti3qc","outputId":"57052c20-c55a-415e-eebf-1c977e38999a","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1558898669937,"user_tz":-60,"elapsed":8645,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["!pip install bert-tensorflow\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 10.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZCJDPOgQOsqJ","colab_type":"code","colab":{}},"source":["# initialiase tensorboard \n","# from https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab\n","\n","\n","# Get TensorBoard running in the background. \n","LOG_DIR = './test_output'\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5sZ3Bl48oaxU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":347},"outputId":"6289d392-4ec7-4ea2-902d-088d40a469de","executionInfo":{"status":"ok","timestamp":1558898673938,"user_tz":-60,"elapsed":12295,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["# # #Download and unzip ngrok. \n","!test -e ngrok-stable-linux-amd64.zip || wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!test -e ngrok || unzip ngrok-stable-linux-amd64.zip\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2019-05-26 19:24:30--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 35.173.3.255, 52.55.191.55, 52.203.66.95, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|35.173.3.255|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 16648024 (16M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  15.88M  15.6MB/s    in 1.0s    \n","\n","2019-05-26 19:24:31 (15.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [16648024/16648024]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mLp9PmAzoWhE","colab_type":"code","colab":{}},"source":["# #Launch ngrok background process...\n","get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eL1A1ibjOwnL","colab_type":"code","outputId":"4b505add-12e1-4cdb-8fc1-fb7edf435c92","colab":{"base_uri":"https://localhost:8080/","height":186},"executionInfo":{"status":"ok","timestamp":1558898675945,"user_tz":-60,"elapsed":13624,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["https://5d1da96c.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bweIS72l5WS5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":226},"outputId":"f9722905-ae01-47b3-e7e6-bd9f3e36de7a","executionInfo":{"status":"ok","timestamp":1558898678911,"user_tz":-60,"elapsed":16090,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["import os, sys\n","sys.path.append('../') # add personal code dir to path for import\n","\n","\n","!test -d neural_process || git clone https://github.com/JTT94/nlp_neural_process.git neural_process\n","sys.path.append('./neural_process/')\n","\n","import random\n","from neural_process import split_context_target, NeuralProcessParams\n","from neural_process.network import *\n","from neural_process.loss import *\n","from neural_process.predict import *\n","from neural_process.process import *\n","\n","from neural_process.tf_model_builder_AUC import *\n","from neural_process.bert_utils import *"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Cloning into 'neural_process'...\n","remote: Enumerating objects: 120, done.\u001b[K\n","remote: Counting objects: 100% (120/120), done.\u001b[K\n","remote: Compressing objects: 100% (57/57), done.\u001b[K\n","remote: Total 120 (delta 73), reused 101 (delta 61), pack-reused 0\n","Receiving objects: 100% (120/120), 680.93 KiB | 1.40 MiB/s, done.\n","Resolving deltas: 100% (73/73), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"69MNF4fEi5Ly","outputId":"37055f9f-39c0-417e-f6e3-fccc1589c8c4","colab":{"base_uri":"https://localhost:8080/","height":94},"executionInfo":{"status":"ok","timestamp":1558898680733,"user_tz":-60,"elapsed":17348,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["# Initialize session\n","sess = tf.Session()\n","K.set_session(sess)\n","K.tensorflow_backend._get_available_gpus()\n"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/job:localhost/replica:0/task:0/device:GPU:0']"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"jDvh4JVNQFck","colab_type":"code","outputId":"62a63521-8a93-4563-f98b-1095e88cb314","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"ok","timestamp":1558898701625,"user_tz":-60,"elapsed":37618,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cc1rwgOZi9F1","colab":{}},"source":["# filename = './cleaned_data.csv'\n","# filename = './data1.csv'\n","# df = pd.read_csv(filename, index_col=0)\n","# cols = ['comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# cols = ['comment','cleaned_comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# df = df[cols]\n","\n","#score_column = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n","# text_col_name = 'cleaned_comment'\n","# text_col_name = 'comment'\n","\n","#--------\n","\n","### For kaggle dataset\n","\n","## Training data\n","\n","# filename = './gdrive/My Drive/Data/train.csv'\n","train_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_train.csv'\n","\n","df = pd.read_csv(train_filename)\n","cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","df = df[cols]\n","\n","score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n","\n","text_col_name = 'comment_text'\n","\n","#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n","for i in score_column:\n","  df[i] = pd.to_numeric(df[i],downcast='float')\n","\n","# over represent toxic comments\n","\n","df['num_toxic_atts'] = df[cols[1:]].apply(lambda x: np.sum(x), axis = 1)\n","\n","df_toxic = df[df.num_toxic_atts  > 0]\n","# print(len(df_toxic))\n","\n","df_healthy_sample = df[df.num_toxic_atts == 0].sample(frac=1.0)[:len(df_toxic)]\n","# print(len(df_healthy_sample))\n","\n","df_train = pd.concat([df_toxic, df_healthy_sample]).sample(frac=1.0)\n","\n","\n","## Test data:\n","\n","test_data_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_test.csv'\n","test_labels_filename = './gdrive/My Drive/Kaggle_toxic_comments/kaggle_test_labels.csv'\n","test_df = pd.read_csv(test_data_filename)\n","test_labels = pd.read_csv(test_labels_filename)\n","test_df = pd.merge(test_df, test_labels, on='id')\n","#get rid of -1 values\n","test_df = test_df.replace(-1, np.nan).dropna(subset=cols)\n","\n","#restrict comment length\n","test_df = test_df[test_df.comment_text.str.len() <= 250]\n","\n","test_df = test_df[cols]\n","#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n","for i in score_column:\n","  test_df[i] = pd.to_numeric(test_df[i],downcast='float')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p_JIbqhY55L_","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mUIe15uTi9Ok","outputId":"04a7c9d8-51de-46c1-b5a5-026e03b8786c","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"ok","timestamp":1558898730221,"user_tz":-60,"elapsed":64310,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["W0526 19:25:27.243856 140326760654720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:29.443532 140326760654720 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"jECyqGRfyht4","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IVESPLmgl98I","colab":{}},"source":["# #Pre process data for bert embedding\n","max_seq_length = 128\n","\n","# import pickle\n","\n","# # train_input_examples = create_examples(df_train, score_column, text_col_name)\n","# # test_input_examples = create_examples(df_test, score_column, text_col_name)\n","\n","# # train_features = convert_examples_to_features(train_input_examples, max_seq_length, tokenizer)\n","# # test_features = convert_examples_to_features(test_input_examples, max_seq_length, tokenizer)\n","\n","# # pickle.dump(train_features, open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'wb'))\n","# # pickle.dump(test_features, open('./gdrive/My Drive/Kaggle_toxic_comments/test_features.p', 'wb'))\n","\n","# # ## Load features previously saved\n","# train_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))\n","# test_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOdj6CWiPYSX","colab_type":"code","colab":{}},"source":["# utility methods\n","from collections import namedtuple\n","NeuralProcessParams = namedtuple('NeuralProcessParams', ['dim_z', 'n_hidden_units_h', 'n_hidden_units_g'])\n","GaussianParams = namedtuple('GaussianParams', ['mu', 'sigma'])\n","\n","\n","def batch_mlp(input, inner_layer_dims, output_dim, variable_scope):\n","  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n","  \n","  Args:\n","    input: input tensor of shape [B,n,d_in].\n","    output_sizes: An iterable containing the output sizes of the MLP as defined \n","        in `basic.Linear`.\n","    variable_scope: String giving the name of the variable scope. If this is set\n","        to be the same as a previously defined MLP, then the weights are reused.\n","    \n","  Returns:\n","    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n","  \"\"\"\n","  # Get the shapes of the input and reshape to parallelise across observations\n","\n","  output = input\n","\n","  \n","  # Pass through MLP\n","  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n","    for i, size in enumerate(inner_layer_dims):\n","      output = tf.nn.relu(\n","          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n","\n","    # Last layer without a ReLu\n","    output = tf.layers.dense(output, output_dim, name=\"layer_{}\".format(i + 1))\n","\n","  return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_gK0D7AhoLK","colab_type":"code","colab":{}},"source":["class Embedder(object):\n","  \n","  def __init__(self, BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True):\n","    self.BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","    self.tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)\n","    self.trainable = trainable\n","    \n","  def __call__(self, input_ids, input_mask, segment_ids):\n","    embedder = hub.Module(self.BERT_model_hub,trainable=self.trainable)\n","    bert_inputs = dict(input_ids=input_ids,\n","                       input_mask=input_mask, \n","                       segment_ids=segment_ids)\n","\n","    bert_outputs = embedder(inputs=bert_inputs,\n","                               signature=\"tokens\", \n","                               as_dict=True)\n","    \n","  # Use \"pooled_output\" for classification tasks on an entire sentence. Use \"sequence_outputs\" for token-level output\n","    return bert_outputs[\"pooled_output\"]\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2HMqLmD8PTEP","colab_type":"code","colab":{}},"source":["class Decoder(object):\n","  \"\"\"The Decoder.\"\"\"\n","\n","  def __init__(self, layer_dims, num_classes):\n","    self.layer_dims = layer_dims\n","    self.num_classes = num_classes\n","    \n","  def __call__(self, input_xs_embedding, z_samples):\n","  \n","        # inputs dimensions\n","    # z_sample has dim [n_draws, dim_z]\n","    # x_star has dim [N_star, dim_x]\n","    n_draws = z_samples.get_shape().as_list()[0]\n","    n_xs = tf.shape(input_xs_embedding)[0]\n","\n","    # Repeat z samples for each x*\n","    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n","\n","    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n","    z_samples_repeat = tf.tile(z_samples, [1, n_xs, 1])\n","\n","    # Repeat x* for each z sample\n","    x_star_repeat = tf.expand_dims(input_xs_embedding, axis=0)\n","    x_star_repeat = tf.tile(x_star_repeat, [n_draws, 1, 1])\n","\n","    # Concatenate x* and z\n","    inputs = tf.concat([x_star_repeat, z_samples_repeat], axis=2)\n","\n","    # decoder mlp\n","    inner_layer_dims = self.layer_dims\n","    output_dim = self.num_classes *2\n","    hidden = batch_mlp(inputs, inner_layer_dims, output_dim, \"decoder\")\n","\n","    # Get the mean an the variance\n","    mu, log_sigma = tf.split(hidden, 2, axis= -1)\n","\n","    # Bound the variance\n","    sigma_star = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n","    mu_star = tf.math.sigmoid(mu)\n","\n","\n","    return GaussianParams(mu_star, sigma_star)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6DmGdqzaGjS","colab_type":"code","colab":{}},"source":["class Encoder(object):\n","\n","  def __init__(self, layer_dims, latent_dim):\n","    self.layer_dims = layer_dims\n","    self.latent_dim = latent_dim\n","    \n","  def __call__(self, xs, ys):\n","    print(xs)\n","    print(ys)\n","    xys = tf.concat([xs, ys], axis=1)\n","\n","\n","    # encoder mlp\n","    inner_layer_dims = self.layer_dims[:-1]\n","    output_dim = self.layer_dims[-1]\n","    rs = batch_mlp(xys, inner_layer_dims, output_dim, \"encoder\")\n","    \n","    # aggregate rs\n","    r = self._aggregate_r(rs)\n","    \n","    # get mu and sigma\n","    z_params = self._get_z_params(r)\n","    \n","    # distribution\n","    dist = tfd.MultivariateNormalDiag(loc=z_params.mu,\n","                                          scale_diag=z_params.sigma)\n","    return dist\n","    \n","  def _aggregate_r(self, context_rs: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Aggregate the output of the encoder to a single representation\n","\n","    Creates an aggregation (mean) operator to combine the encodings of multiple context inputs\n","\n","    Parameters\n","    ----------\n","    context_rs\n","        Input encodings tensor, shape: (n_samples, dim_r)\n","\n","    Returns\n","    -------\n","        Output tensor of aggregation result\n","    \"\"\"\n","    mean = tf.reduce_mean(context_rs, axis=0)\n","    r = tf.reshape(mean, [1, -1])\n","    return r\n","  \n","  def _get_z_params(self, context_r: tf.Tensor) -> GaussianParams:\n","    \"\"\"Map encoding to mean and covariance of the random variable Z\n","\n","    Creates a linear dense layer to map encoding to mu_z, and another linear mapping + a softplus activation for Sigma_z\n","\n","    Parameters\n","    ----------\n","    context_r\n","        Input encoding tensor, shape: (1, dim_r)\n","    params\n","        Neural process parameters\n","\n","    Returns\n","    -------\n","        Output tensors of the mappings for mu_z and Sigma_z\n","    \"\"\"\n","    hidden = context_r\n","    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n","      # First apply intermediate relu layer \n","      hidden = tf.nn.relu(\n","          tf.layers.dense(hidden, \n","                          (self.layer_dims[-1] + self.latent_dim)/2, \n","                          name=\"penultimate_layer\"))\n","      \n","      # Then apply further linear layers to output latent mu and log sigma\n","      mu = tf.layers.dense(hidden, self.latent_dim, name=\"mean_layer\")\n","      log_sigma = tf.layers.dense(hidden, self.latent_dim, name=\"std_layer\")\n","      \n","\n","    # Compute sigma\n","    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n","\n","    return GaussianParams(mu, sigma)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWZZAs1haFkc","colab_type":"code","colab":{}},"source":["class NLP_NeuralProcess(object):\n","  \n","  def __init__(self,\n","                  score_col, \n","                  params = NeuralProcessParams(dim_z=20, \n","                                                     n_hidden_units_h=[128, 128, 128], \n","                                                     n_hidden_units_g=[128, 128, 128]),\n","                  num_classes = 6, \n","                  num_draws = 2, \n","                  lr = 2e-5,\n","                  batch_size = 32, \n","                  num_warmup_steps=100,\n","                  num_train_steps = 10**3,\n","                  save_summary_steps = 100,\n","                  save_checkpoints_steps = 500,\n","                  output_dir = \"./test_output\"\n","                 ):\n","    \n","    self.params = params\n","    self.encoder = Encoder(layer_dims = self.params.n_hidden_units_h, \n","                           latent_dim=self.params.dim_z)\n","    self.decoder = Decoder(layer_dims= self.params.n_hidden_units_g, \n","                           num_classes=num_classes)\n","    self.num_draws = num_draws\n","#     self.estimator = None\n","    #self.embedder = Embedder()\n","    #####  \n","    num_labels = len(score_col)\n","    \n","    # Specify outpit directory and number of checkpoint steps to save\n","    run_config = tf.estimator.RunConfig(model_dir=output_dir,\n","        save_summary_steps=save_summary_steps, save_checkpoints_steps=save_checkpoints_steps)\n","\n","    #####\n","\n","\n","    model_fn = self.model_fn_builder(num_labels = num_labels, learning_rate=lr,\n","      num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n","\n","    self.estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n","      params={\"batch_size\": batch_size})\n","    \n","  def create_model(self, \n","                   target_input_ids, \n","                   target_input_mask, \n","                   target_segment_ids, \n","                   target_scores=None,\n","                   context_input_ids = None, \n","                   context_input_mask = None, \n","                   context_segment_ids= None, \n","                   context_scores = None\n","                   ):\n","    \n","    # apply embedder\n","    embedder = hub.Module(BERT_model_hub,trainable=False)\n","    \n","    valid_context = (context_input_ids is not None) & (context_input_mask is not None) & (context_segment_ids is not None) & (context_scores is not None)\n","    \n","    # target processing - all scenarios\n","    target_inputs = dict(input_ids=target_input_ids,\n","                     input_mask=target_input_mask, \n","                     segment_ids=target_segment_ids)\n","    target_embeddings = embedder(inputs=target_inputs,\n","                               signature=\"tokens\", \n","                               as_dict=True)\n","    target_xs = target_embeddings[\"pooled_output\"]\n","    \n","    \n","    if valid_context:\n","      \n","      # context processing - training\n","      context_inputs = dict(input_ids=context_input_ids,\n","                         input_mask=context_input_mask, \n","                         segment_ids=context_segment_ids)\n","      context_embeddings = embedder(inputs=context_inputs,\n","                                 signature=\"tokens\", \n","                                 as_dict=True)\n","      context_xs = context_embeddings[\"pooled_output\"]\n","      context_ys = context_scores\n","      # total x,y \n","      x_all = tf.concat([context_xs, target_xs], axis=0)\n","      \n","      # get encoding params with context\n","      context_z_dist = self.encoder(context_xs, context_ys)\n","      # predictions with context\n","      posterior_pred = self.decoder(target_xs, context_z_dist.sample(self.num_draws))\n","        \n","       # target scores - context training / evaluation\n","      if target_scores is not None:\n","        target_ys = target_scores\n","        y_all = tf.concat([context_ys, target_ys], axis=0)\n","        all_z_dist = self.encoder(x_all, y_all)\n","        \n","        # loss\n","        loglike = self.loglikelihood(target_ys, posterior_pred)\n","        KL_loss = self.KLqp_gaussian(all_z_dist.parameters['loc'], \n","                                     all_z_dist.parameters['scale_diag'], \n","                                     context_z_dist.parameters['loc'], \n","                                     context_z_dist.parameters['scale_diag'])\n","        loss = tf.negative(loglike) + KL_loss\n","        # context and training / evaluation\n","        return (loss, posterior_pred, target_ys)\n","      \n","      # context prediction\n","      return  (None, posterior_pred, None)\n","    \n","    # no context\n","    else:\n","      x_all = target_xs \n","      # get internal representation\n","      mean_zero = tf.constant(np.repeat(0., params.dim_z))\n","      epsilon_dist = tfd.MultivariateNormalDiag(loc= mean_zero)                            \n","      epsilon = tf.expand_dims(epsilon_dist.sample(self.num_draws), axis=1)\n","      epsilon = tf.cast(epsilon, tf.float32)\n","      prior_predict = self.decoder(x_all, epsilon)\n","      \n","      # target scores - no context training / evaluation\n","      if target_scores is not None:\n","        target_ys = target_scores\n","        loglike = self.loglikelihood(target_ys, prior_predict)\n","        loss = tf.negative(loglike)\n","        # no context/ training / evaluation\n","        return  (loss, prior_predict, target_ys)\n","   \n","    \n","      # no context prediction\n","      return  (None, prior_predict, None)\n","\n","  \n","  def loglikelihood(self, y_star: tf.Tensor, dist):\n","    \"\"\"Log-likelihood of an output given a predicted \"\"\"\n","    p_normal = tfd.MultivariateNormalDiag(loc = dist.mu, scale_diag=dist.sigma)\n","    loglike = p_normal.log_prob(y_star)\n","    loglike = tf.reduce_sum(loglike, axis=0)\n","    loglike = tf.reduce_mean(loglike)\n","    return loglike\n","  \n","  def KLqp_gaussian(self, mu_q: tf.Tensor, sigma_q: tf.Tensor, mu_p: tf.Tensor, sigma_p: tf.Tensor) -> tf.Tensor:\n","    \"\"\"Kullback-Leibler divergence between two Gaussian distributions\n","\n","    Determines KL(q || p) = < log( q / p ) >_q\n","\n","    Parameters\n","    ----------\n","    mu_q\n","        Mean tensor of distribution q, shape: (1, dim)\n","    sigma_q\n","        Variance tensor of distribution q, shape: (1, dim)\n","    mu_p\n","        Mean tensor of distribution p, shape: (1, dim)\n","    sigma_p\n","        Variance tensor of distribution p, shape: (1, dim)\n","\n","    Returns\n","    -------\n","        KL tensor, shape: (1)\n","    \"\"\"\n","    sigma2_q = tf.square(sigma_q) + 1e-16\n","    sigma2_p = tf.square(sigma_p) + 1e-16\n","    temp = sigma2_q / sigma2_p + tf.square(mu_q - mu_p) / sigma2_p - 1.0 + tf.log(sigma2_p / sigma2_q + 1e-16)\n","    return 0.5 * tf.reduce_sum(temp)\n","  \n","  def context_target_split(self, batch_size =32):\n","    btch_sz = batch_size\n","    n_context = tf.random_shuffle(tf.range(1,btch_sz))[0]\n","    \n","    indices = tf.range(0, btch_sz)\n","    context_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context))\n","    target_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context, btch_sz))\n","    \n","    return context_set_indices, target_set_indices\n","    \n","  def model_fn_builder(self, num_labels, learning_rate, num_train_steps, num_warmup_steps):\n","      \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","      \n","      \n","      def model_fn(features, mode, params):  # pylint: disable=unused-argument\n","          \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","          \n","          # run model\n","          # -------------------------------------------------------------------------------------------\n","          target_input_ids = None\n","          target_input_mask = None\n","          target_segment_ids = None \n","          target_scores = None\n","          \n","          context_input_ids = None \n","          context_input_mask = None \n","          context_segment_ids = None\n","          context_scores = None\n","          \n","          # training \n","          if mode == tf.estimator.ModeKeys.TRAIN:\n","            input_ids = std_features[\"input_ids\"]\n","            input_mask = std_features[\"input_mask\"]\n","            segment_ids = std_features[\"segment_ids\"]    \n","            scores = std_features[\"scores\"]\n","            \n","            # context split\n","            context_set_indices, target_set_indices= self.context_target_split(batch_size =32)\n","            \n","            target_input_ids = tf.gather(input_ids,target_set_indices)\n","            target_input_mask = tf.gather(input_mask,target_set_indices)\n","            target_segment_ids = tf.gather(segment_ids,target_set_indices) \n","            target_scores = tf.gather(scores,target_set_indices)\n","\n","            context_input_ids = tf.gather(input_ids,context_set_indices) \n","            context_input_mask = tf.gather(input_mask,context_set_indices) \n","            context_segment_ids = tf.gather(segment_ids,context_set_indices)\n","            context_scores = tf.gather(scores,context_set_indices)\n","            \n","            \n","          elif mode == tf.estimator.ModeKeys.PREDICT:\n","            print('Prediction')\n","            target_input_ids = features[\"input_ids\"]\n","            target_input_mask = features[\"input_mask\"]\n","            target_segment_ids = features[\"segment_ids\"]    \n","            target_scores = features[\"scores\"]\n","            \n","            try:\n","              context_input_ids = features[\"supplied_context_input_ids\"]\n","              context_input_mask = features[\"supplied_context_input_mask\"]\n","              context_segment_ids = features[\"supplied_context_segment_ids\"] \n","              context_scores = features[\"supplied_context_scores\"]\n","               \n","            except:\n","              print(\"****No context supplied ****\")\n","              context_input_ids = None\n","              context_input_mask = None\n","              context_segment_ids = None\n","              context_scores = None\n","            \n","          else:\n","            print('Evaluation')\n","            target_input_ids = features[\"input_ids\"]\n","            target_input_mask = features[\"input_mask\"]\n","            target_segment_ids = features[\"segment_ids\"]    \n","            target_scores = features[\"scores\"]\n","\n","            try:\n","              context_input_ids = features[\"supplied_context_input_ids\"]\n","              context_input_mask = features[\"supplied_context_input_mask\"]\n","              context_segment_ids = features[\"supplied_context_segment_ids\"] \n","              context_scores = features[\"supplied_context_scores\"]\n","               \n","            except:\n","              print(\"****No context supplied ****\")\n","              context_input_ids = None\n","              context_input_mask = None\n","              context_segment_ids = None\n","              context_scores = None\n","\n","\n","          (loss, prediction, true_y) = self.create_model(target_input_ids, \n","                                                         target_input_mask, \n","                                                         target_segment_ids, \n","                                                         target_scores,\n","                                                         context_input_ids, \n","                                                         context_input_mask, \n","                                                         context_segment_ids, \n","                                                         context_scores)\n","\n","          train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","          ystar, _ = tf.nn.moments(prediction.mu,[0])\n","          variance, _ = tf.nn.moments(tf.math.square(prediction.sigma),[0])\n","          \n","          \n","          # output from model\n","          # -------------------------------------------------------------------------------------------\n","\n","          # training \n","          if mode == tf.estimator.ModeKeys.TRAIN:\n","              return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n","\n","          # prediction\n","          elif mode == tf.estimator.ModeKeys.PREDICT:\n","              return tf.estimator.EstimatorSpec(mode=mode, predictions={'prediction_mean': ystar, 'prediction_var': variance})\n","\n","          # evaluation\n","          else:\n","            # Calculate evaluation metrics.\n","            eval_metrics = {}\n","\n","            # AUC\n","            def metric_fn(pred_scores, real_scores, trait_num):\n","                auc_value = tf.metrics.auc(real_scores[:,trait_num], pred_scores[:,trait_num])\n","                accuracy_value = tf.metrics.accuracy(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n","                return {\"auc\"+str(trait_num): auc_value, \"accuracy\"+str(trait_num): accuracy_value}\n","\n","            labels = true_y # need to round them if true labels are not 1 or 0\n","            eval_metrics_lst = [metric_fn(ystar, labels, trait_num) for trait_num in range(num_labels)]\n","\n","            for d in eval_metrics_lst:\n","                tf.summary.scalar(list(d.keys())[0], list(d.values())[0][1]) # make available to tensorboard\n","                eval_metrics.update(d)\n","            return tf.estimator.EstimatorSpec(mode=mode,loss=loss, eval_metric_ops=eval_metrics)\n","\n","      return model_fn\n","  \n","\n","  def prepare_examples(self, df_train, score_column, text_col_name, supplied_context_df=None):\n","      num_labels = len(score_column)\n","      train_input_examples = create_examples(df_train, score_column, text_col_name)\n","      train_features = convert_examples_to_features(train_input_examples, max_seq_length, tokenizer)\n","      \n","      if supplied_context_df is not None:\n","        supplied_context_examples = create_examples(supplied_context_df, score_column, text_col_name)\n","        supplied_context_features = convert_examples_to_features(supplied_context_examples, max_seq_length, tokenizer)\n","        train_input_fn = input_fn_builder(\n","          features=train_features, seq_length=max_seq_length, \n","          num_labels = num_labels, is_training=True, drop_remainder=False,\n","          supplied_context_features = supplied_context_features)\n","      \n","      else:\n","        train_input_fn = input_fn_builder(\n","          features=train_features, seq_length=max_seq_length, \n","          num_labels = num_labels, is_training=True, drop_remainder=False)\n","        \n","      return train_input_fn\n","  \n","  def predict(self, \n","            df_train, \n","            score_col, \n","            text_col,\n","            supplied_context_df=None\n","             ):\n","    \n","    test_input_fn = self.prepare_examples(df_train, score_col, text_col, supplied_context_df)\n","    preds = self.estimator.predict(input_fn=test_input_fn)\n","    \n","    return preds\n","  \n","  def evaluate(self, \n","               eval_steps,\n","               df_train, \n","               score_col, \n","               text_col,\n","               supplied_context_df=None):\n","    num_labels = len(score_col)\n","    eval_input_fn = self.prepare_examples(df_train, score_col, text_col, supplied_context_df)\n","    \n","    result = self.estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n","    return result\n","  \n","  def train(self,num_train_steps,\n","            df_train, \n","            score_col, \n","            text_col,\n","           ):\n","\n","    # Create an input function for training. drop_remainder = True for using TPUs.\n","    train_input_fn = self.prepare_examples(df_train, score_col, text_col)\n","\n","    print('Beginning Training!')\n","    current_time = datetime.now()\n","    self.estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","    print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z8iSEczc6NS1","colab_type":"code","outputId":"83a530fb-9a06-432b-c810-8946b1dd08ca","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1558898731079,"user_tz":-60,"elapsed":60161,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["\n","# tf.reset_default_graph()\n","\n","score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] \n","text_col_name = 'comment_text'\n","\n","params = NeuralProcessParams(dim_z=2, n_hidden_units_h=[128, 128, 128], n_hidden_units_g=[128, 128, 128])\n","\n","\n","neural_process = NLP_NeuralProcess(score_col=score_column, params = params, num_draws = 2)\n","\n","### Train:\n","\n","# neural_process.train(df_train=df_train, \n","#                      score_col= score_column, \n","#                      text_col=text_col_name, \n","#                      num_train_steps=10**3) \n","# #                      num_warmup_steps=5)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': './test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9fb7bef5c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:30.718868 140326760654720 estimator.py:201] Using config: {'_model_dir': './test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9fb7bef5c0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"uHP88st76N4l","colab_type":"code","outputId":"b1378a4a-771e-4f72-dd00-ac434959767c","colab":{"base_uri":"https://localhost:8080/","height":1442},"executionInfo":{"status":"ok","timestamp":1558898799945,"user_tz":-60,"elapsed":125787,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["df_test = test_df[:100]\n","\n","supp_context = test_df[100:200]\n","\n","neural_process.evaluate(10**2,\n","                       df_test, \n","                       score_col= score_column, \n","                       text_col=text_col_name, supplied_context_df = supp_context)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Could not find trained model in model_dir: ./test_output, running initialization to evaluate.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:30.881641 140326760654720 estimator.py:488] Could not find trained model in model_dir: ./test_output, running initialization to evaluate.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:31.025637 140326760654720 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["Evaluation\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:34.569647 140326760654720 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:35.516541 140326760654720 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"IteratorGetNext:6\", shape=(?, 6), dtype=float32)\n","WARNING:tensorflow:From <ipython-input-13-504275df30f7>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0526 19:25:35.654063 140326760654720 deprecation.py:323] From <ipython-input-13-504275df30f7>:28: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["W0526 19:25:36.153474 140326760654720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/learning_rate_decay_v2.py:321: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0526 19:25:36.196435 140326760654720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0526 19:25:37.382138 140326760654720 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:38.222898 140326760654720 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-05-26T19:25:38Z\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:38.253226 140326760654720 evaluation.py:257] Starting evaluation at 2019-05-26T19:25:38Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:39.027012 140326760654720 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:40.365267 140326760654720 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:40.535669 140326760654720 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [10/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:47.998094 140326760654720 evaluation.py:169] Evaluation [10/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [20/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:53.392374 140326760654720 evaluation.py:169] Evaluation [20/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [30/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:25:58.798142 140326760654720 evaluation.py:169] Evaluation [30/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [40/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:04.228682 140326760654720 evaluation.py:169] Evaluation [40/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [50/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:09.679979 140326760654720 evaluation.py:169] Evaluation [50/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [60/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:15.158174 140326760654720 evaluation.py:169] Evaluation [60/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [70/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:20.654474 140326760654720 evaluation.py:169] Evaluation [70/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [80/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:26.155554 140326760654720 evaluation.py:169] Evaluation [80/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [90/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:31.682627 140326760654720 evaluation.py:169] Evaluation [90/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Evaluation [100/100]\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:37.229943 140326760654720 evaluation.py:169] Evaluation [100/100]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-05-26-19:26:37\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:37.535040 140326760654720 evaluation.py:277] Finished evaluation at 2019-05-26-19:26:37\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 0: accuracy0 = 0.8596875, accuracy1 = 0.0, accuracy2 = 0.215, accuracy3 = 0.9528125, accuracy4 = 0.0903125, accuracy5 = 0.03, auc0 = 0.48214254, auc1 = 1.0, auc2 = 0.45062232, auc3 = 0.5161392, auc4 = 0.41402495, auc5 = 0.12006351, global_step = 0, loss = 10.404034\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:37.538020 140326760654720 estimator.py:1979] Saving dict for global step 0: accuracy0 = 0.8596875, accuracy1 = 0.0, accuracy2 = 0.215, accuracy3 = 0.9528125, accuracy4 = 0.0903125, accuracy5 = 0.03, auc0 = 0.48214254, auc1 = 1.0, auc2 = 0.45062232, auc3 = 0.5161392, auc4 = 0.41402495, auc5 = 0.12006351, global_step = 0, loss = 10.404034\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'accuracy0': 0.8596875,\n"," 'accuracy1': 0.0,\n"," 'accuracy2': 0.215,\n"," 'accuracy3': 0.9528125,\n"," 'accuracy4': 0.0903125,\n"," 'accuracy5': 0.03,\n"," 'auc0': 0.48214254,\n"," 'auc1': 1.0,\n"," 'auc2': 0.45062232,\n"," 'auc3': 0.5161392,\n"," 'auc4': 0.41402495,\n"," 'auc5': 0.12006351,\n"," 'global_step': 0,\n"," 'loss': 10.404034}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"_NqA06NHYgxh","colab_type":"code","colab":{}},"source":["preds = neural_process.predict(\n","                       df_test, \n","                       score_col= score_column, \n","                       text_col=text_col_name,supplied_context_df = supp_context)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwZSmXI0YgvZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":12578},"outputId":"ee1d7d48-c887-400c-c939-06eae00ee1f7","executionInfo":{"status":"ok","timestamp":1558898811915,"user_tz":-60,"elapsed":51893,"user":{"displayName":"Ilan Price","photoUrl":"","userId":"14888046437571338619"}}},"source":["i=0\n","for pred in preds:\n","  i += 1\n","  if i<100:\n","    val = df_test.reset_index().loc[[i],:]\n","    print(val['comment_text'])\n","    print(val.values)\n","    print(pred)\n","  else:\n","    break"],"execution_count":21,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Could not find trained model in model_dir: ./test_output, running initialization to predict.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:39.618526 140326760654720 estimator.py:604] Could not find trained model in model_dir: ./test_output, running initialization to predict.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:39.751532 140326760654720 estimator.py:1111] Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["Prediction\n","INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:42.946203 140326760654720 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:43.680471 140326760654720 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"IteratorGetNext:6\", shape=(?, 6), dtype=float32)\n","Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n","Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n","INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:45.504072 140326760654720 estimator.py:1113] Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:46.264655 140326760654720 monitored_session.py:222] Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:47.544112 140326760654720 session_manager.py:491] Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["I0526 19:26:47.697426 140326760654720 session_manager.py:493] Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["1    :Dear god this site is horrible.\n","Name: comment_text, dtype: object\n","[[7 ':Dear god this site is horrible.' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.47745132, 0.6166792 , 0.5257212 , 0.52102816, 0.5961162 ,\n","       0.50846773], dtype=float32), 'prediction_var': array([0.8630141 , 0.43919033, 0.53195393, 0.6121141 , 0.37925267,\n","       0.56666386], dtype=float32)}\n","2    this other one from 1897\n","Name: comment_text, dtype: object\n","[[16 'this other one from 1897' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48166043, 0.6161827 , 0.49238142, 0.53094643, 0.5957934 ,\n","       0.50483537], dtype=float32), 'prediction_var': array([0.9879074 , 0.4267248 , 0.51427376, 0.65802014, 0.3515126 ,\n","       0.54139686], dtype=float32)}\n","3    == Reason for banning throwing == \\n\\n This ar...\n","Name: comment_text, dtype: object\n","[[17\n","  '== Reason for banning throwing == \\n\\n This article needs a section on /why/ throwing is banned. At the moment, to a non-cricket fan, it seems kind of arbitrary.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5073652 , 0.61245036, 0.50793064, 0.52576035, 0.59382844,\n","       0.4790731 ], dtype=float32), 'prediction_var': array([0.97941995, 0.43022984, 0.52545285, 0.7159723 , 0.3468107 ,\n","       0.502742  ], dtype=float32)}\n","4    |blocked]] from editing Wikipedia.   |\n","Name: comment_text, dtype: object\n","[[19 '|blocked]] from editing Wikipedia.   |' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53193784, 0.7423213 , 0.45738292, 0.49405265, 0.673638  ,\n","       0.44292033], dtype=float32), 'prediction_var': array([1.4930794 , 0.46508223, 0.5836151 , 0.6760036 , 0.1861271 ,\n","       0.6282681 ], dtype=float32)}\n","5    == Arabs are committing genocide in Iraq, but ...\n","Name: comment_text, dtype: object\n","[[21\n","  '== Arabs are committing genocide in Iraq, but no protests in Europe. == \\n\\n May Europe also burn in hell.'\n","  1.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52189684, 0.6946694 , 0.46257094, 0.5052544 , 0.65448534,\n","       0.44364375], dtype=float32), 'prediction_var': array([1.3856347 , 0.4453535 , 0.59635043, 0.63544035, 0.25722113,\n","       0.5685892 ], dtype=float32)}\n","6    Please stop. If you continue to vandalize Wiki...\n","Name: comment_text, dtype: object\n","[[22\n","  'Please stop. If you continue to vandalize Wikipedia, as you did to Homosexuality, you will be blocked from editing.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49602902, 0.63608086, 0.4988631 , 0.5190033 , 0.6001936 ,\n","       0.43676382], dtype=float32), 'prediction_var': array([1.114798  , 0.40586638, 0.5519078 , 0.66860414, 0.32970834,\n","       0.5476916 ], dtype=float32)}\n","7    @RedSlash, cut it short. If you have sources s...\n","Name: comment_text, dtype: object\n","[[26\n","  '@RedSlash, cut it short. If you have sources stating the RoK is sovereign post them. Otherwise please aknowledge WP is not the place to make OR.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53241193, 0.6746316 , 0.48003542, 0.5173383 , 0.649021  ,\n","       0.415944  ], dtype=float32), 'prediction_var': array([1.3060019 , 0.45491505, 0.6445844 , 0.5541184 , 0.2548598 ,\n","       0.5940224 ], dtype=float32)}\n","8    . \\n\\n           Jews are not a race because y...\n","Name: comment_text, dtype: object\n","[[28\n","  '. \\n\\n           Jews are not a race because you can only get it from your mother. Your own mention of Ethiopian Jews not testing \\n           as Jews proves it is not, as well as the fact that we accept converts'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5351703 , 0.7260277 , 0.44969785, 0.51081514, 0.69223523,\n","       0.40954423], dtype=float32), 'prediction_var': array([1.2932546 , 0.55210245, 0.6826925 , 0.6325747 , 0.18433326,\n","       0.62569237], dtype=float32)}\n","9    Professors to the Manhatten Project.\n","Name: comment_text, dtype: object\n","[[31 'Professors to the Manhatten Project.' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48483026, 0.6690146 , 0.47741866, 0.5108882 , 0.6049968 ,\n","       0.46847823], dtype=float32), 'prediction_var': array([1.1062186 , 0.39234558, 0.53629684, 0.6241582 , 0.27619913,\n","       0.58989286], dtype=float32)}\n","10    일이삼사오육칠팔구하고십이요 에헤헤 으헤 으헤 으허허\n","Name: comment_text, dtype: object\n","[[34 '일이삼사오육칠팔구하고십이요 에헤헤 으헤 으헤 으허허' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5068627 , 0.7427729 , 0.47169614, 0.4707196 , 0.6808484 ,\n","       0.45000738], dtype=float32), 'prediction_var': array([1.4162153 , 0.52064174, 0.68518543, 0.6410561 , 0.15989184,\n","       0.69184   ], dtype=float32)}\n","11    REDIRECT Talk:Mi Vida Eres Tú\n","Name: comment_text, dtype: object\n","[[40 'REDIRECT Talk:Mi Vida Eres Tú' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5453588 , 0.718233  , 0.4733449 , 0.50008845, 0.6764918 ,\n","       0.42469987], dtype=float32), 'prediction_var': array([1.489944  , 0.44416583, 0.6438341 , 0.616413  , 0.20672396,\n","       0.63690096], dtype=float32)}\n","12    == September 20th Truce == \\n\\n According to s...\n","Name: comment_text, dtype: object\n","[[45\n","  '== September 20th Truce == \\n\\n According to several news sources, a truce was reached in Minsk last night. http://www.bbc.com/news/world-europe-29290246'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.535269  , 0.7140077 , 0.46813053, 0.48800522, 0.6555377 ,\n","       0.421928  ], dtype=float32), 'prediction_var': array([1.4121188 , 0.441195  , 0.59944767, 0.63025784, 0.21265978,\n","       0.6597787 ], dtype=float32)}\n","13    I'd never think I'd need to say it, but Wikipe...\n","Name: comment_text, dtype: object\n","[[46\n","  \"I'd never think I'd need to say it, but Wikipedia isn't a fansite discussion board. If anything is unannounced by any authority, it might as well be false. MMORPGs are overrated,\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.51288235, 0.6590687 , 0.50604534, 0.5316018 , 0.6082375 ,\n","       0.4326677 ], dtype=float32), 'prediction_var': array([1.3362867 , 0.44047996, 0.54984677, 0.65417695, 0.31299308,\n","       0.5498785 ], dtype=float32)}\n","14    But this is not the article about government p...\n","Name: comment_text, dtype: object\n","[[47\n","  'But this is not the article about government position but about the reaction. Add positions to 2008 Kosovo declaration of independence or Foreign relations of Kosovo.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5513114 , 0.7251076 , 0.46411258, 0.47591394, 0.6861752 ,\n","       0.4107222 ], dtype=float32), 'prediction_var': array([1.584091  , 0.40269244, 0.6775594 , 0.62463415, 0.18387191,\n","       0.6326058 ], dtype=float32)}\n","15    DJ Robinson is gay as hell! he sucks his dick ...\n","Name: comment_text, dtype: object\n","[[48 'DJ Robinson is gay as hell! he sucks his dick so much!!!!!' 1.0 0.0\n","  1.0 0.0 1.0 1.0]]\n","{'prediction_mean': array([0.5453588 , 0.718233  , 0.4733449 , 0.50008845, 0.6764918 ,\n","       0.42469987], dtype=float32), 'prediction_var': array([1.489944  , 0.44416583, 0.6438341 , 0.616413  , 0.20672396,\n","       0.63690096], dtype=float32)}\n","16    == Dracula's Grandmother == \\n\\n  \\n Dracula's...\n","Name: comment_text, dtype: object\n","[[49\n","  \"== Dracula's Grandmother == \\n\\n  \\n Dracula's grandmother was a Bulgarian princess, the sister of Ivan Sratzimir. The links with the lands across the Danube remain largely unexamined. I would appreciate any serious contributions. \\n (Kaloyan)\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49122894, 0.68188894, 0.45638502, 0.49284956, 0.64201784,\n","       0.4628976 ], dtype=float32), 'prediction_var': array([1.2124186 , 0.38109952, 0.61500555, 0.60706353, 0.23971379,\n","       0.50923073], dtype=float32)}\n","17    I WILL BURN YOU TO HELL IF YOU REVOKE MY TALK ...\n","Name: comment_text, dtype: object\n","[[56\n","  'I WILL BURN YOU TO HELL IF YOU REVOKE MY TALK PAGE ACCESS!!!!!!!!!!!!!'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5129539 , 0.7247351 , 0.4758134 , 0.48119903, 0.66517687,\n","       0.44458947], dtype=float32), 'prediction_var': array([1.4370066 , 0.41896325, 0.57430947, 0.6316196 , 0.20039923,\n","       0.66181207], dtype=float32)}\n","18    :Fuck off, you anti-semitic cunt.  |\n","Name: comment_text, dtype: object\n","[[59 ':Fuck off, you anti-semitic cunt.  |' 1.0 0.0 1.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.54545736, 0.6977696 , 0.46366102, 0.5065271 , 0.6593834 ,\n","       0.4308382 ], dtype=float32), 'prediction_var': array([1.41624   , 0.40172172, 0.58621603, 0.643308  , 0.24127766,\n","       0.60253   ], dtype=float32)}\n","19    Puwersa ng Masa!\n","Name: comment_text, dtype: object\n","[[60 'Puwersa ng Masa!' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5317424 , 0.5784148 , 0.56379306, 0.55643547, 0.5512501 ,\n","       0.44305104], dtype=float32), 'prediction_var': array([0.93362325, 0.5901137 , 0.61981356, 0.6512486 , 0.441858  ,\n","       0.46712375], dtype=float32)}\n","20    REDIRECT Talk:Kemp's thicket rat\n","Name: comment_text, dtype: object\n","[[65 \"REDIRECT Talk:Kemp's thicket rat\" 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.50301135, 0.61685205, 0.4862576 , 0.52476084, 0.60295486,\n","       0.49389857], dtype=float32), 'prediction_var': array([0.9441494 , 0.41612107, 0.48527604, 0.6752921 , 0.37120277,\n","       0.53256536], dtype=float32)}\n","21    See also . . . http://arxiv.org/abs/hep-ph/000...\n","Name: comment_text, dtype: object\n","[[69 'See also . . . http://arxiv.org/abs/hep-ph/0009204' 0.0 0.0 0.0 0.0\n","  0.0 0.0]]\n","{'prediction_mean': array([0.5304899 , 0.680882  , 0.47006014, 0.51014334, 0.6579665 ,\n","       0.43257946], dtype=float32), 'prediction_var': array([1.3108089 , 0.40202194, 0.6236277 , 0.6112397 , 0.26201352,\n","       0.61670625], dtype=float32)}\n","22    \" \\n :That is ridiculous. Unless there's a goo...\n","Name: comment_text, dtype: object\n","[[75\n","  '\" \\n :That is ridiculous. Unless there\\'s a good and non-disingenuous response, I would absolutely agree with you blocking indef outright. Falsifying sources should simply never be tolerated. //  \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5297451 , 0.71591425, 0.43120912, 0.5068798 , 0.6568271 ,\n","       0.45281225], dtype=float32), 'prediction_var': array([1.4843473 , 0.42312068, 0.55228424, 0.6968881 , 0.21721885,\n","       0.59876156], dtype=float32)}\n","23    How dare you vandalize that page about the HMS...\n","Name: comment_text, dtype: object\n","[[76\n","  \"How dare you vandalize that page about the HMS Beagle! Don't vandalize again, demon!\"\n","  1.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53069127, 0.6488715 , 0.5074502 , 0.52616525, 0.60448444,\n","       0.447389  ], dtype=float32), 'prediction_var': array([1.1769221 , 0.4737949 , 0.5735668 , 0.6703863 , 0.34652907,\n","       0.5039481 ], dtype=float32)}\n","24    aapn bhtla aanand jhala..\n","Name: comment_text, dtype: object\n","[[79 'aapn bhtla aanand jhala..' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52506083, 0.7038256 , 0.4488933 , 0.4921146 , 0.630279  ,\n","       0.43901414], dtype=float32), 'prediction_var': array([1.3872256 , 0.45145983, 0.558367  , 0.67567647, 0.25067407,\n","       0.5550339 ], dtype=float32)}\n","25    ::No, he is an arrogant, self serving, immatur...\n","Name: comment_text, dtype: object\n","[[81\n","  '::No, he is an arrogant, self serving, immature idiot. Get it right.'\n","  1.0 0.0 1.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.5351703 , 0.7260277 , 0.44969785, 0.51081514, 0.69223523,\n","       0.40954423], dtype=float32), 'prediction_var': array([1.2932546 , 0.55210245, 0.6826925 , 0.6325747 , 0.18433326,\n","       0.62569237], dtype=float32)}\n","26    :Thanks for the comment about Wiki-defendernes...\n","Name: comment_text, dtype: object\n","[[89\n","  ':Thanks for the comment about Wiki-defenderness. I like that one. I usually wikiling Wiki-defender. I agree that at first he was somewhat innocent but now have my doubts as he is being really agressive about the whole matter.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5129539 , 0.7247351 , 0.4758134 , 0.48119903, 0.66517687,\n","       0.44458947], dtype=float32), 'prediction_var': array([1.4370066 , 0.41896325, 0.57430947, 0.6316196 , 0.20039923,\n","       0.66181207], dtype=float32)}\n","27    Agree, but that's not the issue. The queston i...\n","Name: comment_text, dtype: object\n","[[94\n","  \"Agree, but that's not the issue. The queston is, in everyday English, does first amendent refer primarily to the US Constitution, or is it ambiguous? My belief remains that even fifth amendment is ambiguous, and first amendment even more so.     \\n *\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5110277 , 0.6379492 , 0.5261824 , 0.51857126, 0.6230538 ,\n","       0.41141683], dtype=float32), 'prediction_var': array([0.956718  , 0.47322237, 0.6235234 , 0.57598627, 0.36692166,\n","       0.5881597 ], dtype=float32)}\n","28    \" \\n\\n == Main towns that are not so main == \\...\n","Name: comment_text, dtype: object\n","[[95\n","  '\" \\n\\n == Main towns that are not so main == \\n\\n I know that you love to write a one sentence article. But, if you want to do that, please, at least do it properly. Stop saying \"\"X is a main town in Y, Z\"\". What is so main about obscure towns?   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49442372, 0.7105436 , 0.45639348, 0.5118083 , 0.6376376 ,\n","       0.46620557], dtype=float32), 'prediction_var': array([1.1810894 , 0.5479375 , 0.55615056, 0.6409745 , 0.1962105 ,\n","       0.5793642 ], dtype=float32)}\n","29    \" \\n\\n == Halliday == \\n\\n Good to see another...\n","Name: comment_text, dtype: object\n","[[97\n","  '\" \\n\\n == Halliday == \\n\\n Good to see another contributor to his article. If SFL is your thing, you might consider joining the Wikipedia:WikiProject_Linguistics/SFLSFL taskforce. Thanks.   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5073644 , 0.6135847 , 0.526598  , 0.53008676, 0.60951656,\n","       0.477858  ], dtype=float32), 'prediction_var': array([0.93826175, 0.4700392 , 0.58128643, 0.6581571 , 0.37087643,\n","       0.5440059 ], dtype=float32)}\n","30    \" \\n ::: That Stephen Barrett is not Board Cer...\n","Name: comment_text, dtype: object\n","[[98\n","  '\" \\n ::: That Stephen Barrett is not Board Certified is not a viewpoint. It is a fact (a well documented one at that). I don\\'t see how WP:PROMINENCE applies.    \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5400928 , 0.66065806, 0.5200586 , 0.52445245, 0.59487206,\n","       0.45798135], dtype=float32), 'prediction_var': array([1.1104858 , 0.5038279 , 0.5574651 , 0.6678634 , 0.3571605 ,\n","       0.46929955], dtype=float32)}\n","31    \" \\n\\n == Cloud feedback == \\n\\n Why is cloud ...\n","Name: comment_text, dtype: object\n","[[105\n","  '\" \\n\\n == Cloud feedback == \\n\\n Why is cloud feedback only under the positive feedbacks? Just about any paper states that it can be both. That the IPCC states that it is \"\"more likely a positive than a negative feedback\"\" doesn\\'t change that.   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48839545, 0.66851836, 0.4705493 , 0.5094    , 0.6025953 ,\n","       0.48217136], dtype=float32), 'prediction_var': array([1.23259   , 0.41258186, 0.5220344 , 0.7216406 , 0.25614884,\n","       0.53741914], dtype=float32)}\n","32    This is a REDIRECT discussion page. Please do ...\n","Name: comment_text, dtype: object\n","[[108\n","  \"This is a REDIRECT discussion page. Please do not leave article related discussion here.  Please go to the article's discussion page.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5036154 , 0.6842902 , 0.4724671 , 0.49570045, 0.6273482 ,\n","       0.44793487], dtype=float32), 'prediction_var': array([1.3474352 , 0.39259186, 0.5504373 , 0.60057795, 0.2648368 ,\n","       0.5785575 ], dtype=float32)}\n","33    السلام عليكم و رحمة الله و بركاته الا الجميع \\...\n","Name: comment_text, dtype: object\n","[[119 'السلام عليكم و رحمة الله و بركاته الا الجميع \\n تفضلوا جميعا' 0.0\n","  0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5198428 , 0.6434127 , 0.5209132 , 0.51207286, 0.6129574 ,\n","       0.46380913], dtype=float32), 'prediction_var': array([1.017472  , 0.45138037, 0.5765885 , 0.6554471 , 0.34693345,\n","       0.53830165], dtype=float32)}\n","34    \"This is just bizarre. Ani Medjool's abuse of ...\n","Name: comment_text, dtype: object\n","[[121\n","  '\"This is just bizarre. Ani Medjool\\'s abuse of wikipedia rules gets rewarded, and then he awards you a barnstar.  WTF?  Anyhow a new person, adept at ridicule, has joined in the conversation at falafel.  Sigh...   \\n\\n \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49130327, 0.6452826 , 0.45976806, 0.53341633, 0.6000677 ,\n","       0.5034478 ], dtype=float32), 'prediction_var': array([1.0586612 , 0.416584  , 0.5285015 , 0.63560843, 0.28608727,\n","       0.5642929 ], dtype=float32)}\n","35    == black mamba == \\n\\n It.is ponious snake of ...\n","Name: comment_text, dtype: object\n","[[131\n","  '== black mamba == \\n\\n It.is ponious snake of the word and but it not kills many people but king cobra kills many people in India'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53948236, 0.7091774 , 0.4540906 , 0.4901452 , 0.68864626,\n","       0.42722988], dtype=float32), 'prediction_var': array([1.4063215 , 0.5284891 , 0.64059246, 0.6118332 , 0.20627686,\n","       0.6333202 ], dtype=float32)}\n","36    :::Looks like this was an old issue so I took ...\n","Name: comment_text, dtype: object\n","[[142\n","  \":::Looks like this was an old issue so I took the liberty of removing it. I don't see any difference between the non-spaced version and the spaced version now. The spaced version just makes the intro look very strange.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.47944164, 0.6147443 , 0.52313954, 0.5214558 , 0.6000538 ,\n","       0.510057  ], dtype=float32), 'prediction_var': array([0.8636954 , 0.43631905, 0.5270747 , 0.6107197 , 0.379888  ,\n","       0.56955856], dtype=float32)}\n","37    **I just revamped WP:FART, I thought a little ...\n","Name: comment_text, dtype: object\n","[[144\n","  '**I just revamped WP:FART, I thought a little housekeeping would give it more street cred.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48966706, 0.698527  , 0.46667773, 0.51395476, 0.6389966 ,\n","       0.45190552], dtype=float32), 'prediction_var': array([1.1397996 , 0.4964432 , 0.6075846 , 0.5621908 , 0.25867793,\n","       0.48247498], dtype=float32)}\n","38    :Does J.P. Harris have anything to say about t...\n","Name: comment_text, dtype: object\n","[[146 ':Does J.P. Harris have anything to say about the Ancre?' 0.0 0.0\n","  0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5095191 , 0.6620804 , 0.49219787, 0.5262593 , 0.61717165,\n","       0.45808858], dtype=float32), 'prediction_var': array([1.1370497 , 0.44677067, 0.52199304, 0.68558216, 0.31955916,\n","       0.5534681 ], dtype=float32)}\n","39    == Don't feed the troll == \\n\\n Responding wit...\n","Name: comment_text, dtype: object\n","[[154\n","  \"== Don't feed the troll == \\n\\n Responding with taunting is exactly the wrong way to respond.  Don't feed the troll.  Remove the post without comment, or don't do anything at all.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.531091  , 0.6968468 , 0.46975434, 0.49696618, 0.6421848 ,\n","       0.43099892], dtype=float32), 'prediction_var': array([1.4205477 , 0.3694146 , 0.6047865 , 0.6275842 , 0.23526543,\n","       0.59324735], dtype=float32)}\n","40    \" \\n\\n :Please accept my apologies.  I interpr...\n","Name: comment_text, dtype: object\n","[[159\n","  '\" \\n\\n :Please accept my apologies.  I interpreted your statement \"\"Find a quote yourself, or move it, I don\\'t care\"\" to mean that you thought the source could be removed instead of finding a quote.  My mistake!   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52409774, 0.7004932 , 0.5009796 , 0.4863884 , 0.6037526 ,\n","       0.4389699 ], dtype=float32), 'prediction_var': array([1.2804799 , 0.47785   , 0.5204996 , 0.60902655, 0.27229136,\n","       0.52610755], dtype=float32)}\n","41    ==Vandalism== \\n\\n Hello Barbara.  \\n It appea...\n","Name: comment_text, dtype: object\n","[[167\n","  \"==Vandalism== \\n\\n Hello Barbara.  \\n It appears you have a long history of vandalism, and after another incident I am afraid I have no choice but to request a 'permanent ban from administrators. I wish you good luck for the future.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.45410413, 0.57954633, 0.53704774, 0.5318263 , 0.6072279 ,\n","       0.49850047], dtype=float32), 'prediction_var': array([0.7460213 , 0.48581955, 0.51183385, 0.6274506 , 0.36374706,\n","       0.5979972 ], dtype=float32)}\n","42    All the other DAPs have similar links and no o...\n","Name: comment_text, dtype: object\n","[[171 'All the other DAPs have similar links and no one has complained.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5378899 , 0.682799  , 0.45157868, 0.5060071 , 0.6576243 ,\n","       0.43631375], dtype=float32), 'prediction_var': array([1.3494227 , 0.45023513, 0.6160039 , 0.6331637 , 0.24166447,\n","       0.6038922 ], dtype=float32)}\n","43    :::also fwiw I nominated polyamorous people fo...\n","Name: comment_text, dtype: object\n","[[172\n","  ':::also fwiw I nominated polyamorous people for deleting, take a look and weigh in if you like.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52430236, 0.675545  , 0.47833037, 0.5130917 , 0.61922336,\n","       0.444014  ], dtype=float32), 'prediction_var': array([1.3562567 , 0.42131805, 0.53323865, 0.657815  , 0.2744222 ,\n","       0.54851115], dtype=float32)}\n","44    I have 10 pictures of armed gaddafi loyalists ...\n","Name: comment_text, dtype: object\n","[[181\n","  'I have 10 pictures of armed gaddafi loyalists (mostly from 2013-2014) they are all taken from facebook/YT videos would they be classified as fair use?'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5358027 , 0.7113314 , 0.4605859 , 0.4897789 , 0.67403644,\n","       0.42454758], dtype=float32), 'prediction_var': array([1.4921343 , 0.44655514, 0.64720523, 0.6207787 , 0.20177852,\n","       0.63298166], dtype=float32)}\n","45    EdJohnston you are wrong, I did not file the r...\n","Name: comment_text, dtype: object\n","[[191\n","  'EdJohnston you are wrong, I did not file the report, BenHen1997 did. You also did not block the other person but only one of the several IPs they were using.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5101223 , 0.64601004, 0.48348665, 0.51660013, 0.64102256,\n","       0.44379961], dtype=float32), 'prediction_var': array([1.2017074 , 0.4442648 , 0.6238172 , 0.6036594 , 0.29342327,\n","       0.58185226], dtype=float32)}\n","46    \" \\n\\n == Shameless Canvass == \\n\\n Hello, Dia...\n","Name: comment_text, dtype: object\n","[[192\n","  '\" \\n\\n == Shameless Canvass == \\n\\n Hello, Diannaa! Thanks for blocking that horrible puke again. When I \"\"translate\"\"  on your page I get mad. He is just a total piece of garbage. Thanks again!   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52430236, 0.675545  , 0.47833037, 0.5130917 , 0.61922336,\n","       0.444014  ], dtype=float32), 'prediction_var': array([1.3562567 , 0.42131805, 0.53323865, 0.657815  , 0.2744222 ,\n","       0.54851115], dtype=float32)}\n","47    WHAT THE HELL \\n\\n   Justin\n","Name: comment_text, dtype: object\n","[[194 'WHAT THE HELL \\n\\n   Justin' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5110954 , 0.6287807 , 0.50917536, 0.5225147 , 0.60424143,\n","       0.4815183 ], dtype=float32), 'prediction_var': array([1.013595  , 0.4175968 , 0.5345824 , 0.64974207, 0.3604089 ,\n","       0.49892294], dtype=float32)}\n","48    WP:SOAPBOX, WP:FORUM, etc. Please engage in yo...\n","Name: comment_text, dtype: object\n","[[203\n","  'WP:SOAPBOX, WP:FORUM, etc. Please engage in your conspiratorial fantasies elsewhere.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49078232, 0.6182221 , 0.5470513 , 0.52789724, 0.5882795 ,\n","       0.49342474], dtype=float32), 'prediction_var': array([0.9475181, 0.5002575, 0.5024374, 0.647251 , 0.3906123, 0.5086467],\n","      dtype=float32)}\n","49    The specific page linked to is just junk in re...\n","Name: comment_text, dtype: object\n","[[204\n","  'The specific page linked to is just junk in regards to our policies on sources - check it for yourself.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5076121 , 0.6098809 , 0.50815487, 0.52584434, 0.597342  ,\n","       0.48204434], dtype=float32), 'prediction_var': array([0.9763031 , 0.4285451 , 0.5209359 , 0.7148459 , 0.34840712,\n","       0.50420284], dtype=float32)}\n","50    on February 21, 2015.\n","Name: comment_text, dtype: object\n","[[209 'on February 21, 2015.' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5323419 , 0.71322083, 0.45375383, 0.50147676, 0.6676853 ,\n","       0.43174794], dtype=float32), 'prediction_var': array([1.4694595, 0.5324689, 0.6074337, 0.647761 , 0.2021876, 0.6146562],\n","      dtype=float32)}\n","51    just a community post: isnt DB2FSS a First tri...\n","Name: comment_text, dtype: object\n","[[212\n","  'just a community post: isnt DB2FSS a First trimester screening, a prenatal test aimed to detect diseases or conditions in a fetus or embryo'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53486145, 0.7110553 , 0.46885544, 0.4903161 , 0.6876925 ,\n","       0.41100293], dtype=float32), 'prediction_var': array([1.3531206 , 0.496319  , 0.67109126, 0.5884035 , 0.19221404,\n","       0.6848294 ], dtype=float32)}\n","52    ::::Buffoon Synonyms:    bozo, buffo, clown, c...\n","Name: comment_text, dtype: object\n","[[215\n","  '::::Buffoon Synonyms:    bozo, buffo, clown, comedian, comic, fool, harlequin, humorist, idiot, jerk, jester, joker, merry-andrew, mime, mimic, mummer, playboy, prankster, ridicule, stooge, wag, wit, zany.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52034414, 0.73144233, 0.49751288, 0.46843207, 0.6877207 ,\n","       0.38574308], dtype=float32), 'prediction_var': array([1.521749  , 0.5667992 , 0.78915995, 0.5845046 , 0.16948213,\n","       0.67596054], dtype=float32)}\n","53    :Eek, but shes cute in an earthy kind of way. ...\n","Name: comment_text, dtype: object\n","[[219\n","  ':Eek, but shes cute in an earthy kind of way. Cant sing for shit though. Thanks for giving me an unhappy memory.'\n","  1.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52032375, 0.69836867, 0.4582721 , 0.5048065 , 0.6704924 ,\n","       0.4368847 ], dtype=float32), 'prediction_var': array([1.4216639 , 0.46855104, 0.59178567, 0.58677816, 0.21232231,\n","       0.6551622 ], dtype=float32)}\n","54    ==Wikimania== \\n Hello.  We're chatting right ...\n","Name: comment_text, dtype: object\n","[[224\n","  \"==Wikimania== \\n Hello.  We're chatting right now at Wikimania.  Pleased to meet you.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48149502, 0.6538631 , 0.4864906 , 0.5152495 , 0.5973407 ,\n","       0.47593194], dtype=float32), 'prediction_var': array([1.08828   , 0.4085378 , 0.5105939 , 0.631443  , 0.29967982,\n","       0.56587744], dtype=float32)}\n","55    for all peoples living East of a certain line,...\n","Name: comment_text, dtype: object\n","[[229\n","  \"for all peoples living East of a certain line, like the Vistula, because he distinguishes them from other large ethnic groups in the same area, the Fenni (probably Finno-Ugric) and the Sarmatians themselves. Taking that together with Jordanes'\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52196634, 0.67659223, 0.47021383, 0.5068034 , 0.6290513 ,\n","       0.43818015], dtype=float32), 'prediction_var': array([1.3399649 , 0.38258728, 0.5706778 , 0.62529373, 0.28305274,\n","       0.57792735], dtype=float32)}\n","56    \" \\n\\n P.S. IMHO, this all falls under the cat...\n","Name: comment_text, dtype: object\n","[[230\n","  '\" \\n\\n P.S. IMHO, this all falls under the category created by  on her talk page, which she appropriately refers to as \"\"loopy talk page discussions/scoldings.\"\"  \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5212114 , 0.72529614, 0.4386572 , 0.50625026, 0.64098287,\n","       0.46476322], dtype=float32), 'prediction_var': array([1.353212  , 0.45953506, 0.5079179 , 0.70832586, 0.21495777,\n","       0.5051068 ], dtype=float32)}\n","57    REDIRECT Talk:Ponhook Lake 10\n","Name: comment_text, dtype: object\n","[[233 'REDIRECT Talk:Ponhook Lake 10' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49517083, 0.60855955, 0.50758165, 0.5257324 , 0.60449433,\n","       0.47974586], dtype=float32), 'prediction_var': array([0.8707795 , 0.42964885, 0.53720343, 0.601942  , 0.36048463,\n","       0.5482511 ], dtype=float32)}\n","58    If the indigenous population is so low why do ...\n","Name: comment_text, dtype: object\n","[[244\n","  'If the indigenous population is so low why do they think they deserve everything'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5139594 , 0.6470759 , 0.5249329 , 0.51307636, 0.5731111 ,\n","       0.45264614], dtype=float32), 'prediction_var': array([1.0741695 , 0.6050884 , 0.5603106 , 0.6188149 , 0.34365076,\n","       0.443676  ], dtype=float32)}\n","59    and lewd sex in China\n","Name: comment_text, dtype: object\n","[[246 'and lewd sex in China' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.46429524, 0.63267004, 0.47357684, 0.5078623 , 0.61468035,\n","       0.45376015], dtype=float32), 'prediction_var': array([1.0298669 , 0.37806934, 0.6459274 , 0.547493  , 0.3094319 ,\n","       0.5680499 ], dtype=float32)}\n","60    == No more April Fool'd edits by me in the art...\n","Name: comment_text, dtype: object\n","[[248\n","  \"== No more April Fool'd edits by me in the article space. == \\n\\n I will not make any more April Fool's edits in the article space. \\n\\n Just in case you never saw it, here is my April Fool's edit from last year.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5415181 , 0.69785887, 0.4640513 , 0.5062385 , 0.6559021 ,\n","       0.44232577], dtype=float32), 'prediction_var': array([1.3281665 , 0.53045803, 0.54360044, 0.58081007, 0.22836947,\n","       0.57443786], dtype=float32)}\n","61    == Bitch Creek Cow Camp Idaho == \\n\\n Hey- Jus...\n","Name: comment_text, dtype: object\n","[[252\n","  '== Bitch Creek Cow Camp Idaho == \\n\\n Hey- Just a heads up, I looked into this one, and indeed, there is a place named, Bitch Creek Cow Camp. Wanted to let you know that the folks in Idaho, need not worry'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.488777  , 0.6655582 , 0.46900165, 0.50937647, 0.60587525,\n","       0.4853428 ], dtype=float32), 'prediction_var': array([1.2250998 , 0.4183408 , 0.52226895, 0.71733904, 0.25448233,\n","       0.541839  ], dtype=float32)}\n","62    \" \\n\\n ==Two short stories== \\n *The list of w...\n","Name: comment_text, dtype: object\n","[[253\n","  '\" \\n\\n ==Two short stories== \\n *The list of works contains something called \"\"The Mysterious Portrait\"\". Is this the same as the short story \"\"The Portrait\"\"?  \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5019077 , 0.7111745 , 0.46035177, 0.51648915, 0.6673555 ,\n","       0.43955493], dtype=float32), 'prediction_var': array([1.1800945 , 0.49676704, 0.5833827 , 0.5481778 , 0.22010842,\n","       0.6234778 ], dtype=float32)}\n","63    \" \\n\\n == May 2015 == \\n \"\n","Name: comment_text, dtype: object\n","[[254 '\" \\n\\n == May 2015 == \\n \"' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5344789 , 0.6414993 , 0.52288365, 0.5115397 , 0.5976987 ,\n","       0.42152104], dtype=float32), 'prediction_var': array([1.175433  , 0.43819857, 0.5834359 , 0.6039181 , 0.32878262,\n","       0.50972927], dtype=float32)}\n","64    ::::Well, it sucks to have a university to be ...\n","Name: comment_text, dtype: object\n","[[258\n","  \"::::Well, it sucks to have a university to be nicknameless. And it's the first time in NCAA history that it has happened. /\"\n","  1.0 0.0 1.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53540224, 0.7255233 , 0.44802874, 0.50094885, 0.6773771 ,\n","       0.4360787 ], dtype=float32), 'prediction_var': array([1.5423906 , 0.42755443, 0.59673   , 0.6623131 , 0.20466904,\n","       0.58439475], dtype=float32)}\n","65    == Argh == \\n\\n Some random idiot deleted the ...\n","Name: comment_text, dtype: object\n","[[259\n","  '== Argh == \\n\\n Some random idiot deleted the whole Japan article <.'\n","  1.0 0.0 1.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.5073039 , 0.74015844, 0.47376406, 0.4661495 , 0.68260515,\n","       0.44542277], dtype=float32), 'prediction_var': array([1.4162741 , 0.52383804, 0.68495214, 0.62281644, 0.15633322,\n","       0.7003898 ], dtype=float32)}\n","66    == hello == \\n\\n you are an T0T@L D0UCH3 B@G\n","Name: comment_text, dtype: object\n","[[260 '== hello == \\n\\n you are an T0T@L D0UCH3 B@G' 0.0 0.0 0.0 0.0 0.0\n","  0.0]]\n","{'prediction_mean': array([0.4921497 , 0.6278727 , 0.4884422 , 0.53845346, 0.60374033,\n","       0.45361495], dtype=float32), 'prediction_var': array([1.1263266 , 0.40126893, 0.5577736 , 0.6572205 , 0.30810273,\n","       0.56733966], dtype=float32)}\n","67    Thanks for your response.  I'll nominate them ...\n","Name: comment_text, dtype: object\n","[[264 \"Thanks for your response.  I'll nominate them today.\" 0.0 0.0 0.0\n","  0.0 0.0 0.0]]\n","{'prediction_mean': array([0.4910616 , 0.6417325 , 0.5121901 , 0.511001  , 0.5982156 ,\n","       0.49336082], dtype=float32), 'prediction_var': array([0.952214  , 0.45252007, 0.49452758, 0.644773  , 0.36416876,\n","       0.56268173], dtype=float32)}\n","68    Hello everyone I'm just here to tell you that ...\n","Name: comment_text, dtype: object\n","[[265 \"Hello everyone I'm just here to tell you that you're all freaks\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.54019576, 0.68806285, 0.4856674 , 0.49382818, 0.62604916,\n","       0.41644078], dtype=float32), 'prediction_var': array([1.4923859 , 0.41473606, 0.5686155 , 0.58905596, 0.25068706,\n","       0.56893826], dtype=float32)}\n","69    \" \\n :To be blunt: so what?  What does that ha...\n","Name: comment_text, dtype: object\n","[[266\n","  '\" \\n :To be blunt: so what?  What does that have to do with improving the article?   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5237081 , 0.6935977 , 0.4767874 , 0.5287628 , 0.65651035,\n","       0.44312903], dtype=float32), 'prediction_var': array([1.419035  , 0.48405567, 0.59265995, 0.5865971 , 0.26721662,\n","       0.58072245], dtype=float32)}\n","70    :Agreed. Moreover, the recent additions place ...\n","Name: comment_text, dtype: object\n","[[269\n","  ':Agreed. Moreover, the recent additions place an undue emphasis on arguments in court briefs, much of which will become far less salient after the Supreme Court actually decides the case in a few months. –  ·'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48490375, 0.6682624 , 0.4772007 , 0.508621  , 0.606789  ,\n","       0.46741617], dtype=float32), 'prediction_var': array([1.1012563 , 0.38957998, 0.5364944 , 0.61706173, 0.2714366 ,\n","       0.59239006], dtype=float32)}\n","71    Please stop your disruptive editing. If you co...\n","Name: comment_text, dtype: object\n","[[280\n","  'Please stop your disruptive editing. If you continue to vandalize Wikipedia, as you did at Warwick School, you will be blocked from editing.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52887803, 0.7175285 , 0.44232386, 0.5049287 , 0.6823958 ,\n","       0.41579604], dtype=float32), 'prediction_var': array([1.533771  , 0.44879317, 0.6358399 , 0.62829095, 0.2042867 ,\n","       0.5837091 ], dtype=float32)}\n","72    , so I propose merging African Bush Elephant i...\n","Name: comment_text, dtype: object\n","[[282\n","  ', so I propose merging African Bush Elephant into African Savanna Elephant'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53075624, 0.7144346 , 0.43189472, 0.50416386, 0.6563161 ,\n","       0.4514748 ], dtype=float32), 'prediction_var': array([1.5003232 , 0.4212951 , 0.5518831 , 0.69399357, 0.21402314,\n","       0.59681046], dtype=float32)}\n","73    == A question == \\n\\n I dont understand how th...\n","Name: comment_text, dtype: object\n","[[283\n","  '== A question == \\n\\n I dont understand how they play, why they play only 34 games when there are 20 teams => 19 x 2 = 38 games. \\n Portland Timbers played 3 teams with LA Galaxy... how ? \\n Maybe someone can explain to me...'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.50345683, 0.68346214, 0.47300446, 0.49428922, 0.6283562 ,\n","       0.4478548 ], dtype=float32), 'prediction_var': array([1.3517159 , 0.39375407, 0.54780936, 0.59914243, 0.26095125,\n","       0.57818663], dtype=float32)}\n","74    No Sandstein, it is not derogatory. Just becau...\n","Name: comment_text, dtype: object\n","[[291\n","  \"No Sandstein, it is not derogatory. Just because it is not your viewpoint does not change the fact that it is FACTUAL. If it wasn't the terminology would not exist. So get your facts straight before you comment on anything.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53415465, 0.7016093 , 0.4543975 , 0.4984936 , 0.6620673 ,\n","       0.42263108], dtype=float32), 'prediction_var': array([1.4860154 , 0.46310306, 0.5879277 , 0.6013012 , 0.25004423,\n","       0.59213126], dtype=float32)}\n","75    : if the photo is deleted, another troll or th...\n","Name: comment_text, dtype: object\n","[[294\n","  ': if the photo is deleted, another troll or the same one, will simply put it back. \\n : unfortunaly, trolls are everywhere, we just have to be pantient and follow wikipedia policies and etiquete -'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5367478 , 0.7097256 , 0.47032523, 0.48965764, 0.6871206 ,\n","       0.40747964], dtype=float32), 'prediction_var': array([1.3538792 , 0.4875878 , 0.669561  , 0.5868871 , 0.19043206,\n","       0.6836058 ], dtype=float32)}\n","76    :::But right now the book is more notable. The...\n","Name: comment_text, dtype: object\n","[[300\n","  \":::But right now the book is more notable. The film isn't out yet.\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5342629 , 0.7125422 , 0.45567244, 0.5010989 , 0.6663145 ,\n","       0.42730197], dtype=float32), 'prediction_var': array([1.472838  , 0.52479595, 0.6105473 , 0.64739853, 0.20022903,\n","       0.61007094], dtype=float32)}\n","77    (we hate america and we are going to bomb the ...\n","Name: comment_text, dtype: object\n","[[302\n","  '(we hate america and we are going to bomb the shit out of there cities, Queza;s will rain)'\n","  1.0 0.0 1.0 1.0 0.0 0.0]]\n","{'prediction_mean': array([0.48561025, 0.63617396, 0.48591065, 0.5138183 , 0.5855584 ,\n","       0.47280154], dtype=float32), 'prediction_var': array([1.1434784 , 0.39750117, 0.5154388 , 0.6532719 , 0.3064599 ,\n","       0.5653956 ], dtype=float32)}\n","78    == Bold textYOU SUCK!!! == \\n\\n  \\n\\n U SUCK H...\n","Name: comment_text, dtype: object\n","[[305 '== Bold textYOU SUCK!!! == \\n\\n  \\n\\n U SUCK HANNAH MONTANA' 1.0\n","  0.0 1.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.49529272, 0.7087934 , 0.45854947, 0.5094366 , 0.63821816,\n","       0.4646325 ], dtype=float32), 'prediction_var': array([1.1820099 , 0.5532526 , 0.5550911 , 0.6316229 , 0.19303346,\n","       0.5797072 ], dtype=float32)}\n","79    REDIRECT Talk:The Amazing Race 20\n","Name: comment_text, dtype: object\n","[[307 'REDIRECT Talk:The Amazing Race 20' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53957975, 0.68495893, 0.49930716, 0.5047709 , 0.6013894 ,\n","       0.42577857], dtype=float32), 'prediction_var': array([1.4670756 , 0.4501841 , 0.5479315 , 0.6638335 , 0.26926225,\n","       0.504455  ], dtype=float32)}\n","80    :::::::::Moi? Ego? I am mortified that you cou...\n","Name: comment_text, dtype: object\n","[[313\n","  ':::::::::Moi? Ego? I am mortified that you could say such a thing - poor old Mona I always thiught she was a miserable looking woman - probably hormonal.'\n","  1.0 0.0 0.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.53482294, 0.74027026, 0.44705254, 0.49353895, 0.6869644 ,\n","       0.4256712 ], dtype=float32), 'prediction_var': array([1.4769235 , 0.59007215, 0.6136277 , 0.6307397 , 0.18166205,\n","       0.58306354], dtype=float32)}\n","81    \" \\n\\n ==Peter Blum== \\n I woulld like to than...\n","Name: comment_text, dtype: object\n","[[323\n","  '\" \\n\\n ==Peter Blum== \\n I woulld like to thankyou for your very fast assistance on the AfD talk page. I have left appriate messages on . Again, Thankyou  \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53601456, 0.71360743, 0.4632656 , 0.48967266, 0.6970743 ,\n","       0.39959532], dtype=float32), 'prediction_var': array([1.4681749 , 0.47970408, 0.6920471 , 0.56985617, 0.19145373,\n","       0.6572162 ], dtype=float32)}\n","82    *Unfortunately, the site you took the text fro...\n","Name: comment_text, dtype: object\n","[[326\n","  '*Unfortunately, the site you took the text from suffers from really poor grammar and spelling.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53957975, 0.68495893, 0.49930716, 0.5047709 , 0.6013894 ,\n","       0.42577857], dtype=float32), 'prediction_var': array([1.4670756 , 0.4501841 , 0.5479315 , 0.6638335 , 0.26926225,\n","       0.504455  ], dtype=float32)}\n","83    : see Axial tilt\n","Name: comment_text, dtype: object\n","[[328 ': see Axial tilt' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.48919523, 0.71456134, 0.4409612 , 0.5028348 , 0.64932764,\n","       0.46379647], dtype=float32), 'prediction_var': array([1.2237539 , 0.5544033 , 0.5633578 , 0.63036525, 0.18758547,\n","       0.57749087], dtype=float32)}\n","84    HE IS A GHAY ASS FUCKER@@!!\n","Name: comment_text, dtype: object\n","[[349 'HE IS A GHAY ASS FUCKER@@!!' 1.0 0.0 1.0 0.0 1.0 1.0]]\n","{'prediction_mean': array([0.48271605, 0.62266123, 0.52411425, 0.4994211 , 0.58765066,\n","       0.48237902], dtype=float32), 'prediction_var': array([0.9801106 , 0.4156248 , 0.4938797 , 0.704867  , 0.31637606,\n","       0.56426567], dtype=float32)}\n","85    \" \\n :Support: I support the merge.  \"\n","Name: comment_text, dtype: object\n","[[354 '\" \\n :Support: I support the merge.  \"' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.52665687, 0.72635806, 0.4629515 , 0.47558013, 0.684523  ,\n","       0.4369441 ], dtype=float32), 'prediction_var': array([1.5003603 , 0.5067767 , 0.6312527 , 0.6341874 , 0.19091451,\n","       0.64250517], dtype=float32)}\n","86    Cool! Thanks, I'll fix this. \\n Tom\n","Name: comment_text, dtype: object\n","[[356 \"Cool! Thanks, I'll fix this. \\n Tom\" 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5097773 , 0.6614162 , 0.4945848 , 0.52453625, 0.615443  ,\n","       0.45581347], dtype=float32), 'prediction_var': array([1.1358345 , 0.44131705, 0.5259166 , 0.67743933, 0.3178231 ,\n","       0.5497772 ], dtype=float32)}\n","87    Simple: You are stupid!\n","Name: comment_text, dtype: object\n","[[361 'Simple: You are stupid!' 1.0 0.0 1.0 0.0 1.0 0.0]]\n","{'prediction_mean': array([0.538357  , 0.6376467 , 0.525538  , 0.5375099 , 0.6153277 ,\n","       0.43922594], dtype=float32), 'prediction_var': array([1.1200938 , 0.50539935, 0.6061433 , 0.65319616, 0.3701669 ,\n","       0.5153359 ], dtype=float32)}\n","88    The only group the US is unambiguously support...\n","Name: comment_text, dtype: object\n","[[363\n","  'The only group the US is unambiguously supportive of is the Kurds.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5075878 , 0.73419535, 0.43529868, 0.50673586, 0.6524826 ,\n","       0.47667202], dtype=float32), 'prediction_var': array([1.3580315 , 0.47819775, 0.51775753, 0.670295  , 0.208938  ,\n","       0.5141281 ], dtype=float32)}\n","89    \" \\n\\n ::: You have my trust. But trust me on ...\n","Name: comment_text, dtype: object\n","[[370\n","  '\" \\n\\n ::: You have my trust. But trust me on this; checkuser work will just eat you up, burn you out and I guarantee you\\'ll hate it. Seriously -   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.4524966 , 0.58065593, 0.5392122 , 0.53269136, 0.6043453 ,\n","       0.49537742], dtype=float32), 'prediction_var': array([0.7470826 , 0.4784283 , 0.50919294, 0.6261066 , 0.36104295,\n","       0.59815603], dtype=float32)}\n","90    \" \\n\\n == A little late, but... == \\n\\n Andy, ...\n","Name: comment_text, dtype: object\n","[[372\n","  '\" \\n\\n == A little late, but... == \\n\\n Andy, I just noticed this  I feel you should know that Sledge did not delete your comment. I did. Your post was not appropriate anywhere on Wikipedia, even addressed to a \"\"troll\"\" or \"\"idiot\"\".   \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5134158, 0.6572834, 0.5080627, 0.5286264, 0.6070405, 0.4317268],\n","      dtype=float32), 'prediction_var': array([1.3465424 , 0.4385904 , 0.5446844 , 0.64373404, 0.3099572 ,\n","       0.54618126], dtype=float32)}\n","91    \" \\n :Looks similar, similar type of edits to ...\n","Name: comment_text, dtype: object\n","[[375\n","  '\" \\n :Looks similar, similar type of edits to actor/artiste pages and Hindu nationalism related pages. Since the pages themselves don\\'t overlap I can\\'t say that it passes the duck test, but you may want to do a WP:SPI. cheers. —\\'\\'\\'\\'\\'\\' \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5237081 , 0.6935977 , 0.4767874 , 0.5287628 , 0.65651035,\n","       0.44312903], dtype=float32), 'prediction_var': array([1.419035  , 0.48405567, 0.59265995, 0.5865971 , 0.26721662,\n","       0.58072245], dtype=float32)}\n","92    , and ISEP is incresing in prestige even more....\n","Name: comment_text, dtype: object\n","[[376\n","  ', and ISEP is incresing in prestige even more. You should know Porto better, and from were many big fish studied. What a stupid conversation, again...'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.53483045, 0.6996504 , 0.45980304, 0.51146126, 0.6570521 ,\n","       0.4523078 ], dtype=float32), 'prediction_var': array([1.4374105 , 0.47213393, 0.56287324, 0.6135844 , 0.25271353,\n","       0.55970424], dtype=float32)}\n","93    :Great\n","Name: comment_text, dtype: object\n","[[379 ':Great' 0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.4917249 , 0.6206613 , 0.5187485 , 0.52181184, 0.60470754,\n","       0.4978187 ], dtype=float32), 'prediction_var': array([0.9123307 , 0.39038837, 0.533152  , 0.6314067 , 0.31993216,\n","       0.5719736 ], dtype=float32)}\n","94    == Islam and Slavery == \\n\\n Is: Wikipedia:Art...\n","Name: comment_text, dtype: object\n","[[387\n","  '== Islam and Slavery == \\n\\n Is: Wikipedia:Articles for deletion/Islam and Slavery \\n Would you care to vote? Thx.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.49978575, 0.650856  , 0.48915648, 0.51091075, 0.60684264,\n","       0.47446042], dtype=float32), 'prediction_var': array([1.1044264 , 0.34467375, 0.53986514, 0.64812887, 0.27090433,\n","       0.5884104 ], dtype=float32)}\n","95    \" \\n\\n  Random Deletion == \\n\\n Deleted \"\"xana...\n","Name: comment_text, dtype: object\n","[[395\n","  '\" \\n\\n  Random Deletion == \\n\\n Deleted \"\"xanax bars fuck you up mah nigga.. fuck wit em. they good\"\" from the links section.\"'\n","  1.0 0.0 1.0 0.0 1.0 1.0]]\n","{'prediction_mean': array([0.5283014 , 0.6822282 , 0.48946065, 0.51011777, 0.62658477,\n","       0.42599627], dtype=float32), 'prediction_var': array([1.3936021 , 0.45325592, 0.528246  , 0.6350574 , 0.26932132,\n","       0.5866498 ], dtype=float32)}\n","96    ===Referenced content removed too=== \\n Moonri...\n","Name: comment_text, dtype: object\n","[[396\n","  \"===Referenced content removed too=== \\n Moonriddengirl decided to remove information cited to secondary news sources, removing the sources   too. This is all going a bit too far in my view (and I'd normally consider myself a 'deletionist').\"\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5519123 , 0.7214605 , 0.46685642, 0.47446495, 0.68705875,\n","       0.4075905 ], dtype=float32), 'prediction_var': array([1.5847535 , 0.40754956, 0.67893505, 0.61126107, 0.18082738,\n","       0.63895416], dtype=float32)}\n","97    ::: so far you have offered zero information  ...\n","Name: comment_text, dtype: object\n","[[398\n","  '::: so far you have offered zero information  just silly assumptions like a make-believe dispute over a court case you have not studied.'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.5288161 , 0.672261  , 0.49287045, 0.5156442 , 0.6464093 ,\n","       0.38942802], dtype=float32), 'prediction_var': array([1.2516558 , 0.49956045, 0.6304232 , 0.51537097, 0.26517916,\n","       0.5529289 ], dtype=float32)}\n","98    == jjjjjjjjjjjjjjjjjjjjj == \\n\\n caguei aqui hihi\n","Name: comment_text, dtype: object\n","[[402 '== jjjjjjjjjjjjjjjjjjjjj == \\n\\n caguei aqui hihi' 0.0 0.0 0.0 0.0\n","  0.0 0.0]]\n","{'prediction_mean': array([0.49192572, 0.6802631 , 0.45735362, 0.4912691 , 0.6430359 ,\n","       0.46229625], dtype=float32), 'prediction_var': array([1.2101989 , 0.38260287, 0.61131483, 0.60085213, 0.23484516,\n","       0.512791  ], dtype=float32)}\n","99    \" \\n\\n I think the \"\"i\"\" is lowercase. Wikiped...\n","Name: comment_text, dtype: object\n","[[403\n","  '\" \\n\\n I think the \"\"i\"\" is lowercase. Wikipedia capitalised the \"\"i\"\" most likely.  \"'\n","  0.0 0.0 0.0 0.0 0.0 0.0]]\n","{'prediction_mean': array([0.509269  , 0.7278358 , 0.4595914 , 0.49058455, 0.67215466,\n","       0.44806245], dtype=float32), 'prediction_var': array([1.3888521 , 0.46398136, 0.5698017 , 0.6320829 , 0.18885916,\n","       0.61744106], dtype=float32)}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nfFbJFGBkW-8","colab_type":"code","colab":{}},"source":["! cp -r ./test_output \"./gdrive/My Drive/Data/\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M5B_xCGcxazj","colab_type":"code","colab":{}},"source":["print(\"***** Eval results *****\")\n","for key in sorted(result.keys()):\n","  print('  {} = {}'.format(key, str(result[key])))\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3OrESQAVOcT9","colab_type":"code","colab":{}},"source":["predict_features = test_features[:100]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3LbhkBuhn83S","colab":{}},"source":["test_input_fn = input_fn_builder(\n","    features=predict_features, seq_length=max_seq_length, \n","    num_labels = num_labels, is_training=False, drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DW94Qp_XXB4J","colab_type":"code","colab":{}},"source":["# test_scores = np.array([predict_features[i].score for i, val in enumerate(predict_features)])\n","\n","preds = estimator.predict(input_fn=test_input_fn)\n","preds_vals = list(preds)\n","pred_means = np.array([val['prediction_mean'] for key, val in enumerate(preds_vals)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5PsPzmyH6sA","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"RJvlFgZ9j_r6","colab_type":"code","colab":{}},"source":["def total_accuaracy(pred_means, test_scores):\n","  preds = np.round(pred_means)\n","  correct_guesses = np.reshape(preds == test_scores,-1)\n","  return np.sum(correct_guesses) / len(correct_guesses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPZACnkUGrr5","colab_type":"code","colab":{}},"source":["def class_accuaracy(pred_means, test_scores):\n","  preds = np.round(pred_means)\n","  correct_guesses = preds == test_scores\n","  return correct_guesses.sum(axis=0)/ len(correct_guesses)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xVcqWkdHT-s","colab_type":"code","colab":{}},"source":["print(total_accuaracy(pred_means, test_scores))\n","print(class_accuaracy(pred_means, test_scores))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjZUS-Ergg00","colab_type":"code","colab":{}},"source":["preds_vals"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m2flPKNig2zn","colab_type":"code","colab":{}},"source":["test_scores"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZ1zjhdxZByn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2VzhiqkLZB2h","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"75VhsEk5ZB5E","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}