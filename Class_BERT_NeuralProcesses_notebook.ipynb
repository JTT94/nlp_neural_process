{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_BERT_NeuralProcesses_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JTT94/nlp_neural_process/blob/master/Class_BERT_NeuralProcesses_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SZKAwmizhzHo",
        "outputId": "608c4835-a6d9-46bc-cf59-645f7be28f1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime \n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0525 10:56:04.882961 140665273620352 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pgB8X8dti3qc",
        "outputId": "2665f589-b204-4191-aa80-37bcb52a23ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCJDPOgQOsqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialiase tensorboard \n",
        "# from https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab\n",
        "\n",
        "\n",
        "# Get TensorBoard running in the background. \n",
        "LOG_DIR = './test_output'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sZ3Bl48oaxU",
        "colab_type": "code",
        "outputId": "8bff4f8f-8dd0-4f33-d7ea-9dc75aaab24d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "# # #Download and unzip ngrok. \n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-25 10:56:12--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.196.237.103, 52.203.53.176, 35.173.6.94, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.196.237.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16648024 (16M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  15.88M  40.5MB/s    in 0.4s    \n",
            "\n",
            "2019-05-25 10:56:13 (40.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [16648024/16648024]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLp9PmAzoWhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #Launch ngrok background process...\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL1A1ibjOwnL",
        "colab_type": "code",
        "outputId": "2ef18f5f-e240-49f0-a9e4-ebd9e43d3937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://fc7904a4.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bweIS72l5WS5",
        "colab_type": "code",
        "outputId": "5f0ccbb7-23d2-4184-c1d4-69589c992418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "import os, sys\n",
        "sys.path.append('../') # add personal code dir to path for import\n",
        "\n",
        "\n",
        "!test -d neural_process || git clone https://github.com/JTT94/nlp_neural_process.git neural_process\n",
        "sys.path.append('./neural_process/')\n",
        "\n",
        "import random\n",
        "from neural_process import split_context_target, NeuralProcessParams\n",
        "from neural_process.network import *\n",
        "from neural_process.loss import *\n",
        "from neural_process.predict import *\n",
        "from neural_process.process import *\n",
        "\n",
        "from neural_process.tf_model_builder_AUC import *\n",
        "from neural_process.bert_utils import *"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'neural_process'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/101)   \u001b[K\rremote: Counting objects:   1% (2/101)   \u001b[K\rremote: Counting objects:   2% (3/101)   \u001b[K\rremote: Counting objects:   3% (4/101)   \u001b[K\rremote: Counting objects:   4% (5/101)   \u001b[K\rremote: Counting objects:   5% (6/101)   \u001b[K\rremote: Counting objects:   6% (7/101)   \u001b[K\rremote: Counting objects:   7% (8/101)   \u001b[K\rremote: Counting objects:   8% (9/101)   \u001b[K\rremote: Counting objects:   9% (10/101)   \u001b[K\rremote: Counting objects:  10% (11/101)   \u001b[K\rremote: Counting objects:  11% (12/101)   \u001b[K\rremote: Counting objects:  12% (13/101)   \u001b[K\rremote: Counting objects:  13% (14/101)   \u001b[K\rremote: Counting objects:  14% (15/101)   \u001b[K\rremote: Counting objects:  15% (16/101)   \u001b[K\rremote: Counting objects:  16% (17/101)   \u001b[K\rremote: Counting objects:  17% (18/101)   \u001b[K\rremote: Counting objects:  18% (19/101)   \u001b[K\rremote: Counting objects:  19% (20/101)   \u001b[K\rremote: Counting objects:  20% (21/101)   \u001b[K\rremote: Counting objects:  21% (22/101)   \u001b[K\rremote: Counting objects:  22% (23/101)   \u001b[K\rremote: Counting objects:  23% (24/101)   \u001b[K\rremote: Counting objects:  24% (25/101)   \u001b[K\rremote: Counting objects:  25% (26/101)   \u001b[K\rremote: Counting objects:  26% (27/101)   \u001b[K\rremote: Counting objects:  27% (28/101)   \u001b[K\rremote: Counting objects:  28% (29/101)   \u001b[K\rremote: Counting objects:  29% (30/101)   \u001b[K\rremote: Counting objects:  30% (31/101)   \u001b[K\rremote: Counting objects:  31% (32/101)   \u001b[K\rremote: Counting objects:  32% (33/101)   \u001b[K\rremote: Counting objects:  33% (34/101)   \u001b[K\rremote: Counting objects:  34% (35/101)   \u001b[K\rremote: Counting objects:  35% (36/101)   \u001b[K\rremote: Counting objects:  36% (37/101)   \u001b[K\rremote: Counting objects:  37% (38/101)   \u001b[K\rremote: Counting objects:  38% (39/101)   \u001b[K\rremote: Counting objects:  39% (40/101)   \u001b[K\rremote: Counting objects:  40% (41/101)   \u001b[K\rremote: Counting objects:  41% (42/101)   \u001b[K\rremote: Counting objects:  42% (43/101)   \u001b[K\rremote: Counting objects:  43% (44/101)   \u001b[K\rremote: Counting objects:  44% (45/101)   \u001b[K\rremote: Counting objects:  45% (46/101)   \u001b[K\rremote: Counting objects:  46% (47/101)   \u001b[K\rremote: Counting objects:  47% (48/101)   \u001b[K\rremote: Counting objects:  48% (49/101)   \u001b[K\rremote: Counting objects:  49% (50/101)   \u001b[K\rremote: Counting objects:  50% (51/101)   \u001b[K\rremote: Counting objects:  51% (52/101)   \u001b[K\rremote: Counting objects:  52% (53/101)   \u001b[K\rremote: Counting objects:  53% (54/101)   \u001b[K\rremote: Counting objects:  54% (55/101)   \u001b[K\rremote: Counting objects:  55% (56/101)   \u001b[K\rremote: Counting objects:  56% (57/101)   \u001b[K\rremote: Counting objects:  57% (58/101)   \u001b[K\rremote: Counting objects:  58% (59/101)   \u001b[K\rremote: Counting objects:  59% (60/101)   \u001b[K\rremote: Counting objects:  60% (61/101)   \u001b[K\rremote: Counting objects:  61% (62/101)   \u001b[K\rremote: Counting objects:  62% (63/101)   \u001b[K\rremote: Counting objects:  63% (64/101)   \u001b[K\rremote: Counting objects:  64% (65/101)   \u001b[K\rremote: Counting objects:  65% (66/101)   \u001b[K\rremote: Counting objects:  66% (67/101)   \u001b[K\rremote: Counting objects:  67% (68/101)   \u001b[K\rremote: Counting objects:  68% (69/101)   \u001b[K\rremote: Counting objects:  69% (70/101)   \u001b[K\rremote: Counting objects:  70% (71/101)   \u001b[K\rremote: Counting objects:  71% (72/101)   \u001b[K\rremote: Counting objects:  72% (73/101)   \u001b[K\rremote: Counting objects:  73% (74/101)   \u001b[K\rremote: Counting objects:  74% (75/101)   \u001b[K\rremote: Counting objects:  75% (76/101)   \u001b[K\rremote: Counting objects:  76% (77/101)   \u001b[K\rremote: Counting objects:  77% (78/101)   \u001b[K\rremote: Counting objects:  78% (79/101)   \u001b[K\rremote: Counting objects:  79% (80/101)   \u001b[K\rremote: Counting objects:  80% (81/101)   \u001b[K\rremote: Counting objects:  81% (82/101)   \u001b[K\rremote: Counting objects:  82% (83/101)   \u001b[K\rremote: Counting objects:  83% (84/101)   \u001b[K\rremote: Counting objects:  84% (85/101)   \u001b[K\rremote: Counting objects:  85% (86/101)   \u001b[K\rremote: Counting objects:  86% (87/101)   \u001b[K\rremote: Counting objects:  87% (88/101)   \u001b[K\rremote: Counting objects:  88% (89/101)   \u001b[K\rremote: Counting objects:  89% (90/101)   \u001b[K\rremote: Counting objects:  90% (91/101)   \u001b[K\rremote: Counting objects:  91% (92/101)   \u001b[K\rremote: Counting objects:  92% (93/101)   \u001b[K\rremote: Counting objects:  93% (94/101)   \u001b[K\rremote: Counting objects:  94% (95/101)   \u001b[K\rremote: Counting objects:  95% (96/101)   \u001b[K\rremote: Counting objects:  96% (97/101)   \u001b[K\rremote: Counting objects:  97% (98/101)   \u001b[K\rremote: Counting objects:  98% (99/101)   \u001b[K\rremote: Counting objects:  99% (100/101)   \u001b[K\rremote: Counting objects: 100% (101/101)   \u001b[K\rremote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects:   2% (1/40)   \u001b[K\rremote: Compressing objects:   5% (2/40)   \u001b[K\rremote: Compressing objects:   7% (3/40)   \u001b[K\rremote: Compressing objects:  10% (4/40)   \u001b[K\rremote: Compressing objects:  12% (5/40)   \u001b[K\rremote: Compressing objects:  15% (6/40)   \u001b[K\rremote: Compressing objects:  17% (7/40)   \u001b[K\rremote: Compressing objects:  20% (8/40)   \u001b[K\rremote: Compressing objects:  22% (9/40)   \u001b[K\rremote: Compressing objects:  25% (10/40)   \u001b[K\rremote: Compressing objects:  27% (11/40)   \u001b[K\rremote: Compressing objects:  30% (12/40)   \u001b[K\rremote: Compressing objects:  32% (13/40)   \u001b[K\rremote: Compressing objects:  35% (14/40)   \u001b[K\rremote: Compressing objects:  37% (15/40)   \u001b[K\rremote: Compressing objects:  40% (16/40)   \u001b[K\rremote: Compressing objects:  42% (17/40)   \u001b[K\rremote: Compressing objects:  45% (18/40)   \u001b[K\rremote: Compressing objects:  47% (19/40)   \u001b[K\rremote: Compressing objects:  50% (20/40)   \u001b[K\rremote: Compressing objects:  52% (21/40)   \u001b[K\rremote: Compressing objects:  55% (22/40)   \u001b[K\rremote: Compressing objects:  57% (23/40)   \u001b[K\rremote: Compressing objects:  60% (24/40)   \u001b[K\rremote: Compressing objects:  62% (25/40)   \u001b[K\rremote: Compressing objects:  65% (26/40)   \u001b[K\rremote: Compressing objects:  67% (27/40)   \u001b[K\rremote: Compressing objects:  70% (28/40)   \u001b[K\rremote: Compressing objects:  72% (29/40)   \u001b[K\rremote: Compressing objects:  75% (30/40)   \u001b[K\rremote: Compressing objects:  77% (31/40)   \u001b[K\rremote: Compressing objects:  80% (32/40)   \u001b[K\rremote: Compressing objects:  82% (33/40)   \u001b[K\rremote: Compressing objects:  85% (34/40)   \u001b[K\rremote: Compressing objects:  87% (35/40)   \u001b[K\rremote: Compressing objects:  90% (36/40)   \u001b[K\rremote: Compressing objects:  92% (37/40)   \u001b[K\rremote: Compressing objects:  95% (38/40)   \u001b[K\rremote: Compressing objects:  97% (39/40)   \u001b[K\rremote: Compressing objects: 100% (40/40)   \u001b[K\rremote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "Receiving objects:   0% (1/101)   \rReceiving objects:   1% (2/101)   \rReceiving objects:   2% (3/101)   \rReceiving objects:   3% (4/101)   \rReceiving objects:   4% (5/101)   \rReceiving objects:   5% (6/101)   \rReceiving objects:   6% (7/101)   \rReceiving objects:   7% (8/101)   \rReceiving objects:   8% (9/101)   \rReceiving objects:   9% (10/101)   \rReceiving objects:  10% (11/101)   \rReceiving objects:  11% (12/101)   \rReceiving objects:  12% (13/101)   \rReceiving objects:  13% (14/101)   \rReceiving objects:  14% (15/101)   \rReceiving objects:  15% (16/101)   \rReceiving objects:  16% (17/101)   \rReceiving objects:  17% (18/101)   \rReceiving objects:  18% (19/101)   \rReceiving objects:  19% (20/101)   \rReceiving objects:  20% (21/101)   \rReceiving objects:  21% (22/101)   \rReceiving objects:  22% (23/101)   \rReceiving objects:  23% (24/101)   \rReceiving objects:  24% (25/101)   \rReceiving objects:  25% (26/101)   \rReceiving objects:  26% (27/101)   \rReceiving objects:  27% (28/101)   \rReceiving objects:  28% (29/101)   \rReceiving objects:  29% (30/101)   \rReceiving objects:  30% (31/101)   \rReceiving objects:  31% (32/101)   \rReceiving objects:  32% (33/101)   \rReceiving objects:  33% (34/101)   \rReceiving objects:  34% (35/101)   \rReceiving objects:  35% (36/101)   \rReceiving objects:  36% (37/101)   \rReceiving objects:  37% (38/101)   \rReceiving objects:  38% (39/101)   \rReceiving objects:  39% (40/101)   \rReceiving objects:  40% (41/101)   \rReceiving objects:  41% (42/101)   \rReceiving objects:  42% (43/101)   \rReceiving objects:  43% (44/101)   \rReceiving objects:  44% (45/101)   \rReceiving objects:  45% (46/101)   \rReceiving objects:  46% (47/101)   \rReceiving objects:  47% (48/101)   \rReceiving objects:  48% (49/101)   \rReceiving objects:  49% (50/101)   \rReceiving objects:  50% (51/101)   \rReceiving objects:  51% (52/101)   \rReceiving objects:  52% (53/101)   \rReceiving objects:  53% (54/101)   \rReceiving objects:  54% (55/101)   \rReceiving objects:  55% (56/101)   \rReceiving objects:  56% (57/101)   \rReceiving objects:  57% (58/101)   \rReceiving objects:  58% (59/101)   \rReceiving objects:  59% (60/101)   \rReceiving objects:  60% (61/101)   \rReceiving objects:  61% (62/101)   \rReceiving objects:  62% (63/101)   \rReceiving objects:  63% (64/101)   \rReceiving objects:  64% (65/101)   \rReceiving objects:  65% (66/101)   \rReceiving objects:  66% (67/101)   \rReceiving objects:  67% (68/101)   \rReceiving objects:  68% (69/101)   \rReceiving objects:  69% (70/101)   \rReceiving objects:  70% (71/101)   \rReceiving objects:  71% (72/101)   \rReceiving objects:  72% (73/101)   \rReceiving objects:  73% (74/101)   \rReceiving objects:  74% (75/101)   \rremote: Total 101 (delta 61), reused 97 (delta 59), pack-reused 0\u001b[K\n",
            "Receiving objects:  75% (76/101)   \rReceiving objects:  76% (77/101)   \rReceiving objects:  77% (78/101)   \rReceiving objects:  78% (79/101)   \rReceiving objects:  79% (80/101)   \rReceiving objects:  80% (81/101)   \rReceiving objects:  81% (82/101)   \rReceiving objects:  82% (83/101)   \rReceiving objects:  83% (84/101)   \rReceiving objects:  84% (85/101)   \rReceiving objects:  85% (86/101)   \rReceiving objects:  86% (87/101)   \rReceiving objects:  87% (88/101)   \rReceiving objects:  88% (89/101)   \rReceiving objects:  89% (90/101)   \rReceiving objects:  90% (91/101)   \rReceiving objects:  91% (92/101)   \rReceiving objects:  92% (93/101)   \rReceiving objects:  93% (94/101)   \rReceiving objects:  94% (95/101)   \rReceiving objects:  95% (96/101)   \rReceiving objects:  96% (97/101)   \rReceiving objects:  97% (98/101)   \rReceiving objects:  98% (99/101)   \rReceiving objects:  99% (100/101)   \rReceiving objects: 100% (101/101)   \rReceiving objects: 100% (101/101), 66.24 KiB | 3.01 MiB/s, done.\n",
            "Resolving deltas:   0% (0/61)   \rResolving deltas:  34% (21/61)   \rResolving deltas:  54% (33/61)   \rResolving deltas:  55% (34/61)   \rResolving deltas:  68% (42/61)   \rResolving deltas:  70% (43/61)   \rResolving deltas:  73% (45/61)   \rResolving deltas:  88% (54/61)   \rResolving deltas:  90% (55/61)   \rResolving deltas: 100% (61/61)   \rResolving deltas: 100% (61/61), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69MNF4fEi5Ly",
        "outputId": "b7fabd10-2940-4ee1-f6f7-82066e03c5f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "K.tensorflow_backend._get_available_gpus()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDvh4JVNQFck",
        "colab_type": "code",
        "outputId": "a6ef39e2-b984-4274-bb77-b95d6549ceff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cc1rwgOZi9F1",
        "colab": {}
      },
      "source": [
        "## For kaggle dataset\n",
        "filename = './gdrive/My Drive/Data/train.csv'\n",
        "df = pd.read_csv(filename)\n",
        "cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "df = df[cols]\n",
        "\n",
        "score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] \n",
        "text_col_name = 'comment_text'\n",
        "\n",
        "#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "for i in score_column:\n",
        "  df[i] = pd.to_numeric(df[i],downcast='float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p_JIbqhY55L_",
        "outputId": "66d238e8-ff00-488e-ec52-4baa83317233",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Restrict comment length\n",
        "df = df[df.comment_text.str.len() <= 250]\n",
        "df['num_toxic_atts'] = df[cols[1:]].apply(lambda x: np.sum(x), axis = 1)\n",
        "\n",
        "df_toxic = df[df.num_toxic_atts  > 0]\n",
        "# print(len(df_toxic))\n",
        "df_healthy_sample = df[df.num_toxic_atts == 0][:len(df_toxic)]\n",
        "print(len(df_healthy_sample))\n",
        "df_healthy_remaining = df[df.num_toxic_atts == 0][len(df_toxic):]\n",
        "print(len(df_healthy_remaining))\n",
        "\n",
        "df_overrep = pd.concat([df_toxic, df_healthy_sample]).sample(frac=1.0)\n",
        "print(len(df_overrep))\n",
        "train_propn = 0.8\n",
        "\n",
        "ratio = len(df_toxic)/(len(df_healthy_sample)+len(df_healthy_remaining))\n",
        "print(ratio)\n",
        "\n",
        "df_train = df_overrep[:int(len(df_overrep)*train_propn)]\n",
        "print(len(df_train))\n",
        "\n",
        "#construct a test set with similar proportion/imbalance in toxic - nontoxic data to the real data set\n",
        "df_test = pd.concat([df_overrep[int(len(df_overrep)*train_propn):], df_healthy_remaining[:int((1-0.8)*(len(df_healthy_remaining)-len(df_train)))]])\n",
        "\n",
        "print(0.5*len(df_overrep[int(len(df_overrep)*train_propn):]) / len(df_test))\n",
        "# df_train.head(50)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11640\n",
            "67417\n",
            "23280\n",
            "0.14723553891496008\n",
            "18624\n",
            "0.16150964340224783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mUIe15uTi9Ok",
        "outputId": "43ae3943-be29-4042-d3a4-9bd04431418d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0525 10:57:06.193778 140665273620352 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 10:57:08.206205 140665273620352 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IVESPLmgl98I",
        "colab": {}
      },
      "source": [
        "# #Pre process data for bert embedding\n",
        "\n",
        "max_seq_length = 128\n",
        "\n",
        "import pickle\n",
        "\n",
        "train_input_examples = create_examples(df_train, score_column, text_col_name)\n",
        "test_input_examples = create_examples(df_test, score_column, text_col_name)\n",
        "\n",
        "train_features = convert_examples_to_features(train_input_examples, max_seq_length, tokenizer)\n",
        "test_features = convert_examples_to_features(test_input_examples, max_seq_length, tokenizer)\n",
        "\n",
        "pickle.dump(train_features, open('./gdrive/My Drive/Data/train_features.p', 'wb'))\n",
        "pickle.dump(test_features, open('./gdrive/My Drive/Data/test_features.p', 'wb'))\n",
        "\n",
        "# # ## Load features previously saved\n",
        "# train_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))\n",
        "# test_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOdj6CWiPYSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# utility methods\n",
        "from collections import namedtuple\n",
        "NeuralProcessParams = namedtuple('NeuralProcessParams', ['dim_z', 'n_hidden_units_h', 'n_hidden_units_g'])\n",
        "GaussianParams = namedtuple('GaussianParams', ['mu', 'sigma'])\n",
        "\n",
        "\n",
        "def batch_mlp(input, inner_layer_dims, output_dim, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "\n",
        "  output = input\n",
        "\n",
        "  \n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(inner_layer_dims):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(output, output_dim, name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_gK0D7AhoLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embedder(object):\n",
        "  \n",
        "  def __init__(self, BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True):\n",
        "    self.embedder = hub.Module(BERT_model_hub,trainable=trainable)\n",
        "  \n",
        "  def __call__(self, input_ids, input_mask, segment_ids):\n",
        "    embedder = hub.Module(BERT_model_hub,trainable=trainable)\n",
        "    bert_inputs = dict(input_ids=input_ids,\n",
        "                       input_mask=input_mask, \n",
        "                       segment_ids=segment_ids)\n",
        "\n",
        "    bert_outputs = self.embedder(inputs=bert_inputs,\n",
        "                               signature=\"tokens\", \n",
        "                               as_dict=True)\n",
        "    \n",
        "  # Use \"pooled_output\" for classification tasks on an entire sentence. Use \"sequence_outputs\" for token-level output\n",
        "    return bert_outputs[\"pooled_output\"]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HMqLmD8PTEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_dims, num_classes):\n",
        "    self.layer_dims = layer_dims\n",
        "    self.num_classes = num_classes\n",
        "    \n",
        "  def __call__(self, input_xs_embedding, z_samples):\n",
        "  \n",
        "        # inputs dimensions\n",
        "    # z_sample has dim [n_draws, dim_z]\n",
        "    # x_star has dim [N_star, dim_x]\n",
        "    n_draws = z_samples.get_shape().as_list()[0]\n",
        "    n_xs = tf.shape(input_xs_embedding)[0]\n",
        "\n",
        "    # Repeat z samples for each x*\n",
        "    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n",
        "\n",
        "    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n",
        "    z_samples_repeat = tf.tile(z_samples, [1, n_xs, 1])\n",
        "\n",
        "    # Repeat x* for each z sample\n",
        "    x_star_repeat = tf.expand_dims(input_xs_embedding, axis=0)\n",
        "    x_star_repeat = tf.tile(x_star_repeat, [n_draws, 1, 1])\n",
        "\n",
        "    # Concatenate x* and z\n",
        "    inputs = tf.concat([x_star_repeat, z_samples_repeat], axis=2)\n",
        "\n",
        "    # decoder mlp\n",
        "    inner_layer_dims = self.layer_dims\n",
        "    output_dim = self.num_classes *2\n",
        "    hidden = batch_mlp(inputs, inner_layer_dims, output_dim, \"decoder\")\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis= -1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma_star = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "    mu_star = tf.math.sigmoid(mu)\n",
        "\n",
        "\n",
        "    return GaussianParams(mu_star, sigma_star)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6DmGdqzaGjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(object):\n",
        "\n",
        "  def __init__(self, layer_dims, latent_dim):\n",
        "    self.layer_dims = layer_dims\n",
        "    self.latent_dim = latent_dim\n",
        "    \n",
        "  def __call__(self, xs, ys):\n",
        "    \n",
        "    xys = tf.concat([xs, ys], axis=1)\n",
        "\n",
        "\n",
        "    # encoder mlp\n",
        "    inner_layer_dims = self.layer_dims[:-1]\n",
        "    output_dim = self.layer_dims[-1]\n",
        "    rs = batch_mlp(xys, inner_layer_dims, output_dim, \"encoder\")\n",
        "    \n",
        "    # aggregate rs\n",
        "    r = self._aggregate_r(rs)\n",
        "    \n",
        "    # get mu and sigma\n",
        "    z_params = self._get_z_params(r)\n",
        "    \n",
        "    # distribution\n",
        "    dist = tfd.MultivariateNormalDiag(loc=z_params.mu,\n",
        "                                          scale_diag=z_params.sigma)\n",
        "    return dist\n",
        "    \n",
        "  def _aggregate_r(self, context_rs: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Aggregate the output of the encoder to a single representation\n",
        "\n",
        "    Creates an aggregation (mean) operator to combine the encodings of multiple context inputs\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    context_rs\n",
        "        Input encodings tensor, shape: (n_samples, dim_r)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Output tensor of aggregation result\n",
        "    \"\"\"\n",
        "    mean = tf.reduce_mean(context_rs, axis=0)\n",
        "    r = tf.reshape(mean, [1, -1])\n",
        "    return r\n",
        "  \n",
        "  def _get_z_params(self, context_r: tf.Tensor) -> GaussianParams:\n",
        "    \"\"\"Map encoding to mean and covariance of the random variable Z\n",
        "\n",
        "    Creates a linear dense layer to map encoding to mu_z, and another linear mapping + a softplus activation for Sigma_z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    context_r\n",
        "        Input encoding tensor, shape: (1, dim_r)\n",
        "    params\n",
        "        Neural process parameters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Output tensors of the mappings for mu_z and Sigma_z\n",
        "    \"\"\"\n",
        "    hidden = context_r\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self.layer_dims[-1] + self.latent_dim)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      \n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self.latent_dim, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self.latent_dim, name=\"std_layer\")\n",
        "      \n",
        "\n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return GaussianParams(mu, sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qfD5rECp7s2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWZZAs1haFkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = NeuralProcessParams(dim_z=20, n_hidden_units_h=[128, 128, 128], n_hidden_units_g=[128, 128, 128])\n",
        "\n",
        "class NLP_NeuralProcess(object):\n",
        "  \n",
        "  def __init__(self, \n",
        "                  params = NeuralProcessParams(dim_z=20, \n",
        "                                                     n_hidden_units_h=[128, 128, 128], \n",
        "                                                     n_hidden_units_g=[128, 128, 128]),\n",
        "                  num_classes = 6, \n",
        "                  num_draws = 2\n",
        "                 ):\n",
        "    \n",
        "    self.params = params\n",
        "    self.encoder = Encoder(layer_dims = self.params.n_hidden_units_h, \n",
        "                           latent_dim=self.params.dim_z)\n",
        "    self.decoder = Decoder(layer_dims= self.params.n_hidden_units_g, \n",
        "                           num_classes=num_classes)\n",
        "    self.num_draws = num_draws\n",
        "    #self.embedder = Embedder()\n",
        "    \n",
        "  def create_model(self, input_ids, input_mask, segment_ids, scores=None):\n",
        "    \n",
        "    # apply embedder\n",
        "    embedder = hub.Module(BERT_model_hub,trainable=True)\n",
        "    bert_inputs = dict(input_ids=input_ids,\n",
        "                       input_mask=input_mask, \n",
        "                       segment_ids=segment_ids)\n",
        "\n",
        "    embeddings = embedder(inputs=bert_inputs,\n",
        "                               signature=\"tokens\", \n",
        "                               as_dict=True)\n",
        "\n",
        "    tf.logging.info(embeddings)\n",
        "    \n",
        "    # total x,y \n",
        "    x_all = embeddings[\"pooled_output\"]\n",
        "    y_all = scores\n",
        "    \n",
        "    # context split\n",
        "    context_xs, context_ys, target_xs, target_ys = self.context_target_split(x_all, y_all, batch_size =32)\n",
        "    \n",
        "    # get encoding params\n",
        "    context_z_dist = self.encoder(context_xs, context_ys)\n",
        "    all_z_dist = self.encoder(x_all, y_all)\n",
        "    \n",
        "    # get internal representation\n",
        "    mean_zero = tf.constant(np.repeat(0., params.dim_z))\n",
        "    epsilon_dist = tfd.MultivariateNormalDiag(loc= mean_zero)                            \n",
        "    epsilon = tf.expand_dims(epsilon_dist.sample(self.num_draws), axis=1)\n",
        "    epsilon = tf.cast(epsilon, tf.float32)\n",
        "    # context representation with target xs\n",
        "    y_pred = self.decoder(target_xs, context_z_dist.sample(self.num_draws))\n",
        "    prior_predict = self.decoder(x_all, epsilon)\n",
        "   \n",
        "    # loss\n",
        "    loglike = self.loglikelihood(target_ys, y_pred)\n",
        "    KL_loss = self.KLqp_gaussian(all_z_dist.parameters['loc'], \n",
        "                                 all_z_dist.parameters['scale_diag'], \n",
        "                                 context_z_dist.parameters['loc'], \n",
        "                                 context_z_dist.parameters['scale_diag'])\n",
        "    loss = tf.negative(loglike) + KL_loss\n",
        "    \n",
        "    return (loss, y_pred, prior_predict, y_all)\n",
        "  \n",
        "  def context_target_split(self, xs, ys, batch_size =32):\n",
        "    btch_sz = tf.minimum(tf.constant(batch_size), tf.shape(xs)[0])\n",
        "    n_context = tf.random_shuffle(tf.range(1,btch_sz))[0]\n",
        "    \n",
        "    indices = tf.range(0, btch_sz)\n",
        "    context_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context))\n",
        "    target_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context, btch_sz))\n",
        "    \n",
        "    context_xs = tf.gather(xs,context_set_indices)\n",
        "    context_ys = tf.gather(ys, context_set_indices)\n",
        "    target_xs = tf.gather(xs,target_set_indices)\n",
        "    target_ys = tf.gather(ys,target_set_indices)\n",
        "    \n",
        "    return context_xs, context_ys, target_xs, target_ys\n",
        "  \n",
        "  def loglikelihood(self, y_star: tf.Tensor, dist):\n",
        "    \"\"\"Log-likelihood of an output given a predicted \"\"\"\n",
        "    p_normal = tfd.MultivariateNormalDiag(loc = dist.mu, scale_diag=dist.sigma)\n",
        "    loglike = p_normal.log_prob(y_star)\n",
        "    loglike = tf.reduce_sum(loglike, axis=0)\n",
        "    loglike = tf.reduce_mean(loglike)\n",
        "    return loglike\n",
        "  \n",
        "  def KLqp_gaussian(self, mu_q: tf.Tensor, sigma_q: tf.Tensor, mu_p: tf.Tensor, sigma_p: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Kullback-Leibler divergence between two Gaussian distributions\n",
        "\n",
        "    Determines KL(q || p) = < log( q / p ) >_q\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu_q\n",
        "        Mean tensor of distribution q, shape: (1, dim)\n",
        "    sigma_q\n",
        "        Variance tensor of distribution q, shape: (1, dim)\n",
        "    mu_p\n",
        "        Mean tensor of distribution p, shape: (1, dim)\n",
        "    sigma_p\n",
        "        Variance tensor of distribution p, shape: (1, dim)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        KL tensor, shape: (1)\n",
        "    \"\"\"\n",
        "    sigma2_q = tf.square(sigma_q) + 1e-16\n",
        "    sigma2_p = tf.square(sigma_p) + 1e-16\n",
        "    temp = sigma2_q / sigma2_p + tf.square(mu_q - mu_p) / sigma2_p - 1.0 + tf.log(sigma2_p / sigma2_q + 1e-16)\n",
        "    return 0.5 * tf.reduce_sum(temp)\n",
        "  \n",
        "  \n",
        "  def model_fn_builder(self, num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "      \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "      def model_fn(features, mode, params):  # pylint: disable=unused-argument\n",
        "          \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "          input_ids = features[\"input_ids\"]\n",
        "          input_mask = features[\"input_mask\"]\n",
        "          segment_ids = features[\"segment_ids\"]    \n",
        "          scores = features[\"scores\"]\n",
        "\n",
        "\n",
        "\n",
        "          (loss, posterior_predict, prior_predict, true_y) = self.create_model(input_ids, input_mask, segment_ids, scores)\n",
        "\n",
        "\n",
        "          train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "\n",
        "          # output from model\n",
        "          ystar, variance = tf.nn.moments(posterior_predict.mu,[0])\n",
        "          prior_ystar, prior_variance = tf.nn.moments(prior_predict.mu,[0])\n",
        "          print(prior_ystar)\n",
        "          print(true_y)\n",
        "          \n",
        "          # Calculate evaluation metrics.\n",
        "          eval_metrics = {}\n",
        "\n",
        "          # AUC\n",
        "          def metric_fn(pred_scores, real_scores, trait_num):\n",
        "              auc_value = tf.metrics.auc(real_scores[:,trait_num], pred_scores[:,trait_num])\n",
        "              accuracy_value = tf.metrics.accuracy(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n",
        "              return {\"auc\"+str(trait_num): auc_value, \"accuracy\"+str(trait_num): accuracy_value}\n",
        "\n",
        "          labels = true_y # need to round them if true labels are not 1 or 0\n",
        "          eval_metrics_lst = [metric_fn(prior_ystar, labels, trait_num) for trait_num in range(num_labels)]\n",
        "\n",
        "          for d in eval_metrics_lst:\n",
        "              tf.summary.scalar(list(d.keys())[0], list(d.values())[0][1]) # make available to tensorboard\n",
        "              eval_metrics.update(d)\n",
        "\n",
        "\n",
        "          # returns \n",
        "          if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "\n",
        "          elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode, predictions={'prediction_mean': prior_ystar, 'prediction_var': prior_variance})\n",
        "\n",
        "\n",
        "          else:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode, eval_metric_ops=eval_metrics)\n",
        "\n",
        "      return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8iSEczc6NS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tf.reset_default_graph()\n",
        "neural_process = NLP_NeuralProcess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1VHHN1pi7Wa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dist = tfd.MultivariateNormalDiag(loc = np.repeat(0., 10))\n",
        "sess = tf.Session()\n",
        "param = GaussianParams(mu=np.array([1.,2.,3.]), sigma=np.array([1.,1.,1.]))\n",
        "\n",
        "tf.expand_dims(dist.sample(2), axis=1)\n",
        "\n",
        "dist.parameters['scale_diag']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3K9kf-1DlHJA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1471
        },
        "outputId": "d3fcc0b8-3c66-486c-e807-06d480342f80"
      },
      "source": [
        "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
        "batch_size = 32\n",
        "# lr = 2e-2\n",
        "lr = 2e-5\n",
        "epochs = 6.0\n",
        "# Warmup is a period of time where the learning rate  is small and gradually increases\n",
        "warmpup_proportion = 0.1\n",
        "\n",
        "# Compute # train and warmup steps from batch size\n",
        "num_train_steps = int(len(train_features) / batch_size *epochs)\n",
        "num_warmup_steps = int(num_train_steps * warmpup_proportion)\n",
        "\n",
        "#####\n",
        "\n",
        "output_dir = \"./test_output\"\n",
        "save_checkpoints_steps = 500\n",
        "save_summary_steps = 100\n",
        "\n",
        "# Specify outpit directory and number of checkpoint steps to save\n",
        "run_config = tf.estimator.RunConfig(model_dir=output_dir,\n",
        "    save_summary_steps=save_summary_steps, save_checkpoints_steps=save_checkpoints_steps)\n",
        "\n",
        "#####\n",
        "\n",
        "num_labels = 6\n",
        "\n",
        "model_fn = neural_process.model_fn_builder(num_labels = num_labels, learning_rate=lr,\n",
        "  num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n",
        "  params={\"batch_size\": batch_size})\n",
        "\n",
        "#####\n",
        "\n",
        "# Create an input function for training. drop_remainder = True for using TPUs.\n",
        "train_input_fn = input_fn_builder(\n",
        "    features=train_features, seq_length=max_seq_length, \n",
        "    num_labels = num_labels, is_training=True, drop_remainder=False)\n",
        "\n",
        "#####\n",
        "\n",
        "print('Beginning Training!')\n",
        "current_time = datetime.now()\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print(\"Training took time \", datetime.now() - current_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': './test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee808d9160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:00.554350 140665273620352 estimator.py:201] Using config: {'_model_dir': './test_output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fee808d9160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Beginning Training!\n",
            "INFO:tensorflow:Calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:10.616548 140665273620352 estimator.py:1111] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:13.084221 140665273620352 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:{'sequence_output': <tf.Tensor 'module_apply_tokens/bert/encoder/Reshape_13:0' shape=(?, ?, 768) dtype=float32>, 'pooled_output': <tf.Tensor 'module_apply_tokens/bert/pooler/dense/Tanh:0' shape=(?, 768) dtype=float32>}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:13.203857 140665273620352 <ipython-input-26-64af29b8f438>:33] {'sequence_output': <tf.Tensor 'module_apply_tokens/bert/encoder/Reshape_13:0' shape=(?, ?, 768) dtype=float32>, 'pooled_output': <tf.Tensor 'module_apply_tokens/bert/pooler/dense/Tanh:0' shape=(?, 768) dtype=float32>}\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"moments_1/Squeeze:0\", shape=(?, 6), dtype=float32)\n",
            "Tensor(\"IteratorGetNext:2\", shape=(?, 6), dtype=float32)\n",
            "INFO:tensorflow:Done calling model_fn.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:24.908412 140665273620352 estimator.py:1113] Done calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Create CheckpointSaverHook.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:24.911662 140665273620352 basic_session_run_hooks.py:527] Create CheckpointSaverHook.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Graph was finalized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:27.392505 140665273620352 monitored_session.py:222] Graph was finalized.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./test_output/model.ckpt-0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:27.403710 140665273620352 saver.py:1270] Restoring parameters from ./test_output/model.ckpt-0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:30.188381 140665273620352 session_manager.py:491] Running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done running local_init_op.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:30.516764 140665273620352 session_manager.py:493] Done running local_init_op.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 0 into ./test_output/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:10:46.552401 140665273620352 basic_session_run_hooks.py:594] Saving checkpoints for 0 into ./test_output/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 12.08922, step = 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:11:04.610977 140665273620352 basic_session_run_hooks.py:249] loss = 12.08922, step = 0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.00534\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:12:44.078731 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.00534\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = 6.0371037, step = 100 (99.472 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:12:44.083027 140665273620352 basic_session_run_hooks.py:247] loss = 6.0371037, step = 100 (99.472 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17376\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:14:09.275090 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17376\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -0.7543148, step = 200 (85.196 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:14:09.279277 140665273620352 basic_session_run_hooks.py:247] loss = -0.7543148, step = 200 (85.196 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:15:34.644200 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17138\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -1.2532037, step = 300 (85.367 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:15:34.646464 140665273620352 basic_session_run_hooks.py:247] loss = -1.2532037, step = 300 (85.367 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17221\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:16:59.953150 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17221\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -6.5112076, step = 400 (85.315 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:16:59.961364 140665273620352 basic_session_run_hooks.py:247] loss = -6.5112076, step = 400 (85.315 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 500 into ./test_output/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:18:24.539054 140665273620352 basic_session_run_hooks.py:594] Saving checkpoints for 500 into ./test_output/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.04831\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:18:35.344739 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.04831\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -11.734198, step = 500 (95.387 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:18:35.348337 140665273620352 basic_session_run_hooks.py:247] loss = -11.734198, step = 500 (95.387 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.16664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:20:01.060735 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.16664\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -11.750626, step = 600 (85.717 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:20:01.065515 140665273620352 basic_session_run_hooks.py:247] loss = -11.750626, step = 600 (85.717 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17033\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:21:26.506646 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17033\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -9.89584, step = 700 (85.443 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:21:26.508804 140665273620352 basic_session_run_hooks.py:247] loss = -9.89584, step = 700 (85.443 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:22:51.918920 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17079\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -10.912644, step = 800 (85.412 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:22:51.921283 140665273620352 basic_session_run_hooks.py:247] loss = -10.912644, step = 800 (85.412 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.17081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:24:17.329931 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.17081\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -9.111476, step = 900 (85.413 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:24:17.334304 140665273620352 basic_session_run_hooks.py:247] loss = -9.111476, step = 900 (85.413 sec)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Saving checkpoints for 1000 into ./test_output/model.ckpt.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:25:41.816960 140665273620352 basic_session_run_hooks.py:594] Saving checkpoints for 1000 into ./test_output/model.ckpt.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global_step/sec: 1.05213\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:25:52.374772 140665273620352 basic_session_run_hooks.py:680] global_step/sec: 1.05213\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:loss = -1.5108039, step = 1000 (95.043 sec)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0525 11:25:52.377210 140665273620352 basic_session_run_hooks.py:247] loss = -1.5108039, step = 1000 (95.043 sec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfFbJFGBkW-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp -r ./test_output \"./gdrive/My Drive/Data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dFExnumOEgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_steps = int(len(df_test) / batch_size)\n",
        "eval_input_fn = input_fn_builder(\n",
        "    features=test_features,\n",
        "    seq_length=max_seq_length,\n",
        "    num_labels = num_labels,\n",
        "    is_training=False,\n",
        "    drop_remainder=True)\n",
        "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5B_xCGcxazj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"***** Eval results *****\")\n",
        "for key in sorted(result.keys()):\n",
        "  print('  {} = {}'.format(key, str(result[key])))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OrESQAVOcT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_features = test_features[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3LbhkBuhn83S",
        "colab": {}
      },
      "source": [
        "test_input_fn = input_fn_builder(\n",
        "    features=predict_features, seq_length=max_seq_length, \n",
        "    num_labels = num_labels, is_training=False, drop_remainder=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW94Qp_XXB4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores = np.array([predict_features[i].score for i, val in enumerate(predict_features)])\n",
        "\n",
        "preds = estimator.predict(input_fn=test_input_fn)\n",
        "preds_vals = list(preds)\n",
        "pred_means = np.array([val['prediction_mean'] for key, val in enumerate(preds_vals)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5PsPzmyH6sA",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJvlFgZ9j_r6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_accuaracy(pred_means, test_scores):\n",
        "  preds = np.round(pred_means)\n",
        "  correct_guesses = np.reshape(preds == test_scores,-1)\n",
        "  return np.sum(correct_guesses) / len(correct_guesses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPZACnkUGrr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_accuaracy(pred_means, test_scores):\n",
        "  preds = np.round(pred_means)\n",
        "  correct_guesses = preds == test_scores\n",
        "  return correct_guesses.sum(axis=0)/ len(correct_guesses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xVcqWkdHT-s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(total_accuaracy(pred_means, test_scores))\n",
        "print(class_accuaracy(pred_means, test_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjZUS-Ergg00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2flPKNig2zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}