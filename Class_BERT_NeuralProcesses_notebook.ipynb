{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class_BERT_NeuralProcesses_notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JTT94/nlp_neural_process/blob/master/Class_BERT_NeuralProcesses_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SZKAwmizhzHo",
        "outputId": "f76496d7-8226-4dab-9f4c-255e5351ecc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "import os\n",
        "import re\n",
        "\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import string\n",
        "from datetime import datetime \n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability import distributions as tfd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7PWS-GcrZbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pgB8X8dti3qc",
        "outputId": "9353d3fd-e56a-4050-b47d-7c1753fbb9e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "!pip install bert-tensorflow\n",
        "import bert\n",
        "from bert import run_classifier\n",
        "from bert import optimization\n",
        "from bert import tokenization\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 11:04:44.523133 139698332669824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jDvh4JVNQFck",
        "outputId": "05c38357-c26d-42f9-dcda-4b305f5efcf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6BzlNM53sIBp",
        "colab": {}
      },
      "source": [
        "# output_directory = \"./gdrive/My Drive/Kaggle_toxic_comments/test_output\"\n",
        "# output_directory = \"./test_output\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CKlcN3PR6RL",
        "colab_type": "code",
        "outputId": "c4780d3a-3c3e-4d4c-baa0-40b2b4ffb182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "DATAFLAG = 'unhealthy'\n",
        "BUCKET = 'rail_jigsaw_modeling' #@param {type:\"string\"}\n",
        "MODEL = 'batch32' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/{}/models/{}'.format(BUCKET, DATAFLAG, MODEL)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Model output directory: gs://rail_jigsaw_modeling/unhealthy/models/batch32 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZCJDPOgQOsqJ",
        "colab": {}
      },
      "source": [
        "# initialiase tensorboard \n",
        "# from https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab\n",
        "\n",
        "\n",
        "# Get TensorBoard running in the background. \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(OUTPUT_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5sZ3Bl48oaxU",
        "outputId": "1638993a-b58b-4847-d526-02ffcdc73e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "# # #Download and unzip ngrok. \n",
        "!test -e ngrok-stable-linux-amd64.zip || wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!test -e ngrok || unzip ngrok-stable-linux-amd64.zip\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-08-08 11:05:39--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 34.195.49.195, 52.22.236.254, 52.204.136.9, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|34.195.49.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  37.6MB/s    in 0.3s    \n",
            "\n",
            "2019-08-08 11:05:39 (37.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLp9PmAzoWhE",
        "colab": {}
      },
      "source": [
        "# #Launch ngrok background process...\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eL1A1ibjOwnL",
        "outputId": "e22300b8-18dc-475e-cf3f-5e06d5ad9a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://7708afdf.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bweIS72l5WS5",
        "outputId": "093f2ff7-84d9-4b63-ebe2-ad81bbb9202d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "import os, sys\n",
        "sys.path.append('../') # add personal code dir to path for import\n",
        "\n",
        "\n",
        "!test -d neural_process || git clone https://github.com/JTT94/nlp_neural_process.git neural_process\n",
        "sys.path.append('./neural_process/')\n",
        "\n",
        "import random\n",
        "from neural_process import split_context_target, NeuralProcessParams\n",
        "from neural_process.network import *\n",
        "from neural_process.loss import *\n",
        "from neural_process.predict import *\n",
        "from neural_process.process import *\n",
        "\n",
        "from neural_process.tf_model_builder_AUC import *\n",
        "from neural_process.bert_utils import *"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'neural_process'...\n",
            "remote: Enumerating objects: 152, done.\u001b[K\n",
            "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 152 (delta 93), reused 132 (delta 79), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (152/152), 794.27 KiB | 5.29 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69MNF4fEi5Ly",
        "outputId": "266cc691-6274-422a-c731-cd489f9400e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "# Initialize session\n",
        "sess = tf.Session()\n",
        "K.set_session(sess)\n",
        "K.tensorflow_backend._get_available_gpus()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0808 11:05:45.509979 139698332669824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0808 11:05:45.511689 139698332669824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cc1rwgOZi9F1",
        "colab": {}
      },
      "source": [
        "# # filename = './cleaned_data.csv'\n",
        "# filename = './gdrive/My Drive/Oxford/RAIL/Jigsaw/Technical Team/toxic_comments/Data/data1.csv'\n",
        "# df = pd.read_csv(filename, index_col=0)\n",
        "# cols = ['comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic']\n",
        "# # cols = ['comment','cleaned_comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', 'healthy']\n",
        "# df = df[cols]\n",
        "\n",
        "# score_column = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic']\n",
        "# # text_col_name = 'cleaned_comment'\n",
        "# text_col_name = 'comment'\n",
        "\n",
        "# df_train = df.sample(frac = 0.8)\n",
        "# df_test = df[~df.isin(df_train)].dropna()\n",
        "\n",
        "# df_train[score_column] = df_train[score_column].round()\n",
        "# df_test[score_column] = df_test[score_column].round()\n",
        "# #--------\n",
        "\n",
        "# # ### For Unhealthy dataset\n",
        "\n",
        "cols = ['comment','antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic', '_trusted_judgments']\n",
        "score_column = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'hostile', 'sarcastic']\n",
        "text_col_name = 'comment'\n",
        "\n",
        "## Training data\n",
        "\n",
        "train_filename = './gdrive/My Drive/unhealthy_rail/unhealthy_train.csv'\n",
        "\n",
        "df_train = pd.read_csv(train_filename)\n",
        "df_train = df_train[cols].round()\n",
        "\n",
        "#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "for i in score_column:\n",
        "  df_train[i] = pd.to_numeric(df_train[i],downcast='float')\n",
        "\n",
        "  \n",
        "## Validation data\n",
        "\n",
        "val_filename = './gdrive/My Drive/unhealthy_rail/unhealthy_val.csv'\n",
        "\n",
        "df_val = pd.read_csv(val_filename)\n",
        "df_val = df_val[cols].round()\n",
        "\n",
        "#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "for i in score_column:\n",
        "  df_val[i] = pd.to_numeric(df_val[i],downcast='float')\n",
        "\n",
        "## Test data\n",
        "\n",
        "test_filename = './gdrive/My Drive/unhealthy_rail/unhealthy_test.csv'\n",
        "\n",
        "df_test = pd.read_csv(test_filename)\n",
        "df_test = df_test[cols].round()\n",
        "\n",
        "#Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "for i in score_column:\n",
        "  df_test[i] = pd.to_numeric(df_test[i],downcast='float')\n",
        "\n",
        "\n",
        "\n",
        "# # # ### For kaggle dataset\n",
        "\n",
        "# ## Training data\n",
        "\n",
        "# # filename = './gdrive/My Drive/Data/train.csv'\n",
        "# train_filename = './gdrive/My Drive/Kaggle_toxic_comments/finalSplit/train_kaggle.csv'\n",
        "\n",
        "# df = pd.read_csv(train_filename)\n",
        "# cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# df = df[cols]\n",
        "\n",
        "# score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# text_col_name = 'comment_text'\n",
        "\n",
        "# #Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "# for i in score_column:\n",
        "#   df[i] = pd.to_numeric(df[i],downcast='float')\n",
        "\n",
        "# df_train = df[df.comment_text.str.len() <= 250]\n",
        "\n",
        "# ## Validation data\n",
        "\n",
        "# # filename = './gdrive/My Drive/Data/train.csv'\n",
        "# val_filename = './gdrive/My Drive/Kaggle_toxic_comments/finalSplit/val_kaggle.csv'\n",
        "\n",
        "# df = pd.read_csv(val_filename)\n",
        "# cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# df = df[cols]\n",
        "\n",
        "# score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# text_col_name = 'comment_text'\n",
        "\n",
        "# #Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "# for i in score_column:\n",
        "#   df[i] = pd.to_numeric(df[i],downcast='float')\n",
        "\n",
        "# df_val = df[df.comment_text.str.len() <= 250]\n",
        "\n",
        "# ## Test data\n",
        "\n",
        "# # filename = './gdrive/My Drive/Data/train.csv'\n",
        "# test_filename = './gdrive/My Drive/Kaggle_toxic_comments/finalSplit/test_kaggle.csv'\n",
        "\n",
        "# df = pd.read_csv(test_filename)\n",
        "# cols = ['comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "# df = df[cols]\n",
        "\n",
        "# score_column = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "# text_col_name = 'comment_text'\n",
        "\n",
        "# #Cast to float - because scores and labels need to be concattenated in the model function, and so need to be same type\n",
        "# for i in score_column:\n",
        "#   df[i] = pd.to_numeric(df[i],downcast='float')\n",
        "\n",
        "# df_test = df[df.comment_text.str.len() <= 250]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i2roS1iGnUvH",
        "colab": {}
      },
      "source": [
        "# data = pd.concat([df_train,test_df])\n",
        "\n",
        "# data =  data.sample(frac = 1.0)\n",
        "\n",
        "# msk = np.random.rand(len(data)) < 0.8\n",
        "\n",
        "# train = data[msk]\n",
        "\n",
        "# temp = data[~msk]\n",
        "\n",
        "# msk2 = np.random.rand(len(temp)) <= 0.5\n",
        "\n",
        "# val = temp[msk2]\n",
        "\n",
        "# test = temp[~msk2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "coSxH1Vgnw_H",
        "colab": {}
      },
      "source": [
        "# train.to_csv(\"train_kaggle.csv\")\n",
        "# test.to_csv(\"test_kaggle.csv\")\n",
        "# val.to_csv(\"val_kaggle.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mUIe15uTi9Ok",
        "outputId": "ae8948a9-46e5-4c37-f024-eb649f483d0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# This is a path to an uncased (all lowercase) version of BERT\n",
        "BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0808 11:05:55.061700 139698332669824 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "W0808 11:05:55.204028 139698332669824 deprecation_wrapper.py:119] From ./neural_process/neural_process/bert_utils.py:39: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0808 11:05:57.029646 139698332669824 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IVESPLmgl98I",
        "colab": {}
      },
      "source": [
        "# #Pre process data for bert embedding\n",
        "max_seq_length = 128\n",
        "\n",
        "import pickle\n",
        "\n",
        "# train_input_examples = create_examples(df_train.sample(100), score_column, text_col_name)\n",
        "# # # test_input_examples = create_examples(df_test, score_column, text_col_name)\n",
        "\n",
        "# train_features = convert_examples_to_features(train_input_examples, max_seq_length, tokenizer)\n",
        "# # # test_features = convert_examples_to_features(test_input_examples, max_seq_length, tokenizer)\n",
        "\n",
        "# pickle.dump(train_features, open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'wb'))\n",
        "# # pickle.dump(test_features, open('./gdrive/My Drive/Kaggle_toxic_comments/test_features.p', 'wb'))\n",
        "\n",
        "# # ## Load features previously saved\n",
        "# train_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))\n",
        "# test_features = pickle.load(open('./gdrive/My Drive/Kaggle_toxic_comments/train_features.p', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wOdj6CWiPYSX",
        "colab": {}
      },
      "source": [
        "# utility methods\n",
        "from collections import namedtuple\n",
        "NeuralProcessParams = namedtuple('NeuralProcessParams', ['dim_z', 'n_hidden_units_h', 'n_hidden_units_g'])\n",
        "GaussianParams = namedtuple('GaussianParams', ['mu', 'sigma'])\n",
        "\n",
        "\n",
        "def batch_mlp(input, inner_layer_dims, output_dim, variable_scope):\n",
        "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
        "  \n",
        "  Args:\n",
        "    input: input tensor of shape [B,n,d_in].\n",
        "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
        "        in `basic.Linear`.\n",
        "    variable_scope: String giving the name of the variable scope. If this is set\n",
        "        to be the same as a previously defined MLP, then the weights are reused.\n",
        "    \n",
        "  Returns:\n",
        "    tensor of shape [B,n,d_out] where d_out=output_sizes[-1]\n",
        "  \"\"\"\n",
        "  # Get the shapes of the input and reshape to parallelise across observations\n",
        "\n",
        "  output = input\n",
        "\n",
        "  \n",
        "  # Pass through MLP\n",
        "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
        "    for i, size in enumerate(inner_layer_dims):\n",
        "      output = tf.nn.relu(\n",
        "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
        "\n",
        "    # Last layer without a ReLu\n",
        "    output = tf.layers.dense(output, output_dim, name=\"layer_{}\".format(i + 1))\n",
        "\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_gK0D7AhoLK",
        "colab": {}
      },
      "source": [
        "# class Embedder(object):\n",
        "  \n",
        "#   def __init__(self, BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True):\n",
        "#     self.BERT_model_hub = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
        "#     self.tokenizer = create_tokenizer_from_hub_module(BERT_model_hub)\n",
        "#     self.trainable = trainable\n",
        "    \n",
        "#   def __call__(self, input_ids, input_mask, segment_ids):\n",
        "#     embedder = hub.Module(self.BERT_model_hub,trainable=self.trainable)\n",
        "#     bert_inputs = dict(input_ids=input_ids,\n",
        "#                        input_mask=input_mask, \n",
        "#                        segment_ids=segment_ids)\n",
        "\n",
        "#     bert_outputs = embedder(inputs=bert_inputs,\n",
        "#                                signature=\"tokens\", \n",
        "#                                as_dict=True)\n",
        "    \n",
        "#   # Use \"pooled_output\" for classification tasks on an entire sentence. Use \"sequence_outputs\" for token-level output\n",
        "#     return bert_outputs[\"pooled_output\"]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2HMqLmD8PTEP",
        "colab": {}
      },
      "source": [
        "class Decoder(object):\n",
        "  \"\"\"The Decoder.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_dims, num_classes):\n",
        "    self.layer_dims = layer_dims\n",
        "    self.num_classes = num_classes\n",
        "    \n",
        "  def __call__(self, input_xs_embedding, z_samples):\n",
        "  \n",
        "        # inputs dimensions\n",
        "    # z_sample has dim [n_draws, dim_z]\n",
        "    # x_star has dim [N_star, dim_x]\n",
        "    n_draws = z_samples.get_shape().as_list()[0]\n",
        "    n_xs = tf.shape(input_xs_embedding)[0]\n",
        "\n",
        "    # Repeat z samples for each x*\n",
        "    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n",
        "\n",
        "    #z_samples_repeat = tf.expand_dims(z_samples, axis=1)\n",
        "    z_samples_repeat = tf.tile(z_samples, [1, n_xs, 1])\n",
        "\n",
        "    # Repeat x* for each z sample\n",
        "    x_star_repeat = tf.expand_dims(input_xs_embedding, axis=0)\n",
        "    x_star_repeat = tf.tile(x_star_repeat, [n_draws, 1, 1])\n",
        "\n",
        "    # Concatenate x* and z\n",
        "    inputs = tf.concat([x_star_repeat, z_samples_repeat], axis=2)\n",
        "\n",
        "    # decoder mlp\n",
        "    inner_layer_dims = self.layer_dims\n",
        "    output_dim = self.num_classes *2\n",
        "    hidden = batch_mlp(inputs, inner_layer_dims, output_dim, \"decoder\")\n",
        "\n",
        "    # Get the mean an the variance\n",
        "    mu, log_sigma = tf.split(hidden, 2, axis= -1)\n",
        "\n",
        "    # Bound the variance\n",
        "    sigma_star = 0.1 + 0.9 * tf.nn.softplus(log_sigma)\n",
        "    mu_star = tf.math.sigmoid(mu)\n",
        "\n",
        "\n",
        "    return GaussianParams(mu_star, sigma_star)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L6DmGdqzaGjS",
        "colab": {}
      },
      "source": [
        "class Encoder(object):\n",
        "\n",
        "  def __init__(self, layer_dims, latent_dim):\n",
        "    self.layer_dims = layer_dims\n",
        "    self.latent_dim = latent_dim\n",
        "    \n",
        "  def __call__(self, xs, ys):\n",
        "    print(xs)\n",
        "    print(ys)\n",
        "    xys = tf.concat([xs, ys], axis=1)\n",
        "\n",
        "\n",
        "    # encoder mlp\n",
        "    inner_layer_dims = self.layer_dims[:-1]\n",
        "    output_dim = self.layer_dims[-1]\n",
        "    rs = batch_mlp(xys, inner_layer_dims, output_dim, \"encoder\")\n",
        "    \n",
        "    # aggregate rs\n",
        "    r = self._aggregate_r(rs)\n",
        "    \n",
        "    # get mu and sigma\n",
        "    z_params = self._get_z_params(r)\n",
        "    \n",
        "    # distribution\n",
        "    dist = tfd.MultivariateNormalDiag(loc=z_params.mu,\n",
        "                                          scale_diag=z_params.sigma)\n",
        "    return dist\n",
        "    \n",
        "  def _aggregate_r(self, context_rs: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Aggregate the output of the encoder to a single representation\n",
        "\n",
        "    Creates an aggregation (mean) operator to combine the encodings of multiple context inputs\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    context_rs\n",
        "        Input encodings tensor, shape: (n_samples, dim_r)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Output tensor of aggregation result\n",
        "    \"\"\"\n",
        "    mean = tf.reduce_mean(context_rs, axis=0)\n",
        "    r = tf.reshape(mean, [1, -1])\n",
        "    return r\n",
        "  \n",
        "  def _get_z_params(self, context_r: tf.Tensor) -> GaussianParams:\n",
        "    \"\"\"Map encoding to mean and covariance of the random variable Z\n",
        "\n",
        "    Creates a linear dense layer to map encoding to mu_z, and another linear mapping + a softplus activation for Sigma_z\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    context_r\n",
        "        Input encoding tensor, shape: (1, dim_r)\n",
        "    params\n",
        "        Neural process parameters\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        Output tensors of the mappings for mu_z and Sigma_z\n",
        "    \"\"\"\n",
        "    hidden = context_r\n",
        "    with tf.variable_scope(\"latent_encoder\", reuse=tf.AUTO_REUSE):\n",
        "      # First apply intermediate relu layer \n",
        "      hidden = tf.nn.relu(\n",
        "          tf.layers.dense(hidden, \n",
        "                          (self.layer_dims[-1] + self.latent_dim)/2, \n",
        "                          name=\"penultimate_layer\"))\n",
        "      \n",
        "      # Then apply further linear layers to output latent mu and log sigma\n",
        "      mu = tf.layers.dense(hidden, self.latent_dim, name=\"mean_layer\")\n",
        "      log_sigma = tf.layers.dense(hidden, self.latent_dim, name=\"std_layer\")\n",
        "      \n",
        "\n",
        "    # Compute sigma\n",
        "    sigma = 0.1 + 0.9 * tf.sigmoid(log_sigma)\n",
        "\n",
        "    return GaussianParams(mu, sigma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LWZZAs1haFkc",
        "colab": {}
      },
      "source": [
        "class NLP_NeuralProcess(object):\n",
        "  \n",
        "  def __init__(self,\n",
        "                  score_col, \n",
        "                  params = NeuralProcessParams(dim_z=20, \n",
        "                                                     n_hidden_units_h=[128, 128, 128], \n",
        "                                                     n_hidden_units_g=[128, 128, 128]),\n",
        "                  num_classes = 6, \n",
        "                  num_draws = 2, \n",
        "                  lr = 2e-5,\n",
        "                  batch_size = 32, \n",
        "                  num_warmup_steps=100,\n",
        "                  num_train_steps = 10**3,\n",
        "                  save_summary_steps = 100,\n",
        "                  save_checkpoints_steps = 500,\n",
        "                  keep_checkpoint_max = None,\n",
        "                  output_dir = \"./test_output\",\n",
        "                  context_features = None\n",
        "                 ):\n",
        "    \n",
        "    self.params = params\n",
        "    self.encoder = Encoder(layer_dims = self.params.n_hidden_units_h, \n",
        "                           latent_dim=self.params.dim_z)\n",
        "    self.decoder = Decoder(layer_dims= self.params.n_hidden_units_g, \n",
        "                           num_classes=num_classes)\n",
        "    self.num_draws = num_draws\n",
        "    self.num_classes = num_classes\n",
        "#     self.estimator = None\n",
        "    #self.embedder = Embedder()\n",
        "    #####  \n",
        "    num_labels = len(score_col)\n",
        "    \n",
        "    \n",
        "    # Specify outpit directory and number of checkpoint steps to save\n",
        "    \n",
        "    run_config = tf.estimator.RunConfig(model_dir=output_dir,\n",
        "          save_summary_steps=save_summary_steps, save_checkpoints_steps=save_checkpoints_steps)\n",
        "    \n",
        "    model_fn = self.model_fn_builder(num_labels = num_labels, learning_rate=lr,\n",
        "      num_train_steps=num_train_steps, num_warmup_steps=num_warmup_steps)\n",
        "    \n",
        "    if context_features is not None:\n",
        "      estimator_params = {\"batch_size\": batch_size, \"context_features\": context_features}\n",
        "      self.estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n",
        "                                              params=estimator_params)\n",
        "    \n",
        "    else: \n",
        "      self.estimator = tf.estimator.Estimator(model_fn=model_fn,config=run_config,\n",
        "            params={\"batch_size\": batch_size})\n",
        "    \n",
        "    \n",
        "  def create_model(self, \n",
        "                   target_input_ids, \n",
        "                   target_input_mask, \n",
        "                   target_segment_ids, \n",
        "                   target_scores=None,\n",
        "                   context_input_ids = None, \n",
        "                   context_input_mask = None, \n",
        "                   context_segment_ids= None, \n",
        "                   context_scores = None\n",
        "                   ):\n",
        "    \n",
        "    # apply embedder\n",
        "    embedder = hub.Module(BERT_model_hub,trainable=True)\n",
        "    \n",
        "    valid_context = (context_input_ids is not None) & (context_input_mask is not None) & (context_segment_ids is not None) & (context_scores is not None)\n",
        "    \n",
        "    # target processing - all scenarios\n",
        "    target_inputs = dict(input_ids=target_input_ids,\n",
        "                     input_mask=target_input_mask, \n",
        "                     segment_ids=target_segment_ids)\n",
        "    target_embeddings = embedder(inputs=target_inputs,\n",
        "                               signature=\"tokens\", \n",
        "                               as_dict=True)\n",
        "    target_xs = target_embeddings[\"pooled_output\"]\n",
        "    \n",
        "    \n",
        "    if valid_context:\n",
        "      \n",
        "      # context processing - training\n",
        "      context_inputs = dict(input_ids=context_input_ids,\n",
        "                         input_mask=context_input_mask, \n",
        "                         segment_ids=context_segment_ids)\n",
        "      context_embeddings = embedder(inputs=context_inputs,\n",
        "                                 signature=\"tokens\", \n",
        "                                 as_dict=True)\n",
        "      context_xs = context_embeddings[\"pooled_output\"]\n",
        "      context_ys = context_scores\n",
        "      # total x,y \n",
        "      x_all = tf.concat([context_xs, target_xs], axis=0)\n",
        "      \n",
        "      # get encoding params with context\n",
        "      context_z_dist = self.encoder(context_xs, context_ys)\n",
        "      # predictions with context\n",
        "      posterior_pred = self.decoder(target_xs, context_z_dist.sample(self.num_draws))\n",
        "        \n",
        "       # target scores - context training / evaluation\n",
        "      if target_scores is not None:\n",
        "        target_ys = target_scores\n",
        "        y_all = tf.concat([context_ys, target_ys], axis=0)\n",
        "        all_z_dist = self.encoder(x_all, y_all)\n",
        "        \n",
        "        # loss\n",
        "        loglike = self.loglikelihood(target_ys, posterior_pred)\n",
        "        KL_loss = self.KLqp_gaussian(all_z_dist.parameters['loc'], \n",
        "                                     all_z_dist.parameters['scale_diag'], \n",
        "                                     context_z_dist.parameters['loc'], \n",
        "                                     context_z_dist.parameters['scale_diag'])\n",
        "        loss = tf.negative(loglike) + KL_loss\n",
        "        # context and training / evaluation\n",
        "        return (loss, posterior_pred, target_ys)\n",
        "      \n",
        "      # context prediction\n",
        "      return  (None, posterior_pred, None)\n",
        "    \n",
        "    # no context\n",
        "    else:\n",
        "      x_all = target_xs \n",
        "      # get internal representation\n",
        "      mean_zero = tf.constant(np.repeat(0., params.dim_z))\n",
        "      epsilon_dist = tfd.MultivariateNormalDiag(loc= mean_zero)                            \n",
        "      epsilon = tf.expand_dims(epsilon_dist.sample(self.num_draws), axis=1)\n",
        "      epsilon = tf.cast(epsilon, tf.float32)\n",
        "      prior_predict = self.decoder(x_all, epsilon)\n",
        "      \n",
        "      # target scores - no context training / evaluation\n",
        "      if target_scores is not None:\n",
        "        target_ys = target_scores\n",
        "        loglike = self.loglikelihood(target_ys, prior_predict)\n",
        "        loss = tf.negative(loglike)\n",
        "        # no context/ training / evaluation\n",
        "        return  (loss, prior_predict, target_ys)\n",
        "   \n",
        "    \n",
        "      # no context prediction\n",
        "      return  (None, prior_predict, None)\n",
        "\n",
        "  \n",
        "  def loglikelihood(self, y_star: tf.Tensor, dist):\n",
        "    \"\"\"Log-likelihood of an output given a predicted \"\"\"\n",
        "    p_normal = tfd.MultivariateNormalDiag(loc = dist.mu, scale_diag=dist.sigma)\n",
        "    loglike = p_normal.log_prob(y_star)\n",
        "    loglike = tf.reduce_sum(loglike, axis=0)\n",
        "    loglike = tf.reduce_mean(loglike)\n",
        "    return loglike\n",
        "  \n",
        "  def KLqp_gaussian(self, mu_q: tf.Tensor, sigma_q: tf.Tensor, mu_p: tf.Tensor, sigma_p: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Kullback-Leibler divergence between two Gaussian distributions\n",
        "\n",
        "    Determines KL(q || p) = < log( q / p ) >_q\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu_q\n",
        "        Mean tensor of distribution q, shape: (1, dim)\n",
        "    sigma_q\n",
        "        Variance tensor of distribution q, shape: (1, dim)\n",
        "    mu_p\n",
        "        Mean tensor of distribution p, shape: (1, dim)\n",
        "    sigma_p\n",
        "        Variance tensor of distribution p, shape: (1, dim)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        KL tensor, shape: (1)\n",
        "    \"\"\"\n",
        "    sigma2_q = tf.square(sigma_q) + 1e-16\n",
        "    sigma2_p = tf.square(sigma_p) + 1e-16\n",
        "    temp = sigma2_q / sigma2_p + tf.square(mu_q - mu_p) / sigma2_p - 1.0 + tf.log(sigma2_p / sigma2_q + 1e-16)\n",
        "    return 0.5 * tf.reduce_sum(temp)\n",
        "  \n",
        "  def context_target_split(self, batch_size =32):\n",
        "    btch_sz = batch_size\n",
        "    n_context = tf.random_shuffle(tf.range(1,btch_sz))[0]\n",
        "    \n",
        "    indices = tf.range(0, btch_sz)\n",
        "    context_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context))\n",
        "    target_set_indices = tf.gather(tf.random_shuffle(indices),tf.range(n_context, btch_sz))\n",
        "    \n",
        "    return context_set_indices, target_set_indices\n",
        "    \n",
        "  def model_fn_builder(self, num_labels, learning_rate, num_train_steps, num_warmup_steps):\n",
        "      \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "      \n",
        "      \n",
        "      def model_fn(features, mode, params):  # pylint: disable=unused-argument\n",
        "          \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "          \n",
        "          # run model\n",
        "          # -------------------------------------------------------------------------------------------\n",
        "          target_input_ids = None\n",
        "          target_input_mask = None\n",
        "          target_segment_ids = None \n",
        "          target_scores = None\n",
        "          \n",
        "          context_input_ids = None \n",
        "          context_input_mask = None \n",
        "          context_segment_ids = None\n",
        "          context_scores = None\n",
        "          \n",
        "          # training \n",
        "          if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "            input_ids = features[\"input_ids\"]\n",
        "            input_mask = features[\"input_mask\"]\n",
        "            segment_ids = features[\"segment_ids\"]    \n",
        "            scores = features[\"scores\"]\n",
        "            \n",
        "            # context split\n",
        "            context_set_indices, target_set_indices= self.context_target_split(batch_size =32)\n",
        "            \n",
        "            target_input_ids = tf.gather(input_ids,target_set_indices)\n",
        "            target_input_mask = tf.gather(input_mask,target_set_indices)\n",
        "            target_segment_ids = tf.gather(segment_ids,target_set_indices) \n",
        "            target_scores = tf.gather(scores,target_set_indices)\n",
        "\n",
        "            context_input_ids = tf.gather(input_ids,context_set_indices) \n",
        "            context_input_mask = tf.gather(input_mask,context_set_indices) \n",
        "            context_segment_ids = tf.gather(segment_ids,context_set_indices)\n",
        "            context_scores = tf.gather(scores,context_set_indices)\n",
        "            \n",
        "            \n",
        "          elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "            print('Prediction')\n",
        "            target_input_ids = features[\"input_ids\"]\n",
        "            target_input_mask = features[\"input_mask\"]\n",
        "            target_segment_ids = features[\"segment_ids\"]    \n",
        "            target_scores = features[\"scores\"]\n",
        "            \n",
        "            try:\n",
        "              context_input_ids = features[\"supplied_context_input_ids\"][0]\n",
        "              context_input_mask = features[\"supplied_context_input_mask\"][0]\n",
        "              context_segment_ids = features[\"supplied_context_segment_ids\"][0] \n",
        "              context_scores = features[\"supplied_context_scores\"][0]\n",
        "               \n",
        "            except:\n",
        "              print(\"****No context supplied ****\")\n",
        "              context_input_ids = None\n",
        "              context_input_mask = None\n",
        "              context_segment_ids = None\n",
        "              context_scores = None\n",
        "            \n",
        "          else:\n",
        "            print('Evaluation')\n",
        "            target_input_ids = features[\"input_ids\"]\n",
        "            target_input_mask = features[\"input_mask\"]\n",
        "            target_segment_ids = features[\"segment_ids\"]    \n",
        "            target_scores = features[\"scores\"]\n",
        "\n",
        "            try:\n",
        "              context_input_ids = features[\"supplied_context_input_ids\"][0]\n",
        "              context_input_mask = features[\"supplied_context_input_mask\"][0]\n",
        "              context_segment_ids = features[\"supplied_context_segment_ids\"][0] \n",
        "              context_scores = features[\"supplied_context_scores\"][0]\n",
        "               \n",
        "            except:\n",
        "              print(\"****No context supplied ****\")\n",
        "              context_input_ids = None\n",
        "              context_input_mask = None\n",
        "              context_segment_ids = None\n",
        "              context_scores = None\n",
        "\n",
        "\n",
        "          (loss, prediction, true_y) = self.create_model(target_input_ids, \n",
        "                                                         target_input_mask, \n",
        "                                                         target_segment_ids, \n",
        "                                                         target_scores,\n",
        "                                                         context_input_ids, \n",
        "                                                         context_input_mask, \n",
        "                                                         context_segment_ids, \n",
        "                                                         context_scores)\n",
        "\n",
        "          train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
        "          ystar, _ = tf.nn.moments(prediction.mu,[0])\n",
        "          variance, _ = tf.nn.moments(tf.math.square(prediction.sigma),[0])\n",
        "          \n",
        "          # Calculate evaluation metrics\n",
        "          eval_metrics = {}\n",
        "          # AUC\n",
        "          def metric_fn(pred_scores, real_scores, trait_num):\n",
        "              auc_value = tf.metrics.auc(real_scores[:,trait_num], pred_scores[:,trait_num])\n",
        "              accuracy_value = tf.metrics.accuracy(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n",
        "              recall_value = tf.metrics.recall(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n",
        "              precision_value = tf.metrics.precision(labels = tf.round(real_scores[:,trait_num]), predictions=tf.round(pred_scores[:,trait_num]))\n",
        "              return {\"auc\"+str(trait_num): auc_value, \"accuracy\"+str(trait_num): accuracy_value, \"recall\"+str(trait_num): recall_value, \"precision\"+str(trait_num):precision_value}\n",
        "\n",
        "          labels = true_y # need to round them if true labels are not 1 or 0\n",
        "          eval_metrics_lst = [metric_fn(ystar, labels, trait_num) for trait_num in range(num_labels)]\n",
        "\n",
        "          for d in eval_metrics_lst:\n",
        "              tf.summary.scalar(list(d.keys())[0], list(d.values())[0][1])  # make available to tensorboard\n",
        "              tf.summary.scalar(list(d.keys())[1], list(d.values())[1][1])  # make available to tensorboard\n",
        "              eval_metrics.update(d)\n",
        "              \n",
        "          \n",
        "          # output from model\n",
        "          # -------------------------------------------------------------------------------------------\n",
        "\n",
        "          # training \n",
        "          if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "\n",
        "          # prediction\n",
        "          elif mode == tf.estimator.ModeKeys.PREDICT:\n",
        "              return tf.estimator.EstimatorSpec(mode=mode, predictions={'prediction_mean': ystar, 'prediction_var': variance})\n",
        "\n",
        "          # evaluation\n",
        "          else:\n",
        "            return tf.estimator.EstimatorSpec(mode=mode,loss=loss, eval_metric_ops=eval_metrics)\n",
        "\n",
        "      return model_fn\n",
        "\n",
        "\n",
        "  def prepare_examples(self, df, score_column, text_col_name, supplied_context_df=None):\n",
        "      num_labels = len(score_column)\n",
        "      input_examples = create_examples(df, score_column, text_col_name)\n",
        "      input_features = convert_examples_to_features(input_examples, max_seq_length, tokenizer)\n",
        "      \n",
        "      if supplied_context_df is not None:\n",
        "        supplied_context_examples = create_examples(supplied_context_df, score_column, text_col_name)\n",
        "        supplied_context_features = convert_examples_to_features(supplied_context_examples, max_seq_length, tokenizer)\n",
        "        input_fn = input_fn_builder(\n",
        "          features=input_features, seq_length=max_seq_length, \n",
        "          num_labels = num_labels, is_training=True, drop_remainder=False,\n",
        "          supplied_context_features = supplied_context_features)\n",
        "      \n",
        "      else:\n",
        "        input_fn = input_fn_builder(\n",
        "          features=input_features, seq_length=max_seq_length, \n",
        "          num_labels = num_labels, is_training=True, drop_remainder=False)\n",
        "        \n",
        "      return input_fn\n",
        "  \n",
        "  def predict(self, \n",
        "            df, \n",
        "            score_col, \n",
        "            text_col,\n",
        "            supplied_context_df=None\n",
        "             ):\n",
        "    \n",
        "    pred_input_fn = self.prepare_examples(df, score_col, text_col, supplied_context_df)\n",
        "    preds = self.estimator.predict(input_fn=pred_input_fn)\n",
        "    \n",
        "    return preds\n",
        "  \n",
        "  def evaluate(self, \n",
        "               eval_steps,\n",
        "               df, \n",
        "               score_col, \n",
        "               text_col,\n",
        "               supplied_context_df=None):\n",
        "    \n",
        "    eval_input_fn = self.prepare_examples(df, score_col, text_col, supplied_context_df)\n",
        "    \n",
        "    result = self.estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "    return result\n",
        "  \n",
        "  def train(self,num_train_steps,\n",
        "            df_train, \n",
        "            score_col, \n",
        "            text_col,\n",
        "           ):\n",
        "\n",
        "    # Create an input function for training. drop_remainder = True for using TPUs.\n",
        "    train_input_fn = self.prepare_examples(df_train, score_col, text_col)\n",
        "\n",
        "    print('Beginning Training!')\n",
        "    current_time = datetime.now()\n",
        "    self.estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "    print(\"Training took time \", datetime.now() - current_time)\n",
        "    \n",
        "  \n",
        "  def train_and_evaluate(self,\n",
        "                         df_train,\n",
        "                         df_eval,\n",
        "                         score_col, \n",
        "                         text_col,\n",
        "                         num_train_steps,\n",
        "                         eval_steps,\n",
        "                         supplied_context_df=None):\n",
        "    \n",
        "    train_input_fn = self.prepare_examples(df_train, score_col, text_col)\n",
        "    eval_input_fn = self.prepare_examples(df_eval, score_col, text_col, supplied_context_df)\n",
        "    \n",
        "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, steps = eval_steps)\n",
        "    \n",
        "    result = tf.estimator.train_and_evaluate(self.estimator, train_spec, eval_spec)\n",
        "    return result\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z8iSEczc6NS1",
        "outputId": "4036f593-baab-49a5-96cc-03dc0fefb8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "\n",
        "# tf.reset_default_graph()\n",
        "\n",
        "params = NeuralProcessParams(dim_z=1000, n_hidden_units_h=[512, 256, 128], n_hidden_units_g=[512, 256, 128])\n",
        "num_train_steps = 10*(10**3)\n",
        "\n",
        "neural_process = NLP_NeuralProcess(score_col=score_column, params = params, num_draws = 25, \n",
        "                                   num_train_steps=num_train_steps,\n",
        "                                   num_classes = len(score_column),\n",
        "                                   output_dir = OUTPUT_DIR,\n",
        "                                  context_features = None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0808 11:05:58.068946 139698332669824 estimator.py:209] Using config: {'_model_dir': 'gs://rail_jigsaw_modeling/unhealthy/models/batch32', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f0d60e29e48>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-3t1yb4RrW5y",
        "colab": {}
      },
      "source": [
        "# # df_test = test_df\n",
        "# batch_size = 32\n",
        "# eval_steps = int(len(df_val) / batch_size)\n",
        "\n",
        "# neural_process.train_and_evaluate(df_train=df_train,\n",
        "#                                   df_eval = df_val,\n",
        "#                                   num_train_steps=num_train_steps,\n",
        "#                                   eval_steps = eval_steps,\n",
        "#                                   score_col= score_column, \n",
        "#                                   text_col=text_col_name,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFg8H0rdqjc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_context = df_train.sample(20, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uHP88st76N4l",
        "outputId": "edb1728f-a4c1-411d-bfa4-528043499983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "batch_size = 32\n",
        "eval_steps = int(len(df_test) / batch_size)\n",
        "\n",
        "neural_process.evaluate(eval_steps,\n",
        "                       df_test, \n",
        "                       score_col= score_column, \n",
        "                       text_col=text_col_name, supplied_context_df = random_context)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0808 11:39:29.046445 139698332669824 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0808 11:39:32.515992 139698332669824 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0808 11:39:33.721351 139698332669824 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(20, 768), dtype=float32)\n",
            "Tensor(\"strided_slice_3:0\", shape=(20, 6), dtype=float32)\n",
            "Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n",
            "Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0808 11:39:47.282434 139698332669824 estimator.py:1147] Done calling model_fn.\n",
            "I0808 11:39:47.302659 139698332669824 evaluation.py:255] Starting evaluation at 2019-08-08T11:39:47Z\n",
            "I0808 11:39:48.784996 139698332669824 monitored_session.py:240] Graph was finalized.\n",
            "I0808 11:39:49.020799 139698332669824 saver.py:1280] Restoring parameters from gs://rail_jigsaw_modeling/unhealthy/models/batch32/model.ckpt-30000\n",
            "I0808 11:41:16.657028 139698332669824 session_manager.py:500] Running local_init_op.\n",
            "I0808 11:41:17.081788 139698332669824 session_manager.py:502] Done running local_init_op.\n",
            "I0808 11:41:27.194047 139698332669824 evaluation.py:167] Evaluation [14/140]\n",
            "I0808 11:41:34.401983 139698332669824 evaluation.py:167] Evaluation [28/140]\n",
            "I0808 11:41:41.796669 139698332669824 evaluation.py:167] Evaluation [42/140]\n",
            "I0808 11:41:49.301441 139698332669824 evaluation.py:167] Evaluation [56/140]\n",
            "I0808 11:41:56.696326 139698332669824 evaluation.py:167] Evaluation [70/140]\n",
            "I0808 11:42:03.917508 139698332669824 evaluation.py:167] Evaluation [84/140]\n",
            "I0808 11:42:11.056041 139698332669824 evaluation.py:167] Evaluation [98/140]\n",
            "I0808 11:42:18.096768 139698332669824 evaluation.py:167] Evaluation [112/140]\n",
            "I0808 11:42:25.080800 139698332669824 evaluation.py:167] Evaluation [126/140]\n",
            "I0808 11:42:32.015079 139698332669824 evaluation.py:167] Evaluation [140/140]\n",
            "I0808 11:42:32.724852 139698332669824 evaluation.py:275] Finished evaluation at 2019-08-08-11:42:32\n",
            "I0808 11:42:32.726058 139698332669824 estimator.py:2039] Saving dict for global step 30000: accuracy0 = 0.9375, accuracy1 = 0.91450894, accuracy2 = 0.9558036, accuracy3 = 0.97098213, accuracy4 = 0.96875, accuracy5 = 0.92901784, auc0 = 0.57798177, auc1 = 0.60003, auc2 = 0.5898204, auc3 = 0.55191123, auc4 = 0.5430317, auc5 = 0.5617589, global_step = 30000, loss = 132.25116, precision0 = 0.3164557, precision1 = 0.23880596, precision2 = 0.2195122, precision3 = 0.2037037, precision4 = 0.5, precision5 = 0.14285715, recall0 = 0.0996016, recall1 = 0.17266187, recall2 = 0.05142857, recall3 = 0.1122449, recall4 = 0.05, recall5 = 0.078947365\n",
            "I0808 11:43:08.236235 139698332669824 estimator.py:2099] Saving 'checkpoint_path' summary for global step 30000: gs://rail_jigsaw_modeling/unhealthy/models/batch32/model.ckpt-30000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy0': 0.9375,\n",
              " 'accuracy1': 0.91450894,\n",
              " 'accuracy2': 0.9558036,\n",
              " 'accuracy3': 0.97098213,\n",
              " 'accuracy4': 0.96875,\n",
              " 'accuracy5': 0.92901784,\n",
              " 'auc0': 0.57798177,\n",
              " 'auc1': 0.60003,\n",
              " 'auc2': 0.5898204,\n",
              " 'auc3': 0.55191123,\n",
              " 'auc4': 0.5430317,\n",
              " 'auc5': 0.5617589,\n",
              " 'global_step': 30000,\n",
              " 'loss': 132.25116,\n",
              " 'precision0': 0.3164557,\n",
              " 'precision1': 0.23880596,\n",
              " 'precision2': 0.2195122,\n",
              " 'precision3': 0.2037037,\n",
              " 'precision4': 0.5,\n",
              " 'precision5': 0.14285715,\n",
              " 'recall0': 0.0996016,\n",
              " 'recall1': 0.17266187,\n",
              " 'recall2': 0.05142857,\n",
              " 'recall3': 0.1122449,\n",
              " 'recall4': 0.05,\n",
              " 'recall5': 0.078947365}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_1UKwVTxxgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test2 = df_test[df_test._trusted_judgments<6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NqA06NHYgxh",
        "colab": {}
      },
      "source": [
        "preds = neural_process.predict(\n",
        "                       df_test2, \n",
        "                       score_col= score_column, \n",
        "                       text_col=text_col_name,supplied_context_df = random_context)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7hEFhEGHkrVy",
        "colab": {}
      },
      "source": [
        "\n",
        "test_scores = df_test2[score_column]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fri1X4hdkrev",
        "outputId": "3e1000b5-3058-4c25-ca63-2addd0db421f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "uncertainty_per_score = pd.DataFrame(columns=['presence','accuracy', 'variance', 'num_judgments'])\n",
        "uncertainty_per_row = pd.DataFrame(columns=['presence','accuracy', 'variance', 'num_judgments'])\n",
        "pred_means = pd.DataFrame(columns = score_column)\n",
        "pred_variances = pd.DataFrame(columns = score_column)\n",
        "\n",
        "for i, pred in enumerate(preds):\n",
        "  if i<len(test_scores):\n",
        "    scores = np.array(test_scores.iloc[i])\n",
        "    #     print(scores)\n",
        "    pred_means = pred_means.append({score_column[k]:pred['prediction_mean'][k] for k in range(len(score_column))}, ignore_index=True)\n",
        "    #     print(mean_prediction)\n",
        "    variance = pred['prediction_var']\n",
        "    pred_variances = pred_variances.append({score_column[k]:variance[k] for k in range(len(score_column))}, ignore_index=True)  \n",
        "    mean_prediction = np.round(pred['prediction_mean'])\n",
        "    acc = scores==mean_prediction\n",
        "    row_acc = int( sum(scores==mean_prediction)==len(scores) )\n",
        "    presence = int(sum(scores)>0)\n",
        "    uncertainty_per_row = uncertainty_per_row.append({'presence': presence,'accuracy': row_acc, 'variance': np.mean(variance), 'num_judgments':df_test.iloc[i,-1]}, ignore_index=True)\n",
        "    for j in range(len(scores)):\n",
        "      uncertainty_per_score = uncertainty_per_score.append({'presence':scores[j], 'accuracy': int(acc[j]), 'variance': variance[j], 'num_judgments':int(df_test.iloc[i,-1])}, ignore_index=True)\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0808 11:20:07.354612 139698332669824 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0808 11:20:11.079793 139698332669824 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0808 11:20:11.952297 139698332669824 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(300, 768), dtype=float32)\n",
            "Tensor(\"strided_slice_3:0\", shape=(300, 6), dtype=float32)\n",
            "Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n",
            "Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0808 11:20:25.788328 139698332669824 estimator.py:1147] Done calling model_fn.\n",
            "I0808 11:20:27.293401 139698332669824 monitored_session.py:240] Graph was finalized.\n",
            "I0808 11:20:27.525445 139698332669824 saver.py:1280] Restoring parameters from gs://rail_jigsaw_modeling/unhealthy/models/batch32/model.ckpt-30000\n",
            "I0808 11:21:16.229130 139698332669824 session_manager.py:500] Running local_init_op.\n",
            "I0808 11:21:16.643101 139698332669824 session_manager.py:502] Done running local_init_op.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65kV2A4--Nr7",
        "colab_type": "code",
        "outputId": "b406faee-9500-48fe-e874-0cdf8c0b00e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "uncertainty_per_score.corr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>presence</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>variance</th>\n",
              "      <th>num_judgments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>presence</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.805235</td>\n",
              "      <td>-0.007815</td>\n",
              "      <td>0.000882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>-0.805235</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.213548</td>\n",
              "      <td>0.003375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variance</th>\n",
              "      <td>-0.007815</td>\n",
              "      <td>-0.213548</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.003348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_judgments</th>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.003375</td>\n",
              "      <td>-0.003348</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               presence  accuracy  variance  num_judgments\n",
              "presence       1.000000 -0.805235 -0.007815       0.000882\n",
              "accuracy      -0.805235  1.000000 -0.213548       0.003375\n",
              "variance      -0.007815 -0.213548  1.000000      -0.003348\n",
              "num_judgments  0.000882  0.003375 -0.003348       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGL2n_1re_Xk",
        "colab_type": "code",
        "outputId": "480d4ef0-9495-4466-8ed5-3e6f90633172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "uncertainty_per_row.corr()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>presence</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>variance</th>\n",
              "      <th>num_judgments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>presence</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.752693</td>\n",
              "      <td>-0.010856</td>\n",
              "      <td>-0.006546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accuracy</th>\n",
              "      <td>-0.752693</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.287406</td>\n",
              "      <td>0.014728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variance</th>\n",
              "      <td>-0.010856</td>\n",
              "      <td>-0.287406</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.005299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>num_judgments</th>\n",
              "      <td>-0.006546</td>\n",
              "      <td>0.014728</td>\n",
              "      <td>-0.005299</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               presence  accuracy  variance  num_judgments\n",
              "presence       1.000000 -0.752693 -0.010856      -0.006546\n",
              "accuracy      -0.752693  1.000000 -0.287406       0.014728\n",
              "variance      -0.010856 -0.287406  1.000000      -0.005299\n",
              "num_judgments -0.006546  0.014728 -0.005299       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gc3034n5pYtd",
        "outputId": "af375e6c-0536-4e2c-b264-8b277dde2cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "print(\"Average variance for accurate/correct predictions:\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.accuracy==1]['variance'].mean())\n",
        "print(\"Average variance for inaccurate/false predictions:\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.accuracy==0]['variance'].mean())\n",
        "print(\"Ratio of average variance incorrect::correct\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.accuracy==0]['variance'].mean()/uncertainty_per_score[uncertainty_per_score.accuracy==1]['variance'].mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average variance for accurate/correct predictions:\n",
            "0.01892358005760754\n",
            "Average variance for inaccurate/false predictions:\n",
            "0.1041500688000671\n",
            "Ratio of average variance incorrect::correct\n",
            "5.503719089253269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h5PsPzmyH6sA"
      },
      "source": [
        "Check for correlation with presence/absence of attribtue with variance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOcNVys4-Nl-",
        "colab_type": "code",
        "outputId": "e311e20c-4371-47b6-a782-f7b4502f83f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "print(\"Average variance for predictions when attribute is present:\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.presence==1]['variance'].mean())\n",
        "print(\"Average variance for predictions when attribute is absent:\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.presence==0]['variance'].mean())\n",
        "print(\"Ratio of average variance present::absent\")\n",
        "print(uncertainty_per_score[uncertainty_per_score.presence==1]['variance'].mean()/uncertainty_per_score[uncertainty_per_score.presence==0]['variance'].mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average variance for predictions when attribute is present:\n",
            "0.02882015669502065\n",
            "Average variance for predictions when attribute is absent:\n",
            "0.023344012376548595\n",
            "Ratio of average variance present::absent\n",
            "1.2345845362887742\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL1LWvFh-NpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uncertainty_per_row = pd.DataFrame(columns=['presence','accuracy', 'variance', 'num_judgments'])\n",
        "pred_means = pd.DataFrame(columns = score_column)\n",
        "pred_variances = pd.DataFrame(columns = score_column)\n",
        "\n",
        "for i, pred in enumerate(preds):\n",
        "  if i<len(test_scores):\n",
        "    scores = np.array(test_scores.iloc[i])\n",
        "    pred_means = pred_means.append({score_column[k]:pred['prediction_mean'][k] for k in range(len(score_column))}, ignore_index=True)\n",
        "    variance = pred['prediction_var']\n",
        "    pred_variances = pred_variances.append({score_column[k]:variance[k] for k in range(len(score_column))}, ignore_index=True)  \n",
        "    mean_prediction = np.round(pred['prediction_mean'])\n",
        "    row_acc = int( sum(scores==mean_prediction)==len(scores) )\n",
        "    presence = int(sum(scores)>0)\n",
        "    uncertainty_per_row = uncertaintyper_row.append({'presence': presence,'accuracy': acc, 'variance': np.mean(variance), 'num_judgments':df_test.iloc[i,-1]}, ignore_index=True)\n",
        "  else:\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RJvlFgZ9j_r6",
        "colab": {}
      },
      "source": [
        "def total_accuaracy(pred_means, test_scores):\n",
        "  preds = np.round(pred_means)\n",
        "  comparison = preds == test_scores\n",
        "  correct_guesses = np.reshape(comparison.values,-1)\n",
        "  return np.sum(correct_guesses) / len(correct_guesses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cPZACnkUGrr5",
        "colab": {}
      },
      "source": [
        "def class_accuaracy(pred_means, test_scores):\n",
        "  preds = np.round(pred_means)\n",
        "  comparison = preds == test_scores\n",
        "  correct_guesses = comparison.values\n",
        "  return correct_guesses.sum(axis=0)/ len(correct_guesses)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-xVcqWkdHT-s",
        "outputId": "083c9935-5fca-4925-8b62-8da2c822253d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "print(total_accuaracy(pred_means, test_scores.reset_index(drop=True)))\n",
        "print(class_accuaracy(pred_means, test_scores.reset_index(drop=True)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9060351812645391\n",
            "[0.79283306 0.95742178 0.8519564  0.99568729 0.87359837 0.96471418]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wjZUS-Ergg00",
        "outputId": "f6bcda19-d41a-4d7e-c161-f81329af473b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        }
      },
      "source": [
        "def subset_accuracy(att, att_num):\n",
        "  print(\"accuracy on \"+att+ \" comments:\")\n",
        "  print(class_accuaracy(pred_means.iloc[np.where(test_scores[att] == 1)].reset_index(drop=True),\n",
        "                        test_scores.iloc[np.where(test_scores[att] == 1)].reset_index(drop=True))[att_num])\n",
        "\n",
        "  print(\"proportion of \"+att+ \" comments:\")\n",
        "  print(len(test_scores.iloc[np.where(test_scores[att] == 1)].reset_index(drop=True))/len(test_scores))\n",
        "  print()\n",
        "\n",
        "for i, att in enumerate(score_column):\n",
        "  subset_accuracy(att, i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy on toxic comments:\n",
            "0.11676646706586827\n",
            "proportion of toxic comments:\n",
            "0.1309495804908649\n",
            "\n",
            "accuracy on severe_toxic comments:\n",
            "0.030864197530864196\n",
            "proportion of severe_toxic comments:\n",
            "0.012702893436838392\n",
            "\n",
            "accuracy on obscene comments:\n",
            "0.09008097165991903\n",
            "proportion of obscene comments:\n",
            "0.07747196738022426\n",
            "\n",
            "accuracy on threat comments:\n",
            "0.0\n",
            "proportion of threat comments:\n",
            "0.004312710734729083\n",
            "\n",
            "accuracy on insult comments:\n",
            "0.06285072951739619\n",
            "proportion of insult comments:\n",
            "0.06986591390261115\n",
            "\n",
            "accuracy on identity_hate comments:\n",
            "0.013333333333333334\n",
            "proportion of identity_hate comments:\n",
            "0.011761938367442954\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m2flPKNig2zn",
        "outputId": "0eb97fbe-d5d2-4676-db24-4bc7b7663b35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "len(test_scores[test_scores.severe_toxic==1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "229"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NZ1zjhdxZByn",
        "colab": {}
      },
      "source": [
        "at_least_one = (test_scores.toxic==1) | (test_scores.severe_toxic==1) | \\\n",
        "               (test_scores.obscene==1) | (test_scores.threat==1) | \\\n",
        "               (test_scores.insult==1) | (test_scores.identity_hate==1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2VzhiqkLZB2h",
        "colab": {}
      },
      "source": [
        "print(uncertainty[uncertainty.accuracy==True]['variance'].mean())\n",
        "print(uncertainty[uncertainty.accuracy==False]['variance'].mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "75VhsEk5ZB5E",
        "outputId": "d1d7411d-4cdc-43de-c697-c41c82472cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wXgIhm8zujWn",
        "outputId": "4e9d86f8-5349-47a8-f0d2-ee399497879e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "try: \n",
        "  print(\"this\")\n",
        "  True/\"this\"\n",
        "except:\n",
        "  print(\"didnt work\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "this\n",
            "didnt work\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iWn5PHX-4Tdt",
        "colab": {}
      },
      "source": [
        "unhealthy_df = df_train[(df_train.antagonize == 1) | (df_train.condescending == 1) |(df_train.dismissive == 1) | (df_train.generalisation == 1) |\n",
        "                 (df_train.hostile == 1) | (df_train.sarcastic == 1)]\n",
        "healthy_df = df_train[(df_train.antagonize == 0) & (df_train.condescending == 0) & (df_train.dismissive == 0) & (df_train.generalisation == 0) &\n",
        "                 (df_train.hostile == 0) & (df_train.sarcastic == 0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2B_-7NtJLjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_df = pd.concat([unhealthy_df.sample(10), healthy_df.sample(40)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZygYX_CTJMb9",
        "colab_type": "code",
        "outputId": "bd2aa212-fe95-466c-e347-627cb0f3b333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        }
      },
      "source": [
        "batch_size = 32\n",
        "eval_steps = int(len(df_test) / batch_size)\n",
        "\n",
        "neural_process.evaluate(eval_steps,\n",
        "                       df_test, \n",
        "                       score_col= score_column, \n",
        "                       text_col=text_col_name, supplied_context_df = context_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I0706 16:53:42.960248 140042348476288 estimator.py:1145] Calling model_fn.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "I0706 16:53:46.208950 140042348476288 saver.py:1499] Saver not created because there are no variables in the graph to restore\n",
            "I0706 16:53:46.811548 140042348476288 saver.py:1499] Saver not created because there are no variables in the graph to restore\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"module_apply_tokens_1/bert/pooler/dense/Tanh:0\", shape=(50, 768), dtype=float32)\n",
            "Tensor(\"strided_slice_3:0\", shape=(50, 6), dtype=float32)\n",
            "Tensor(\"concat:0\", shape=(?, 768), dtype=float32)\n",
            "Tensor(\"concat_3:0\", shape=(?, 6), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "I0706 16:53:59.571602 140042348476288 estimator.py:1147] Done calling model_fn.\n",
            "I0706 16:53:59.595152 140042348476288 evaluation.py:255] Starting evaluation at 2019-07-06T16:53:59Z\n",
            "I0706 16:54:01.813944 140042348476288 monitored_session.py:240] Graph was finalized.\n",
            "I0706 16:54:02.037882 140042348476288 saver.py:1280] Restoring parameters from gs://rail_jigsaw_modeling/unhealthy/models/batch32/model.ckpt-30000\n",
            "I0706 16:54:44.575266 140042348476288 session_manager.py:500] Running local_init_op.\n",
            "I0706 16:54:45.075836 140042348476288 session_manager.py:502] Done running local_init_op.\n",
            "I0706 16:54:59.687331 140042348476288 evaluation.py:167] Evaluation [14/140]\n",
            "I0706 16:55:10.956083 140042348476288 evaluation.py:167] Evaluation [28/140]\n",
            "I0706 16:55:22.650189 140042348476288 evaluation.py:167] Evaluation [42/140]\n",
            "I0706 16:55:34.293686 140042348476288 evaluation.py:167] Evaluation [56/140]\n",
            "I0706 16:55:45.616694 140042348476288 evaluation.py:167] Evaluation [70/140]\n",
            "I0706 16:55:56.693059 140042348476288 evaluation.py:167] Evaluation [84/140]\n",
            "I0706 16:56:07.710218 140042348476288 evaluation.py:167] Evaluation [98/140]\n",
            "I0706 16:56:18.718191 140042348476288 evaluation.py:167] Evaluation [112/140]\n",
            "I0706 16:56:29.797264 140042348476288 evaluation.py:167] Evaluation [126/140]\n",
            "I0706 16:56:41.022957 140042348476288 evaluation.py:167] Evaluation [140/140]\n",
            "I0706 16:56:41.623285 140042348476288 evaluation.py:275] Finished evaluation at 2019-07-06-16:56:41\n",
            "I0706 16:56:41.624446 140042348476288 estimator.py:2039] Saving dict for global step 30000: accuracy0 = 0.9375, accuracy1 = 0.9140625, accuracy2 = 0.9558036, accuracy3 = 0.9707589, accuracy4 = 0.96875, accuracy5 = 0.93013394, auc0 = 0.5799889, auc1 = 0.598453, auc2 = 0.58911043, auc3 = 0.5513847, auc4 = 0.5432443, auc5 = 0.5626207, global_step = 30000, loss = 133.39682, precision0 = 0.3164557, precision1 = 0.24257426, precision2 = 0.225, precision3 = 0.2037037, precision4 = 0.46666667, precision5 = 0.15322581, recall0 = 0.0996016, recall1 = 0.17437722, recall2 = 0.051136363, recall3 = 0.11111111, recall4 = 0.05035971, recall5 = 0.08370044\n",
            "I0706 16:56:43.479632 140042348476288 estimator.py:2099] Saving 'checkpoint_path' summary for global step 30000: gs://rail_jigsaw_modeling/unhealthy/models/batch32/model.ckpt-30000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy0': 0.9375,\n",
              " 'accuracy1': 0.9140625,\n",
              " 'accuracy2': 0.9558036,\n",
              " 'accuracy3': 0.9707589,\n",
              " 'accuracy4': 0.96875,\n",
              " 'accuracy5': 0.93013394,\n",
              " 'auc0': 0.5799889,\n",
              " 'auc1': 0.598453,\n",
              " 'auc2': 0.58911043,\n",
              " 'auc3': 0.5513847,\n",
              " 'auc4': 0.5432443,\n",
              " 'auc5': 0.5626207,\n",
              " 'global_step': 30000,\n",
              " 'loss': 133.39682,\n",
              " 'precision0': 0.3164557,\n",
              " 'precision1': 0.24257426,\n",
              " 'precision2': 0.225,\n",
              " 'precision3': 0.2037037,\n",
              " 'precision4': 0.46666667,\n",
              " 'precision5': 0.15322581,\n",
              " 'recall0': 0.0996016,\n",
              " 'recall1': 0.17437722,\n",
              " 'recall2': 0.051136363,\n",
              " 'recall3': 0.11111111,\n",
              " 'recall4': 0.05035971,\n",
              " 'recall5': 0.08370044}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybMNCnafKHmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}